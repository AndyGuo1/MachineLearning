# 梯度下降算法应用于线性回归问题
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/01-Week1/4-Parameter Learning/3-Gradient Descent For Linear Regression.mp4" type="video/mp4">
</video>
## 中文
### 梯度下降算法与线性回归问题结合
![梯度下降算法与线性回归问题结合](amWiki/images/001/01-Week1/4-Parameter Learning/16_梯度下降算法与线性回归问题结合.jpg)
在以前的视频中我们谈到 关于梯度下降算法 梯度下降是很常用的算法 它不仅被用在线性回归上 和线性回归模型、平方误差代价函数 在这段视频中 我们要将梯度下降 和代价函数结合 在后面的视频中 我们将用到此算法 并将其应用于 具体的拟合直线的线性回归算法里 这就是 我们在之前的课程里所做的工作 这是梯度下降法 这个算法你应该很熟悉 这是线性回归模型 还有线性假设和平方误差代价函数
### 梯度下降算法微分项推导
![梯度下降算法微分项推导](amWiki/images/001/01-Week1/4-Parameter Learning/17_梯度下降算法微分项推导.jpg)
我们将要做的就是 用梯度下降的方法 来最小化平方误差代价函数 为了 使梯度下降 为了 写这段代码 我们需要的关键项 是这里这个微分项 所以.我们需要弄清楚 这个偏导数项是什么 并结合这里的 代价函数J 的定义 就是这样 一个求和项 代价函数就是 这个误差平方项 我这样做 只是 把定义好的代价函数 插入了这个微分式 再简化一下 这等于是 这一个求和项 θ0 + θ1x(1) - y(i) θ0 + θ1x(1) - y(i) 这一项其实就是 我的假设的定义 然后把这一项放进去 实际上我们需要弄清楚这两个 偏导数项是什么 这两项分别是 j=0 和j=1的情况 因此我们要弄清楚 θ0 和 θ1 对应的偏导数项是什么 我只把答案写出来 事实上 第一项可简化为1 / m 乘以求和式 对所有训练样本求和 求和项是 h(x(i))-y(i) 而这一项 对θ(1)的微分项 得到的是这样一项 对吧 所以 偏导数项 从这个等式 到下面的等式 计算这些偏导数项需要一些多元微积分 如果你掌握了微积分 你可以随便自己推导这些 然后你检查你的微分 你实际上会得到我给出的答案 但如果你 不太熟悉微积分 别担心 你可以直接用这些 已经算出来的结果 你不需要掌握微积分 或者别的东西 来完成作业 你只需要会用梯度下降就可以 在定义这些以后 在我们算出 这些微分项以后 这些微分项 实际上就是代价函数J的斜率 现在可以将它们放回 我们的梯度下降算法 所以这就是专用于 线性回归的梯度下降 反复执行括号中的式子直到收敛 θ0和θ1不断被更新 都是加上一个-α/m 乘上后面的求和项 所以这里这一项 所以这就是我们的线性回归算法 对吧? 当然 这一项就是关于θ0的偏导数 在上一张幻灯片中推出的而第二项 这一项是刚刚的推导出的 关于θ1的 偏导数项
### 执行梯度下降注意细节_同时更新θ0和θ1
![执行梯度下降注意细节_同时更新θ0和θ1](amWiki/images/001/01-Week1/4-Parameter Learning/18_执行梯度下降注意细节_同时更新θ0和θ1.jpg)
提醒一下 执行梯度下降时 有一个细节要注意 就是必须要 同时更新θ0和θ1 所以 让我们来看看梯度下降是如何工作的 我们用梯度下降解决问题的 一个原因是 它更容易得到局部最优值
### 回顾梯度下降
![回顾梯度下降](amWiki/images/001/01-Week1/4-Parameter Learning/19_回顾梯度下降.jpg)  
![回顾梯度下降](amWiki/images/001/01-Week1/4-Parameter Learning/20_回顾梯度下降.jpg)  
当我第一次解释梯度下降时 我展示过这幅图 在表面上 不断下降 并且我们知道了 根据你的初始化 你会得到不同的局部最优解 你知道.你可以结束了.在这里或这里。
### 线性回归中的代价函数一定是一个凸函数
![线性回归中的代价函数一定是一个凸函数](amWiki/images/001/01-Week1/4-Parameter Learning/21_线性回归中的代价函数一定是一个凸函数.jpg)
但是 事实证明 用于线性回归的 代价函数 总是这样一个 弓形的样子 这个函数的专业术语是 这是一个凸函数 我不打算在这门课中 给出凸函数的定义 凸函数(convex function) 但不正式的说法是它就是一个弓形的函数 因此 这个函数 没有任何局部最优解 只有一个全局最优解 并且无论什么时候 你对这种代价函数 使用线性回归 梯度下降法得到的结果 总是收敛到全局最优值 因为没有全局最优以外的其他局部最优点
### 直观感受梯度下降算法执行过程
![直观感受梯度下降算法执行过程](amWiki/images/001/01-Week1/4-Parameter Learning/22_直观感受梯度下降算法执行过程_θ0和θ1初始化.jpg)
现在 让我们来看看这个算法的执行过程 像往常一样 这是假设函数的图 还有代价函数J的图 让我们来看看如何 初始化参数的值通常来说 初始化参数为零 θ0和θ1都在零 但为了展示需要 在这个梯度下降的实现中 我把θ0初始化为-900 θ1初始化为-0.1
![直观感受梯度下降算法执行过程](amWiki/images/001/01-Week1/4-Parameter Learning/23_直观感受梯度下降算法执行过程.jpg)
![直观感受梯度下降算法执行过程](amWiki/images/001/01-Week1/4-Parameter Learning/24_直观感受梯度下降算法执行过程.jpg)
![直观感受梯度下降算法执行过程](amWiki/images/001/01-Week1/4-Parameter Learning/25_直观感受梯度下降算法执行过程.jpg)
![直观感受梯度下降算法执行过程](amWiki/images/001/01-Week1/4-Parameter Learning/26_直观感受梯度下降算法执行过程.jpg)
![直观感受梯度下降算法执行过程](amWiki/images/001/01-Week1/4-Parameter Learning/27_直观感受梯度下降算法执行过程.jpg)
![直观感受梯度下降算法执行过程](amWiki/images/001/01-Week1/4-Parameter Learning/28_直观感受梯度下降算法执行过程.jpg)
![直观感受梯度下降算法执行过程](amWiki/images/001/01-Week1/4-Parameter Learning/29_直观感受梯度下降算法执行过程.jpg)
![直观感受梯度下降算法执行过程](amWiki/images/001/01-Week1/4-Parameter Learning/30_直观感受梯度下降算法执行过程.jpg)
### 批量梯度下降算法
![批量梯度下降算法](amWiki/images/001/01-Week1/4-Parameter Learning/31_批量梯度下降算法.jpg)
最后给出另一个名字,我们刚才说的算法有时被称为批梯度下降.事实证明在机器学习中，我并不知道我觉得我们研究机器学习的人并不擅长给机器学习算法命名。但是这个术语批梯度下降指的是我们正在看的训练例子中的梯度下降的每一步都是这样。 所以在梯度下降时，当计算导数时，我们计算的是和。因此，梯度下降的过程，我们最终会计算出类似于我们的m个训练样本的和，所以，批梯度下降指的是我们学习视频中看到的训练例子一样。再次强调，这真的不是一个号名字，但是这是研究机器学习的人就是这么叫的。结果是有时梯度下降的其他版本不是批次的。不要看完整的训练集，而是每次只看一小部分的训练集。以后在这么课程中，我们也将谈谈其他版本的梯度下降。但目前来说，使用这个算法我们只是学习它或者使用批梯度下降你知道如何在线性回归问题中使用它。因此，这是线性回归问题和梯度下降算法结合。如果你之前学习过高级线性大叔，你们中的一些人可能之前就在高级线性代数中学习过了。您可能知道，有一个解决方案是可以用数学方法来解决代价函数最小化问题的，而并不需要用梯度下降这种算法。在这门课程的后面，我们将谈谈那种解决方案，用来解决代价函数J的最小化问题，而并不需要这种复杂的梯度下降。那种方案成为：正规方程。但如果你听说过那种方式，就会知道梯度下降算法在大数据集下会比正规方程要好。现在我们知道了梯度下降，我们就可以在很多不同的背景或者不同的机器学习问题中使用它。因此，恭喜你已经学完了你的第一个机器学习算法。在后面我们将会有练习,在那里我们会让你实现梯度下降算法，并希望你自己认识认识到这种算法的正确性。但是在这之前，我首先要告诉你，在下一系列的视频中。第一个要告诉你的就是是关于梯度下降算法的泛化，这将会让它更强大。我想在下个视频中我会告诉你们。
### 视频内嵌习题
![视频内嵌习题](amWiki/images/001/01-Week1/4-Parameter Learning/32_视频内嵌习题.jpg)
## English
### Gradient Descent Algorithm for linear regression
![Gradient Descent Algorithm for linear regression](amWiki/images/001/01-Week1/4-Parameter Learning/16_梯度下降算法与线性回归问题结合.jpg)
In previous videos, we talked about the gradient descent algorithm and we talked about the linear regression model and the squared error cost function. In this video we're gonna put together gradient descent with our cost function, and that will give us an algorithm for linear regression or putting a straight line to our data.So this was what we worked out in the previous videos. This gradient descent algorithm which you should be familiar and here's the linear regression model with our linear hypothesis and our squared error cost function. What we're going to do is apply gradient descent to minimize our squared error cost function.
### Figure out derivative term
![Figure out derivative term](amWiki/images/001/01-Week1/4-Parameter Learning/17_梯度下降算法微分项推导.jpg)
Now in order to apply gradient descent, in order to, you know, write this piece of code, the key term we need is this derivative term over here. So you need to figure out what is this partial derivative term and plugging in the definition of the cause function j. This turns out to be this.Sum from y equals 1 though m. Of this squared error cost function term. And all I did here was I just, you know plug in the definition of the cost function there. And simplifying a little bit more, this turns out to be equal to this. Sigma i equals one through m of theta zero plus theta one x i minus Yi squared. And all I did there was I took the definition for my hypothesis and plugged it in there. And turns out we need to figure out what is this partial derivative for two cases for J equals 0 and J equals 1. So we want to figure out what is this partial derivative for both the theta 0 case and the theta 1 case. And I'm just going to write out the answers. It turns out this first term is, simplifies to 1/M sum from over my training step of just that of X(i)- Y(i) and for this term partial derivative let's write the theta 1, it turns out I get this term. Minus Y(i) times X(i). Okay and computing these partial derivatives, so we're going from this equation. Right going from this equation to either of the equations down there. Computing those partial derivative terms requires some multivariate calculus. If you know calculus, feel free to work through the derivations yourself and check that if you take the derivatives, you actually get the answers that I got. But if you're less familiar with calculus, don't worry about it and it's fine to just take these equations that were worked out and you won't need to know calculus or anything like that, in order to do the homework so let's implement gradient descent and get back to work.So armed with these definitions or armed with what we worked out to be the derivatives which is really just the slope of the cost function j we can now plug them back in to our gradient descent algorithm. So here's gradient descent for linear regression which is gonna repeat until convergence, theta 0 and theta 1 get updated as you know this thing minus alpha times the derivative term.So this term here.So here's our linear regression algorithm.This first term here.That term is of course just the partial derivative with respect to theta zero, that we worked out on a previous slide. And this second term here, that term is just a partial derivative in respect to theta 1, that we worked out on the previous line.
### Implementing gradient descent_update theta 0 and theta 1 simultaneously
![Implementing gradient descent_update theta 0 and theta 1 simultaneously](amWiki/images/001/01-Week1/4-Parameter Learning/18_执行梯度下降注意细节_同时更新θ0和θ1.jpg)
And just as a quick reminder, you must, when implementing gradient descent. There's actually this detail that you should be implementing it so the update theta 0 and theta 1 simultaneously.
###  Review gradient descent
![Review gradient descent](amWiki/images/001/01-Week1/4-Parameter Learning/19_回顾梯度下降.jpg)  
![Review gradient descent](amWiki/images/001/01-Week1/4-Parameter Learning/20_回顾梯度下降.jpg)  
So. Let's see how gradient descent works. One of the issues we saw with gradient descent is that it can be susceptible to local optima. So when I first explained gradient descent I showed you this picture of it going downhill on the surface, and we saw how depending on where you initialize it, you can end up at different local optima. You will either wind up here or here.
### Cost Function for linear regression is a convex function
![Cost Function for linear regression is a convex function](amWiki/images/001/01-Week1/4-Parameter Learning/21_线性回归中的代价函数一定是一个凸函数.jpg)
But, it turns out that that the cost function for linear regression is always going to be a bow shaped function like this. The technical term for this is that this is called a convex function. And I'm not gonna give the formal definition for what is a convex function, C, O, N, V, E, X. But informally a convex function means a bowl shaped function and so this function doesn't have any local optima except for the one global optimum. And does gradient descent on this type of cost function which you get whenever you're using linear regression it will always converge to the global optimum. Because there are no other local optimum, global optimum.
###  Gradient Descent Algorithm in action
So now let's see this algorithm in action.As usual, here are plots of the hypothesis function and of my cost function j. And so let's say I've initialized my parameters at this value. Let's say, usually you initialize your parameters at zero, zero. Theta zero and theta equals zero. But for the demonstration, in this physical infrontation I've initialized you know, theta zero at 900 and theta one at about -0.1 okay. And so this corresponds to h(x)=-900-0.1x, [the intercept should be +900] is this line, out here on the cost function.
![Algorithm in action_initialized](amWiki/images/001/01-Week1/4-Parameter Learning/22_直观感受梯度下降算法执行过程_θ0和θ1初始化.jpg)
Now, if we take one step in gradient descent, we end up going from this point out here, over to the down and left, to that second point over there. And you notice that my line changed a little bit, and as I take another step of gradient descent, my line on the left will change.Right? And I've also moved to a new point on my cost function.And as I take further steps of gradient descent, I'm going down in cost. So my parameters and such are following this trajectory.And if you look on the left, this corresponds with hypotheses. That seem to be getting to be better and better fits to the data until eventually I've now wound up at the global minimum and this global minimum corresponds to this hypothesis, which gets me a good fit to the data.And so that's gradient descent, and we've just run it and gotten a good fit to my data set of housing prices. And you can now use it to predict, you know, if your friend has a house size 1250 square feet, you can now read off the value and tell them that I don't know maybe they could get $250,000 for their house.
![Algorithm in action](amWiki/images/001/01-Week1/4-Parameter Learning/23_直观感受梯度下降算法执行过程.jpg)
![Algorithm in action](amWiki/images/001/01-Week1/4-Parameter Learning/24_直观感受梯度下降算法执行过程.jpg)
![Algorithm in action](amWiki/images/001/01-Week1/4-Parameter Learning/25_直观感受梯度下降算法执行过程.jpg)
![Algorithm in action](amWiki/images/001/01-Week1/4-Parameter Learning/26_直观感受梯度下降算法执行过程.jpg)
![Algorithm in action](amWiki/images/001/01-Week1/4-Parameter Learning/27_直观感受梯度下降算法执行过程.jpg)
![Algorithm in action](amWiki/images/001/01-Week1/4-Parameter Learning/28_直观感受梯度下降算法执行过程.jpg)
![Algorithm in action](amWiki/images/001/01-Week1/4-Parameter Learning/29_直观感受梯度下降算法执行过程.jpg)
![Algorithm in action](amWiki/images/001/01-Week1/4-Parameter Learning/30_直观感受梯度下降算法执行过程.jpg)
### Batch gradient descent
![Batch gradient descent](amWiki/images/001/01-Week1/4-Parameter Learning/31_批量梯度下降算法.jpg)
Finally just to give this another name it turns out that the algorithm that we just went over is sometimes called batch gradient descent. And it turns out in machine learning I don't know I feel like us machine learning people were not always great at giving names to algorithms. But the term batch gradient descent refers to the fact that in every step of gradient descent, we're looking at all of the training examples. So in gradient descent, when computing the derivatives, we're computing the sums [INAUDIBLE]. So ever step of gradient descent we end up computing something like this that sums over our m training examples and so the term batch gradient descent refers to the fact that we're looking at the entire batch of training examples. And again, it's really not a great name, but this is what machine learning people call it. And it turns out that there are sometimes other versions of gradient descent that are not batch versions, but they are instead. Do not look at the entire training set but look at small subsets of the training sets at a time. And we'll talk about those versions later in this course as well. But for now using the algorithm we just learned about or using batch gradient descent you now know how to implement gradient descent for linear regression.So that's linear regression with gradient descent. If you've seen advanced linear algebra before, so some of you may have taken a class in advanced linear algebra. You might know that there exists a solution for numerically solving for the minimum of the cost function j without needing to use an iterative algorithm like gradient descent. Later in this course we'll talk about that method as well that just solves for the minimum of the cost function j without needing these multiple steps of gradient descent. That other method is called the normal equations method. But in case you've heard of that method it turns out that gradient descent will scale better to larger data sets than that normal equation method. And now that we know about gradient descent we'll be able to use it in lots of different contexts and we'll use it in lots of different machine learning problems as well.So congrats on learning about your first machine learning algorithm. We'll later have exercises in which we'll ask you to implement gradient descent and hopefully see these algorithms right for yourselves. But before that I first want to tell you in the next set of videos. The first one to tell you about a generalization of the gradient descent algorithm that will make it much more powerful. And I guess I'll tell you about that in the next video.
### Exam_in the video
![Exam_in the video](amWiki/images/001/01-Week1/4-Parameter Learning/32_视频内嵌习题.jpg)
