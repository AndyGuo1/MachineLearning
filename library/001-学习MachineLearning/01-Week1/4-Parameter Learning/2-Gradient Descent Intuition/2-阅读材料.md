# 梯度下降直观感受
## 中文
翻译暂略。以后在github发布出来，希望读者完善咯，给各位热爱学习的同学做点贡献。
## English
In this video we explored the scenario where we used one parameter θ1 and plotted its cost function to implement a gradient descent. Our formula for a single parameter was :

Repeat until convergence:  
![只有一个参数θ1下的梯度下降算法公式](amWiki/images/001/01-Week1/4-Parameter Learning/13_只有一个参数θ1下的梯度下降算法公式.jpg)  
Regardless of the slope's sign for ![梯度下降算法微分项](amWiki/images/001/01-Week1/4-Parameter Learning/14_梯度下降算法微分项.jpg), θ1 eventually converges to its minimum value. The following graph shows that when the slope is negative, the value of θ1 increases and when it is positive, the value of θ1 decreases.  
![通过例子理解梯度下降算法中的微分项](amWiki/images/001/01-Week1/4-Parameter Learning/8_通过例子理解梯度下降算法中的微分项.jpg)
On a side note, we should adjust our parameter α to ensure that the gradient descent algorithm converges in a reasonable time. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.
![理解梯度下降算法中的学习速率α](amWiki/images/001/01-Week1/4-Parameter Learning/9_理解梯度下降算法中的学习速率α.jpg)
How does gradient descent converge with a fixed step size α?

The intuition behind the convergence is that  ![梯度下降算法微分项](amWiki/images/001/01-Week1/4-Parameter Learning/14_梯度下降算法微分项.jpg) approaches 0 as we approach the bottom of our convex function. At the minimum, the derivative will always be 0 and thus we get:  
![初始值为局部最优下微分项公式](amWiki/images/001/01-Week1/4-Parameter Learning/15_初始值为局部最优下微分项公式.jpg)  
![举例说明梯度下降算法在接近局部最优点时自动减小学习速率α](amWiki/images/001/01-Week1/4-Parameter Learning/12_举例说明梯度下降算法在接近局部最优点时自动减小学习速率α.jpg)
