# 代价函数-直观感受2
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/01-Week1/3-Mode and Cost Function/4-Cost Function_3.mp4" type="video/mp4">
</video>
## 中文
## 中文
### θ0、θ1不等于0下的假设函数和代价函数
![θ0、θ1不等于0下的假设函数和代价函数](amWiki/images/001/01-Week1/3-Mode and Cost Function/17_θ0、θ1不等于0下的假设函数和代价函数.jpg)
这节课中 我们将更深入地学习代价函数的作用 这段视频的内容假设你已经认识轮廓图 如果你对轮廓图不太熟悉的话 这段视频中的某些内容你可能会听不懂 但不要紧 如果你跳过这段视频的话 也没什么关系 不听这节课对后续课程理解影响不大 和之前一样 这是我们的几个重要公式 包括了假设h、参数θ、代价函数J 以及优化目标 跟前一节视频不同的是 我还是把θ写成θ0、θ1的形式 便于这里我们要对代价函数进行的可视化
### 房价预测例子下的假设函数和代价函数
![房价预测例子下的假设函数和代价函数](amWiki/images/001/01-Week1/3-Mode and Cost Function/18_房价预测例子下的假设函数和代价函数.jpg)
和上次一样 首先来理解假设h和代价函数J 这是房价数据组成的训练集数据 让我们来构建某种假设 就像这条线一样 很显然这不是一个很好的假设 但不管怎样 如果我假设θ0等于50 θ1等于0.06的话 那么我将得到这样一个假设函数 对应于这条直线 给出θ0和θ1的值 我们要在右边画出代价函数的图像 上一次 我们是只有一个θ1 也就是说 画出的代价函数是关于θ1的函数 但现在我们有两个参数 θ0和θ1 因此图像就会复杂一些了 当只有一个参数θ1的时候 我们画出来是这样一个弓形函数
### θ0、θ1不等于0下的代价函数【3D图形】
![θ0、θ1不等于0下的代价函数【3D图形】](amWiki/images/001/01-Week1/3-Mode and Cost Function/19_θ0、θ1不等于0下的代价函数【3D图形】.jpg)
而现在我们有了两个参数 那么代价函数 仍然呈现类似的某种弓形 实际上这取决于训练样本 你可能会得到这样的图形 因此这是一个三维曲面图 两个轴分别表示θ0和θ1 随着你改变θ0和θ1的大小 你便会得到不同的代价函数 J(θ0,θ1) 对于某个特定的点 (θ0,θ1) 这个曲面的高度 也就是竖直方向的高度 就表示代价函数 J(θ0,θ1) 的值 不难发现这是一个弓形曲面 我们来看看三维图 这是这个曲面的三维图 水平轴是θ0、θ1 竖直方向表示 J(θ0,θ1) 旋转一下这个图 你就更能理解这个弓形曲面所表示的代价函数了
### θ0、θ1不等于0下的假设函数和代价函数轮廓图
![θ0、θ1不等于0下的假设函数和代价函数轮廓图】](amWiki/images/001/01-Week1/3-Mode and Cost Function/20_θ0、θ1不等于0下的假设函数和代价函数轮廓图.jpg)
在这段视频的后半部分 为了描述方便 我将不再像这样给你用三维曲面图的方式解释代价函数J 而还是用轮廓图来表示 contour plot 或 contour figure 意思一样 右边就是一个轮廓图 两个轴分别表示 θ0 和 θ1 而这些一圈一圈的椭圆形 每一个圈就表示 J(θ0,θ1) 相同的所有点的集合 具体举例来说 我们选三个点出来 这三个桃红色的点 都表示相同的 J(θ0,θ1) 的值 对吧 横纵坐标分别是θ0 θ1 这三个点的 J(θ0,θ1) 值是相同的 如果你之前没怎么接触轮廓图的话 你就这么想 你就想象一个弓形的函数从屏幕里冒出来 因此最小值 也就是这个弓形的最低点就是这个点 对吧 也就是这一系列同心椭圆的中心点 想象一下这个弓形从屏幕里冒出来 所以这些椭圆形 都从我的屏幕上冒出相同的高度 弓形的最小值点是这个位置 因此轮廓图是一种很方便的方法 能够直观地观察 代价函数J 接下来让我们看几个例子 在这里有一点 这个点表示θ0等于800 θ1大概等于-0.15 那么这个红色的点 代表了某个 (θ0,θ1) 组成的数值组 而这个点也对应于左边这样一条线 对吧 θ0等于800 也就是跟纵轴相交于大约800 斜率大概是-0.15 当然 这条线并不能很好地拟合数据 对吧 以这组 θ0 θ1 为参数的这个假设 h(x) 并不是数据的较好拟合 并且你也发现了 这个代价值 就是这里的这个值 距离最小值点还很远 也就是说这个代价值还是算比较大的 因此不能很好拟合数据
### 不同θ0、θ1取值下的假设函数与代价函数例子
让我们再来看几个例子 这是另一个假设 你不难发现 这依然不是一个好的拟合 但比刚才稍微好一点 这是我的 θ0 θ1 点 这是 θ0 的值 大约为360 θ1 的值为0 我们把它写下来 θ0=360 θ1=0 因此这组θ值对应的假设是 这条水平的直线 也就是h(x) = 360 + 0 × x 这就是假设 这个假设同样也有某个代价值 而这个代价值就对应于这个代价函数在这一点的高度
![不同θ0、θ1取值下的假设函数与代价函数例子](amWiki/images/001/01-Week1/3-Mode and Cost Function/21_不同θ0、θ1取值下的假设函数与代价函数例子.jpg)

让我们再来看一些例子 这是另一个例子 这个点这组 θ0 和 θ1 对应这样一条假设h(x) 同样地 还是对数据拟合不好 离最小值更远了
![不同θ0、θ1取值下的假设函数与代价函数例子](amWiki/images/001/01-Week1/3-Mode and Cost Function/22_不同θ0、θ1取值下的假设函数与代价函数例子.jpg)

最后一个例子 这个点其实不是最小值 但已经非常靠近最小值点了 这个点对数据的拟合就很不错 它对应这样两个θ0 和 θ1 的值 同时也对应这样一个 h(x) 这个点虽然不在最小值点 但非常接近了 因此误差平方和 或者说 训练样本和假设的距离的平方和 这个距离值的平方和 非常接近于最小值 尽管它还不是最小值
![不同θ0、θ1取值下的假设函数与代价函数例子](amWiki/images/001/01-Week1/3-Mode and Cost Function/23_不同θ0、θ1取值下的假设函数与代价函数例子.jpg)  
好的 通过这些图形 我希望你能更好地 理解这些代价函数 J 所表达的值 它们是什么样的 它们对应的假设是什么样的 以及什么样的假设对应的点 更接近于代价函数J的最小值  
当然 我们真正需要的是一种有效的算法 能够自动地找出这些使代价函数J取最小值的参数θ0和θ1来 对吧 我想我们也不希望编个程序 把这些点画出来 然后人工的方法来读出这些点的数值 这很明显不是一个好办法 事实上 我们后面就会学到 我们会遇到更复杂、更高维度、更多参数的情况 这在我们在后面的视频中很快就会遇到 而这些情况是很难画出图的 因此更无法将其可视化 因此我们真正需要的 是编写程序来找出这些最小化代价函数的θ0和θ1的值 在下一节视频中 我们将介绍一种算法 能够自动地找出能使代价函数 J 最小化的参数θ0和θ1的值
## English

### θ0、θ1不等于0下的假设函数和代价函数
![θ0、θ1不等于0下的假设函数和代价函数](amWiki/images/001/01-Week1/3-Mode and Cost Function/17_θ0、θ1不等于0下的假设函数和代价函数.jpg)  
In this video, lets delve deeper and get even better intuition about what the cost function is doing. This video assumes that you're familiar with contour plots. If you are not familiar with contour plots or contour figures some of the illustrations in this video may or may not make sense to you but is okay and if you end up skipping this video or some of it does not quite make sense because you haven't seen contour plots before. That's okay and you will still understand the rest of this course without those parts of this. Here's our problem formulation as usual, with the hypothesis parameters, cost function, and our optimization objective. Unlike before, unlike the last video, I'm going to keep both of my parameters, theta zero, and theta one, as we generate our visualizations for the cost function.   
### 房价预测例子下的假设函数和代价函数
![房价预测例子下的假设函数和代价函数](amWiki/images/001/01-Week1/3-Mode and Cost Function/18_房价预测例子下的假设函数和代价函数.jpg)
So, same as last time, we want to understand the hypothesis H and the cost function J. So, here's my training set of housing prices and let's make some hypothesis. You know, like that one, this is not a particularly good hypothesis. But, if I set theta zero=50 and theta one=0.06, then I end up with this hypothesis down here and that corresponds to that straight line. Now given these value of theta zero and theta one, we want to plot the corresponding, you know, cost function on the right. What we did last time was, right, when we only had theta one. In other words, drawing plots that look like this as a function of theta one.

### θ0、θ1不等于0下的代价函数【3D图形】
![θ0、θ1不等于0下的代价函数【3D图形】](amWiki/images/001/01-Week1/3-Mode and Cost Function/19_θ0、θ1不等于0下的代价函数【3D图形】.jpg)
But now we have two parameters, theta zero, and theta one, and so the plot gets a little more complicated. It turns out that when we have only one parameter, that the parts we drew had this sort of bow shaped function. Now, when we have two parameters, it turns out the cost function also has a similar sort of bow shape. And, in fact, depending on your training set, you might get a cost function that maybe looks something like this. So, this is a 3-D surface plot, where the axes are labeled theta zero and theta one. So as you vary theta zero and theta one, the two parameters, you get different values of the cost function J (theta zero, theta one) and the height of this surface above a particular point of theta zero, theta one. Right, that's, that's the vertical axis. The height of the surface of the points indicates the value of J of theta zero, J of theta one. And you can see it sort of has this bow like shape. Let me show you the same plot in 3D. So here's the same figure in 3D, horizontal axis theta one and vertical axis J(theta zero, theta one), and if I rotate this plot around. You kinda of a get a sense, I hope, of this bowl shaped surface as that's what the cost function J looks like.
### θ0、θ1不等于0下的假设函数和代价函数轮廓图
![θ0、θ1不等于0下的假设函数和代价函数轮廓图】](amWiki/images/001/01-Week1/3-Mode and Cost Function/20_θ0、θ1不等于0下的假设函数和代价函数轮廓图.jpg)
Now for the purpose of illustration in the rest of this video I'm not actually going to use these sort of 3D surfaces to show you the cost function J, instead I'm going to use contour plots. Or what I also call contour figures. I guess they mean the same thing. To show you these surfaces. So here's an example of a contour figure, shown on the right, where the axis are theta zero and theta one. And what each of these ovals, what each of these ellipsis shows is a set of points that takes on the same value for J(theta zero, theta one). So concretely, for example this, you'll take that point and that point and that point. All three of these points that I just drew in magenta, they have the same value for J (theta zero, theta one). Okay. Where, right, these, this is the theta zero, theta one axis but those three have the same Value for J (theta zero, theta one) and if you haven't seen contour plots much before think of, imagine if you will. A bow shaped function that's coming out of my screen. So that the minimum, so the bottom of the bow is this point right there, right? This middle, the middle of these concentric ellipses. And imagine a bow shape that sort of grows out of my screen like this, so that each of these ellipses, you know, has the same height above my screen. And the minimum with the bow, right, is right down there. And so the contour figures is a, is way to, is maybe a more convenient way to visualize my function J. [sound] So, let's look at some examples. Over here, I have a particular point, right? And so this is, with, you know, theta zero equals maybe about 800, and theta one equals maybe a -0.15 . And so this point, right, this point in red corresponds to one set of pair values of theta zero, theta one and the corresponding, in fact, to that hypothesis, right, theta zero is about 800, that is, where it intersects the vertical axis is around 800, and this is slope of about -0.15. Now this line is really not such a good fit to the data, right. This hypothesis, h(x), with these values of theta zero, theta one, it's really not such a good fit to the data. And so you find that, it's cost. Is a value that's out here that's you know pretty far from the minimum right it's pretty far this is a pretty high cost because this is just not that good a fit to the data.
### 不同θ0、θ1取值下的假设函数与代价函数例子
Let's look at some more examples. Now here's a different hypothesis that's you know still not a great fit for the data but may be slightly better so here right that's my point that those are my parameters theta zero theta one and so my theta zero value. Right? That's bout 360 and my value for theta one. Is equal to zero. So, you know, let's break it out. Let's take theta zero equals 360 theta one equals zero. And this pair of parameters corresponds to that hypothesis, corresponds to flat line, that is, h(x) equals 360 plus zero times x. So that's the hypothesis. And this hypothesis again has some cost, and that cost is, you know, plotted as the height of the J function at that point.
![不同θ0、θ1取值下的假设函数与代价函数例子](amWiki/images/001/01-Week1/3-Mode and Cost Function/21_不同θ0、θ1取值下的假设函数与代价函数例子.jpg)

Let's look at just a couple of examples. Here's one more, you know, at this value of theta zero, and at that value of theta one, we end up with this hypothesis, h(x) and again, not a great fit to the data, and is actually further away from the minimum.
![不同θ0、θ1取值下的假设函数与代价函数例子](amWiki/images/001/01-Week1/3-Mode and Cost Function/22_不同θ0、θ1取值下的假设函数与代价函数例子.jpg)

Last example, this is actually not quite at the minimum, but it's pretty close to the minimum. So this is not such a bad fit to the, to the data, where, for a particular value, of, theta zero. Which, one of them has value, as in for a particular value for theta one. We get a particular h(x). And this is, this is not quite at the minimum, but it's pretty close. And so the sum of squares errors is sum of squares distances between my, training samples and my hypothesis. Really, that's a sum of square distances, right? Of all of these errors. This is pretty close to the minimum even though it's not quite the minimum.
![不同θ0、θ1取值下的假设函数与代价函数例子](amWiki/images/001/01-Week1/3-Mode and Cost Function/23_不同θ0、θ1取值下的假设函数与代价函数例子.jpg)
So with these figures I hope that gives you a better understanding of what values of the cost function J, how they are and how that corresponds to different hypothesis and so as how better hypotheses may corresponds to points that are closer to the minimum of this cost function J. Now of course what we really want is an efficient algorithm, right, a efficient piece of software for automatically finding The value of theta zero and theta one, that minimizes the cost function J, right? And what we, what we don't wanna do is to, you know, how to write software, to plot out this point, and then try to manually read off the numbers, that this is not a good way to do it. And, in fact, we'll see it later, that when we look at more complicated examples, we'll have high dimensional figures with more parameters, that, it turns out, we'll see in a few, we'll see later in this course, examples where this figure, you know, cannot really be plotted, and this becomes much harder to visualize. And so, what we want is to have software to find the value of theta zero, theta one that minimizes this function and in the next video we start to talk about an algorithm for automatically finding that value of theta zero and theta one that minimizes the cost function J.
