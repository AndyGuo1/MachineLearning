# 代价函数-直观感受1
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/01-Week1/3-Mode and Cost Function/3-Cost Function_2.mp4" type="video/mp4">
</video>
## 中文
### 代价函数在做什么？
![代价函数在做什么](amWiki/images/001/01-Week1/3-Mode and Cost Function/9_代价函数在做什么.jpg)
在上一个视频中 我们给了代价函数一个数学上的定义 在这个视频里 让我们通过一些例子来获取一些直观的感受 看看代价函数到底是在干什么 回顾一下 这是我们上次所讲过的内容 我们想找一条直线来拟合我们的数据 所以我们用 θ0 θ1 等参数 得到了这个假设 而且通过选择不同的参数 我们会得到不同的直线拟合 所以拟合出的数据就像这样 然后我们还有一个代价函数 这就是我们的优化目标 在这个视频里 为了更好地 将代价函数可视化 我将使用一个简化的假设函数 就是右边这个函数 然后我将会用这个简化的假设 也就是 θ1*x 我们可以将这个函数看成是 把 θ0 设为0 所以我只有一个参数 也就是 θ1 代价函数看起来与之前的很像 唯一的区别是现在 h(x) 等于 θ1*x 只有一个参数 θ1 所以我的 优化目标是将 J(θ1) 最小化 用图形来表示就是 如果 θ0 等于零 也就意味这我们选择的假设函数 会经过原点 也就是经过坐标 (0,0) 通过利用简化的假设得到的代价函数 我们可以试着更好地理解
### 理解两个重要函数_假设函数和代价函数
#### θ1=1,J(1)=？
![θ1=1,J(1)的值](amWiki/images/001/01-Week1/3-Mode and Cost Function/10_θ1=1,J[1]的值.jpg)
代价函数这个概念 **我们要理解的是这两个重要的函数 第一个是假设函数 第二个是代价函数** 注意这个假设函数 h(x) 对于一个固定的 θ1 这是一个关于 x 的函数 所以这个**假设函数就是一个关于 x** 这个房子大小**的函数** 与此不同的是 **代价函数 J 是一个关于参数 θ1 的函数** 而 θ1 控制着这条直线的斜率
现在我们把这写函数都画出来 试着更好地理解它们 我们从假设函数开始 比如说这里是我的训练样本 它包含了三个点 (1,1) (2,2) 和 (3,3) 现在我们选择一个值 θ1 所以当 θ1 等于1 如果这是我选择的 θ1 那么我的假设函数看起来就会像是这条直线 我将要指出的是 当我描绘出我的假设函数 X轴 我的横轴被标定为X轴 X轴是表示房子大小的量 现在暂时把 θ1 定为1 我想要做的就是 算出在 θ1 等于 1 的时候 J(θ1) 等于多少 所以我们 按照这个思路来计算代价函数的大小 和之前一样 代价函数定义如下 是吧 对这个误差平方项进行求和 这就等于 这样一个形式 简化以后就等于 三个0的平方和 当然还是0 现在 在代价函数里 我们发现所有这些值都等于0 因为对于我所选定的这三个训练样本 ( 1 ,1 ) (2,2) 和 (3,3) 如果 θ1 等于 1 那么 h(x(i)) 就会正好等于 y(i) 让我把这个写得好一点 对吧 所以 h(x) - y 所有的这些值都会等于零 这也就是为什么 J(1) 等于零 所以 我们现在知道了 J(1) 是0 让我把这个画出来 我将要在屏幕右边画出我的代价函数 J 要注意的是 因为我的代价函数是关于参数 θ1 的函数 当我描绘我的代价函数时 X轴就是 θ1 现在我有 J(1) 等于零 让我们继续把函数画出来 结果我们会得到这样一个点
#### θ1=0.5,J(0.5)=？
![θ1=0.5,J(0.5)的值](amWiki/images/001/01-Week1/3-Mode and Cost Function/11_θ1=0.5,J[0.5]的值.jpg)
现在我们来看其它一些样本 θ1 可以被设定为 某个范围内各种可能的取值 所以 θ1 可以取负数 0 或者正数 所以如果 θ1 等于0.5会发生什么呢 继续把它画出来 现在要把 θ1 设为0.5 在这个条件下 我的假设函数看起来就是这样 这条线的斜率等于0.5 现在让我们计算 J(0.5) 所以这将会等于1除以2m 乘以那一块 其实我们不难发现后面的求和 就是这条线段的高度的平方 加上这条线段高度的平方 再加上这条线段高度的平方 三者求和 对吗？ 就是 y(i) 与预测值 h(x(i)) 的差 对吗 所以第一个样本将会是0.5减去1的平方 因为我的假设函数预测的值是0.5 而实际值则是1 第二个样本 我得到的是1减去2的平方 因为我的假设函数预测的值是1 但是实际房价是2 最后 加上 1.5减去3的平方 那么这就等于1除以2乘以3 因为训练样本有三个点所以 m 等于3 对吧 然后乘以括号里的内容 简化后就是3.5 所以这就等于3.5除以6 也就约等于0.68 让我们把这个点画出来 不好意思 有一个计算错误 这实际上该是0.58 所以我们把点画出来 大约会是在这里 对吗
#### 视频内嵌习题_求θ1=0的代价函数值J(0)?
现在 让我们再多做一个点 让我们试试θ1等于0 J(0) 会等于多少呢
![视频内嵌习题_求θ1=0的代价函数值J(0)](amWiki/images/001/01-Week1/3-Mode and Cost Function/12_视频内嵌习题_求θ1=0的代价函数值J[0].jpg)
如果θ1等于0 那么 h(x) 就会等于一条水平的线 对了 就会像这样是水平的 所以 测出这些误差 我们将会得到 J(0) 等于 1除以 2m 乘以1的平方 加上2的平方 加上3的平方 也就是 1除以6乘以14 也就是2.3左右 所以让我们接着把这个点也画出来 所以这个点最后是2.3
### 假设函数与代价函数绘图
![假设函数与代价函数绘图](amWiki/images/001/01-Week1/3-Mode and Cost Function/13_假设函数与代价函数绘图.jpg)
当然我们可以接着设定 θ1 等于别的值 进行计算 你也可以把 θ1 设定成一个负数 所以如果 θ1 是负数 那么 h(x) 将会等于 打个比方说 －0.5 乘以x 然后 θ1 就是 -0.5 那么这将会 对应着一个斜率为-0.5的假设函数 而且你可以 继续计算这些误差 结果你会发现 对于0.5 结果会是非常大的误差 最后会得到一个较大的数值 类似于5.25 等等 对于不同的 θ1 你可以计算出这些对应的值 对吗 结果你会发现 你算出来的这些值 你得到一条这样的曲线 通过计算这些值 你可以慢慢地得到这条线 这就是 J(θ) 的样子了 我们来回顾一下 **任何一个 θ1 的取值对应着一个不同的 假设函数 或者说对应着左边一条不同的拟合直线 对于任意的θ1 你可以算出一个不同的 J(θ1) 的取值** 举个例子 你知道的 θ1 等于1时对应着穿过这些数据的这条直线 当 θ1 等于0.5 也就是这个玫红色的点 也许对应着这条线 然后 θ1 等于0 也就是蓝色的这个点 对应着 这条水平的线 对吧 所以对于任意一个 θ1 的取值 我们会得到 一个不同的 J(θ1) 而且我们可以利用这些来描出右边的这条曲线 现在你还记得 学习算法的优化目标 是我们想找到一个 θ1 的值 来将 J(θ1) 最小化 对吗 这是我们线性回归的目标函数 嗯 看这条曲线 让 J(θ1) 最小化的值 是 θ1 等于1 然后你看 这个确实就对应着最佳的通过了数据点的拟合直线 这条直线就是由 θ1=1 的设定而得到的 然后 对于这个特定的训练样本 我们最后能够完美地拟合 这就是为什么最小化 J(θ1) 对应着寻找一个最佳拟合直线的目标 总结一下 在这个视频里 我们看到了一些图形 来理解代价函数 要做到这个 我们简化了算法 让这个函数只有一个参数 θ1 也就是说我们把 θ0 设定为0 在下一个视频里 我们将回到原来的问题的公式 然后看一些 带有 θ0 和 θ1 的图形 也就是说不把 θ0 设置为0了 希望这会让你更好地理解在原来的线性回归公式里 代价函数 J 的意义
## English
### What the cost function is doing?
![What the cost function is doing](amWiki/images/001/01-Week1/3-Mode and Cost Function/9_代价函数在做什么.jpg)
In the previous video, we gave the mathematical definition of the cost function. In this video, let's look at some examples, to get back to intuition about what the cost function is doing, and why we want to use it. To recap, here's what we had last time. We want to fit a straight line to our data, so we had this formed as a hypothesis with these parameters theta zero and theta one, and with different choices of the parameters we end up with different straight line fits. So the data which are fit like so, and there's a cost function, and that was our optimization objective. [sound] So this video, in order to better visualize the cost function J, I'm going to work with a simplified hypothesis function, like that shown on the right. So I'm gonna use my simplified hypothesis, which is just theta one times X. We can, if you want, think of this as setting the parameter theta zero equal to 0. So I have only one parameter theta one and my cost function is similar to before except that now H of X that is now equal to just theta one times X. And I have only one parameter theta one and so my optimization objective is to minimize j of theta one. In pictures what this means is that if theta zero equals zero that corresponds to choosing only hypothesis functions that pass through the origin, that pass through the point (0, 0). Using this simplified definition of a hypothesizing cost function let's try to understand the cost function concept better. It turns out that two key functions we want to understand. The first is the hypothesis function, and the second is a cost function. So, notice that the hypothesis, right, H of X. For a face value of theta one, this is a function of X. So the hypothesis is a function of, what is the size of the house X. In contrast, the cost function, J, that's a function of the parameter, theta one, which controls the slope of the straight line. Let's plot these functions and try to understand them both better.
### Understand two functions_Hypothesis Function and Cost Function
#### θ1=1,J(1)=？
![θ1=1,J(1)'s value](amWiki/images/001/01-Week1/3-Mode and Cost Function/10_θ1=1,J[1]的值.jpg)
Let's start with the hypothesis. On the left, let's say here's my training set with three points at (1, 1), (2, 2), and (3, 3). Let's pick a value theta one, so when theta one equals one, and if that's my choice for theta one, then my hypothesis is going to look like this straight line over here. And I'm gonna point out, when I'm plotting my hypothesis function. X-axis, my horizontal axis is labeled X, is labeled you know, size of the house over here. Now, of temporary, set theta one equals one, what I want to do is figure out what is j of theta one, when theta one equals one. So let's go ahead and compute what the cost function has for. You'll devalue one. Well, as usual, my cost function is defined as follows, right? Some from, some of 'em are training sets of this usual squared error term. And, this is therefore equal to. And this. Of theta one x I minus y I and if you simplify this turns out to be. That. Zero Squared to zero squared to zero squared which is of course, just equal to zero. Now, inside the cost function. It turns out each of these terms here is equal to zero. Because for the specific training set I have or my 3 training examples are (1, 1), (2, 2), (3,3). If theta one is equal to one. Then h of x. H of x i. Is equal to y I exactly, let me write this better. Right? And so, h of x minus y, each of these terms is equal to zero, which is why I find that j of one is equal to zero. So, we now know that j of one Is equal to zero. Let's plot that. What I'm gonna do on the right is plot my cost function j. And notice, because my cost function is a function of my parameter theta one, when I plot my cost function, the horizontal axis is now labeled with theta one. So I have j of one zero zero so let's go ahead and plot that. End up with. An X over there.
#### θ1=0.5,J(0.5)=？
![θ1=0.5,J(0.5)'s value](amWiki/images/001/01-Week1/3-Mode and Cost Function/11_θ1=0.5,J[0.5]的值.jpg)
Now lets look at some other examples. Theta-1 can take on a range of different values. Right? So theta-1 can take on the negative values, zero, positive values. So what if theta-1 is equal to 0.5. What happens then? Let's go ahead and plot that. I'm now going to set theta-1 equals 0.5, and in that case my hypothesis now looks like this. As a line with slope equals to 0.5, and, lets compute J, of 0.5. So that is going to be one over 2M of, my usual cost function. It turns out that the cost function is going to be the sum of square values of the height of this line. Plus the sum of square of the height of that line, plus the sum of square of the height of that line, right? ?Cause just this vertical distance, that's the difference between, you know, Y. I. and the predicted value, H of XI, right? So the first example is going to be 0.5 minus one squared. Because my hypothesis predicted 0.5. Whereas, the actual value was one. For my second example, I get, one minus two squared, because my hypothesis predicted one, but the actual housing price was two. And then finally, plus. 1.5 minus three squared. And so that's equal to one over two times three. Because, M when trading set size, right, have three training examples. In that, that's times simplifying for the parentheses it's 3.5. So that's 3.5 over six which is about 0.68. So now we know that j of 0.5 is about 0.68.[Should be 0.58] Lets go and plot that. Oh excuse me, math error, it's actually 0.58. So we plot that which is maybe about over there. Okay?
#### θ1=0,J(0)=？
Now, let's do one more. How about if theta one is equal to zero, what is J of zero equal to?
![θ1=0,J(0)'s value](amWiki/images/001/01-Week1/3-Mode and Cost Function/12_视频内嵌习题_求θ1=0的代价函数值J[0].jpg)
It turns out that if theta one is equal to zero, then H of X is just equal to, you know, this flat line, right, that just goes horizontally like this. And so, measuring the errors. We have that J of zero is equal to one over two M, times one squared plus two squared plus three squared, which is, One six times fourteen which is about 2.3. So let's go ahead and plot as well. So it ends up with a value around 2.3 and of course we can keep on doing this for other values of theta one.
### Plot The Hypothesis Function and Cost Function
![Plot The Hypothesis Function and Cost Function](amWiki/images/001/01-Week1/3-Mode and Cost Function/13_假设函数与代价函数绘图.jpg)
It turns out that you can have you know negative values of theta one as well so if theta one is negative then h of x would be equal to say minus 0.5 times x then theta one is minus 0.5 and so that corresponds to a hypothesis with a slope of negative 0.5. And you can actually keep on computing these errors. This turns out to be, you know, for 0.5, it turns out to have really high error. It works out to be something, like, 5.25. And so on, and the different values of theta one, you can compute these things, right? And it turns out that you, your computed range of values, you get something like that. And by computing the range of values, you can actually slowly create out. What does function J of Theta say and that's what J of Theta is. To recap, for each value of theta one, right? Each value of theta one corresponds to a different hypothesis, or to a different straight line fit on the left. And for each value of theta one, we could then derive a different value of j of theta one. And for example, you know, theta one=1, corresponded to this straight line straight through the data. Whereas theta one=0.5. And this point shown in magenta corresponded to maybe that line, and theta one=zero which is shown in blue that corresponds to this horizontal line. Right, so for each value of theta one we wound up with a different value of J of theta one and we could then use this to trace out this plot on the right. Now you remember, the optimization objective for our learning algorithm is we want to choose the value of theta one. That minimizes J of theta one. Right? This was our objective function for the linear regression. Well, looking at this curve, the value that minimizes j of theta one is, you know, theta one equals to one. And low and behold, that is indeed the best possible straight line fit through our data, by setting theta one equals one. And just, for this particular training set, we actually end up fitting it perfectly. And that's why minimizing j of theta one corresponds to finding a straight line that fits the data well. So, to wrap up. In this video, we looked up some plots. To understand the cost function. To do so, we simplify the algorithm. So that it only had one parameter theta one. And we set the parameter theta zero to be only zero. In the next video. We'll go back to the original problem formulation and look at some visualizations involving both theta zero and theta one. That is without setting theta zero to zero. And hopefully that will give you, an even better sense of what the cost function j is doing in the original linear regression formulation.
