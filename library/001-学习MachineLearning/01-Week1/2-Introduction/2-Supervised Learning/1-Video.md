# 监督学习
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/01-Week1/2-Introduciton/3-Supervised Learning.mp4" type="video/mp4">
</video>
## 中文
在本视频中，我将介绍一种也许是最常见的机器学习问题。 即监督学习。后面将给出监督学习更正式的定义， 现在最好以示例来说明什么是监督学习。 之后再给出正式的定义。
### 预测房价
![预测房价](amWiki/images/001/01-Week1/2-Introduciton/2-房价预测_线性回归问题.png)
假设你想预测房价（无比需要啊！）， 之前，某学生已经从某地收集了数据集（不是中国的，囧） 其中一个数据集是这样的。 这是横坐标，即不同房子的面积，单位平方脚（^-^） 纵轴上是房价，单位 千美元。 根据给定数据，假设你朋友有栋房子，750平尺（70平米） 想知道这房子能卖多少，好卖掉。 那么，学习算法怎么帮你呢？学习算法可以： 绘出一条直线，让直线尽可能匹配到所有数据。 基于此，看上去，那个房子应该、可能、也许、大概 卖到15万美元（一平米两千刀！）。但这不是唯一的学习算法。 可能还有更好的。比如不用直线了， 可能平方函数会更好， 即二次多项式更符合数据集。如果你这样做， 预测结果就应该是20万刀（一平三千刀，涨价好快）。 后面我们会介绍到如何选择 是选择直线还是平方函数来拟合。 没有明确的选择，就不知哪个能给你的朋友 更好的卖房建议。只是这些每个都是很好的学习算法例子。 也是监督学习的例子。 **术语监督学习，意指给出一个算法， 需要部分数据集已经有正确答案。**比如给定房价数据集， 对于里面每个数据，算法都知道对应的正确房价， 即这房子实际卖出的价格。算法的结果就是 算出更多的正确价格，比如那个新房子， 你朋友想卖的那个。** 用更术语的方式来定义， 监督学习又叫回归问题，（应该是回归属于监督中的一种） 意指要预测一个连续值的输出，比如房价。** 虽然从技术上，一般把房价记到美分单位。 所以实际还是个离散值，但通常把它看作实际数字， 是一个标量值，一个连续值的数，而术语回归， 意味着要预测这类连续值属性的种类。
### 预测胸部肿瘤是恶性良性
![预测肿瘤](amWiki/images/001/01-Week1/2-Introduciton/3-肿瘤预测_分类问题.png)
另一个监督学习的例子，我和一些朋友 之前研究的领域。让我们来看医学记录， 并预测胸部肿瘤是恶性良性。 如果某人发现有胸部肿瘤，恶性肿瘤有害又危险， 良性肿瘤则是少害。 显然人们很关注这个。让我们看一个收集好的数据集， 假设在数据集中，横轴表示肿瘤的大小， 纵轴我打算圈上0或1，是或否， 即肿瘤是恶性的还是良性的。 所以如图所示，可以看到这个大小的肿瘤块 是良性的，还有这些大小的都是良性的。 不幸地是也看到一些恶性肿瘤，比如这些大小的肿瘤。 所以，有5个良性块，在这一块， 还有5个恶性的，它们纵轴值为1. 现在假设某人杯具地得胸部肿瘤了， 大小大概是这么大。 对应的机器学习问题就是，你能否估算出一个概率， 即肿瘤为恶或为良的概率？ 专业地说，这是个分类问题。 分类是要预测一个离散值输出。 这里是0或1，恶性或良性。事实证明， 在分类问题中，有时会有超过两个的值， 输出的值可能超过两种。举个具体例子， 胸部肿瘤可能有三种类型，所以要预测离散值0，1，2，3 0就是良性肿瘤，没有癌症。 1 表示1号癌症，假设总共有三种癌症。 2 是2号癌症，3 就是3号癌症。 这同样是个分类问题，因为它的输出的离散值集合 分别对应于无癌，1号，2号，3号癌症 我再露一小手，在分类问题中，还有另一种作图方式 来描述数据。我画你猜。要用到些许不同的符号集合 来描绘数据。如果肿瘤大小作为唯一属性， 被用于预测恶性良性，可以把数据作图成这样。 使用不同的符号来表示良性和 恶性，即阴性和阳性。所以，不再统一画叉叉了， 改用圈圈来代表良性肿瘤，就像这样。 仍沿用X（叉叉）代表恶性肿瘤。希望你能明白。 我所做的就是，把在上面的数据， 映射下来。再用不同的符号， 圈和叉来分别代表良性和恶性。
### 预测胸部肿瘤是恶性良性[多参数]
![预测肿瘤扩展](amWiki/images/001/01-Week1/2-Introduciton/4-肿瘤预测_多个变量.png)
在上例中，只使用了一个特征属性，即肿瘤块大小， 来预测肿瘤是恶性良性。在其它机器学习问题里， 有着不只一个的特征和属性。 例子，现在不只是知道肿瘤大小， 病人年龄和肿瘤大小都知道了。这种情况下， 数据集如表图所示，有些病人，年龄、肿瘤已知， 不同的病人，会有一点不一样， 肿瘤恶性，则用叉来代表。所以，假设 有一朋友得了肿瘤。肿瘤大小和年龄 落在此处。那么依据这个给定的数据集，学习算法 所做的就是画一条直线，分开 恶性肿瘤和良性肿瘤，所以学习算法会 画条直线，像这样，把两类肿瘤分开。 然后你就能判断你朋友的肿瘤是...了 如果它在那边，学习算法就说 你朋友的肿瘤在良性一边，因此更可能 是良性的。好，本例中，总共有两个特征， 即病人年龄和肿瘤大小。在别的ML问题中， 经常会用到更多特征，我朋友研究这个问题时， 通常使用这些特征：比如块的厚度，即胸部肿瘤的厚度 肿瘤细胞大小和形状的一致性， 等等。它表明， 最有趣的学习算法（本课中将学到） 能够处理，无穷多个特征。不是3到5个这么少。 在这张幻灯片中，我已经列举了总共5个不同的特征。 但对于一些学习问题， 真要用到的不只是三五个特征， 要用到无数多个特征，非常多的属性， 所以，你的学习算法要使用很多的属性 或特征、线索来进行预测。那么，你如何处理 无限多特征呢？甚至你如何存储无数的东西 进电脑里，又要避免内存不足？ 事实上，等我们介绍一种叫支持向量机的算法时， 就知道存在一个简洁的数学方法，能让电脑处理无限多的特征。 想像下，我不是这边写两个特征， 右边写三个特征。而是，写一个无限长的特征表， 不停地写特征，似乎是个无限长的特征的表。 但是，我们也有能力设计一个算法来处理这个问题。 所以再从头复述一遍。本课中，我们介绍**监督学习。 其基本思想是，监督学习中，对于数据集中的每个数据， 都有相应的正确答案，**（训练集） 算法就是基于这些来做出预测。就像那个房价， 或肿瘤的性质。后面介绍了回归问题。 即通过回归来预测一个连续值输出。 我们还谈到了分类问题， 目标是预测离散值输出。
### 内嵌习题_区分回归问题_分类问题   
下面是个小测验题目： 假设你有家公司，希望研究相应的学习算法去 解决两个问题。第一个问题，你有一堆货物的清单。 假设一些货物有几千件可卖， 你想预测出，你能在未来三个月卖出多少货物。 第二个问题，你有很多用户， 你打算写程序来检查每个用户的帐目。 对每个用户的帐目， 判断这个帐目是否被黑过（hacked or compromised）。 请问，这两个问题是分类问题，还是回归问题？ 当视频暂停时，请用你的鼠标进行选择， 四选一，选择你认为正确的答案。 好，希望你刚才答对了。问题一是个回归问题 因为如果我有几千件货物， 可能只好把它当作一个实际的值，一个连续的值。 也把卖出的数量当作连续值。 第二个问题，则是分类问题，因为可以把 我想预测的一个值设为0，来表示账目没有被hacked 另一个设为1，表示已经被hacked。 就像乳癌例子中，0表示良性，1表示恶性。 所以这个值为0或1，取决于是否被hacked， 有算法能预测出是这两个离散值中的哪个。 因为只有少量的离散值，所以这个就是个分类问题。 这就是监督学习，下个视频将会介绍 无监督学习，学习算法的另一主要类型。
![内嵌习题](amWiki/images/001/01-Week1/2-Introduciton/5-内嵌习题_区分回归问题_分类问题.png)


## English
### Predict housing prices
![Predict housing prices](amWiki/images/001/01-Week1/2-Introduciton/2-房价预测_线性回归问题.png)
In this video I am going to define what is probably the most common type of machine learning problem, which is supervised learning. I'll define supervised learning more formally later, but it's probably best to explain or start with an example of what it is and we'll do the formal definition later. Let's say you want to predict housing prices. A while back, a student collected data sets from the Institute of Portland Oregon. And let's say you plot a data set and it looks like this. Here on the horizontal axis, the size of different houses in square feet, and on the vertical axis, the price of different houses in thousands of dollars. So. Given this data, let's say you have a friend who owns a house that is, say 750 square feet and hoping to sell the house and they want to know how much they can get for the house. So how can the learning algorithm help you? One thing a learning algorithm might be able to do is put a straight line through the data or to fit a straight line to the data and, based on that, it looks like maybe the house can be sold for maybe about $150,000. But maybe this isn't the only learning algorithm you can use. There might be a better one. For example, instead of sending a straight line to the data, we might decide that it's better to fit a quadratic function or a second-order polynomial to this data. And if you do that, and make a prediction here, then it looks like, well, maybe we can sell the house for closer to $200,000. One of the things we'll talk about later is how to choose and how to decide do you want to fit a straight line to the data or do you want to fit the quadratic function to the data and there's no fair picking whichever one gives your friend the better house to sell. But each of these would be a fine example of a learning algorithm. So this is an example of a supervised learning algorithm. **And the term supervised learning refers to the fact that we gave the algorithm a data set in which the "right answers" were given.** That is, we gave it a data set of houses in which for every example in this data set, we told it what is the right price so what is the actual price that, that house sold for and the toss of the algorithm was to just produce more of these right answers such as for this new house, you know, that your friend may be trying to sell. **To define with a bit more terminology this is also called a regression problem and by regression problem I mean we're trying to predict a continuous value output. **Namely the price. So technically I guess prices can be rounded off to the nearest cent. So maybe prices are actually discrete values, but usually we think of the price of a house as a real number, as a scalar value, as a continuous value number and the term regression refers to the fact that we're trying to predict the sort of continuous values attribute.
### Predict breast cancer  
![Predict breast cancer](amWiki/images/001/01-Week1/2-Introduciton/3-肿瘤预测_分类问题.png)
 Here's another supervised learning example, some friends and I were actually working on this earlier. Let's see you want to look at medical records and try to predict of a breast cancer as malignant or benign. If someone discovers a breast tumor, a lump in their breast, a malignant tumor is a tumor that is harmful and dangerous and a benign tumor is a tumor that is harmless. So obviously people care a lot about this. Let's see a collected data set and suppose in your data set you have on your horizontal axis the size of the tumor and on the vertical axis I'm going to plot one or zero, yes or no, whether or not these are examples of tumors we've seen before are malignantâwhich is oneâor zero if not malignant or benign. So let's say our data set looks like this where we saw a tumor of this size that turned out to be benign. One of this size, one of this size. And so on. And sadly we also saw a few malignant tumors, one of that size, one of that size, one of that size... So on. So this example... I have five examples of benign tumors shown down here, and five examples of malignant tumors shown with a vertical axis value of one. And let's say we have a friend who tragically has a breast tumor, and let's say her breast tumor size is maybe somewhere around this value. The machine learning question is, can you estimate what is the probability, what is the chance that a tumor is malignant versus benign? To introduce a bit more terminology this is an example of a classification problem. The term classification refers to the fact that here we're trying to predict a discrete value output: zero or one, malignant or benign. And it turns out that in classification problems sometimes you can have more than two values for the two possible values for the output. As a concrete example maybe there are three types of breast cancers and so you may try to predict the discrete value of zero, one, two, or three with zero being benign. Benign tumor, so no cancer. And one may mean, type one cancer, like, you have three types of cancer, whatever type one means. And two may mean a second type of cancer, a three may mean a third type of cancer. But this would also be a classification problem, because this other discrete value set of output corresponding to, you know, no cancer, or cancer type one, or cancer type two, or cancer type three. In classification problems there is another way to plot this data. Let me show you what I mean. Let me use a slightly different set of symbols to plot this data. So if tumor size is going to be the attribute that I'm going to use to predict malignancy or benignness, I can also draw my data like this. I'm going to use different symbols to denote my benign and malignant, or my negative and positive examples. So instead of drawing crosses, I'm now going to draw O's for the benign tumors. Like so. And I'm going to keep using X's to denote my malignant tumors. Okay? I hope this is beginning to make sense. All I did was I took, you know, these, my data set on top and I just mapped it down. To this real line like so. And started to use different symbols, circles and crosses, to denote malignant versus benign examples. Now, in this example we use only one feature or one attribute, mainly, the tumor size in order to predict whether the tumor is malignant or benign.
 ### Predict breast cancer [Muti_Features]
![Predict breast cancer [Muti_Features]](amWiki/images/001/01-Week1/2-Introduciton/4-肿瘤预测_多个变量.png)
 In other machine learning problems when we have more than one feature, more than one attribute. Here's an example. Let's say that instead of just knowing the tumor size, we know both the age of the patients and the tumor size. In that case maybe your data set will look like this where I may have a set of patients with those ages and that tumor size and they look like this. And a different set of patients, they look a little different, whose tumors turn out to be malignant, as denoted by the crosses. So, let's say you have a friend who tragically has a tumor. And maybe, their tumor size and age falls around there. So given a data set like this, what the learning algorithm might do is throw the straight line through the data to try to separate out the malignant tumors from the benign ones and, so the learning algorithm may decide to throw the straight line like that to separate out the two classes of tumors. And. You know, with this, hopefully you can decide that your friend's tumor is more likely to if it's over there, that hopefully your learning algorithm will say that your friend's tumor falls on this benign side and is therefore more likely to be benign than malignant. In this example we had two features, namely, the age of the patient and the size of the tumor. In other machine learning problems we will often have more features, and my friends that work on this problem, they actually use other features like these, which is clump thickness, the clump thickness of the breast tumor. Uniformity of cell size of the tumor. Uniformity of cell shape of the tumor, and so on, and other features as well. And it turns out one of the interes-, most interesting learning algorithms that we'll see in this class is a learning algorithm that can deal with, not just two or three or five features, but an infinite number of features. On this slide, I've listed a total of five different features. Right, two on the axes and three more up here. But it turns out that for some learning problems, what you really want is not to use, like, three or five features. But instead, you want to use an infinite number of features, an infinite number of attributes, so that your learning algorithm has lots of attributes or features or cues with which to make those predictions. So how do you deal with an infinite number of features. How do you even store an infinite number of things on the computer when your computer is gonna run out of memory. It turns out that when we talk about an algorithm called the Support Vector Machine, there will be a neat mathematical trick that will allow a computer to deal with an infinite number of features. Imagine that I didn't just write down two features here and three features on the right. But, imagine that I wrote down an infinitely long list, I just kept writing more and more and more features. Like an infinitely long list of features. Turns out, we'll be able to come up with an algorithm that can deal with that. So, just to recap. In this class we'll talk about supervised learning. **And the idea is that, in supervised learning, in every example in our data set, we are told what is the "correct answer"** that we would have quite liked the algorithms have predicted on that example. Such as the price of the house, or whether a tumor is malignant or benign. We also talked about the regression problem. And by regression, that means that our goal is to predict a continuous valued output. And we talked about the classification problem, where the goal is to predict a discrete value output.  
###  Exam about supervised learning two types[regression/classification]
![内嵌习题](amWiki/images/001/01-Week1/2-Introduciton/5-内嵌习题_区分回归问题_分类问题.png)
  Just a quick wrap up question: Suppose you're running a company and you want to develop learning algorithms to address each of two problems. In the first problem, you have a large inventory of identical items. So imagine that you have thousands of copies of some identical items to sell and you want to predict how many of these items you sell within the next three months. In the second problem, problem two, you'd like-- you have lots of users and you want to write software to examine each individual of your customer's accounts, so each one of your customer's accounts; and for each account, decide whether or not the account has been hacked or compromised. So, for each of these problems, should they be treated as a classification problem, or as a regression problem? When the video pauses, please use your mouse to select whichever of these four options on the left you think is the correct answer. So hopefully, you got that this is the answer. For problem one, I would treat this as a regression problem, because if I have, you know, thousands of items, well, I would probably just treat this as a real value, as a continuous value. And treat, therefore, the number of items I sell, as a continuous value. And for the second problem, I would treat that as a classification problem, because I might say, set the value I want to predict with zero, to denote the account has not been hacked. And set the value one to denote an account that has been hacked into. So just like, you know, breast cancer, is, zero is benign, one is malignant. So I might set this be zero or one depending on whether it's been hacked, and have an algorithm try to predict each one of these two discrete values. And because there's a small number of discrete values, I would therefore treat it as a classification problem. So, that's it for supervised learning and in the next video I'll talk about unsupervised learning, which is the other major category of learning algorithms.
