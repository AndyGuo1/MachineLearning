# 分类问题_假设函数表达(逻辑回归)
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/03-Week3/1-Logistic Regression/2-Hypothesis Representation.mp4" type="video/mp4">
</video>
## 中文
### 分类问题逻辑回归模型表达
![分类问题逻辑回归模型表达](amWiki/images/001/03-Week3/1-Logistic Regression/6-分类问题逻辑回归模型表达.jpg)  
让我们开始谈谈逻辑回归 在这段视频中 我要给你展示假设函数的表达式 也就是说 在分类问题中 要用什么样的函数来表示我们的假设 此前我们说过 希望我们的分类器 的输出值在0和1之间 因此 我们 希望想出一个 满足某个性质的假设函数 这个性质是它的预测值要在0和1之间 当我们使用线性回归的时候 这是一种假设函数的形式 其中 h(x) 等于 θ 的转置乘以 x 对于逻辑回归来说 我要把这个稍微改一下 把假设函数改成 g(θ 的转置乘以 x) 其中 我将定义 函数g如下： 当z是一个实数时 g(z)=1/(1+e^(-z)) g(z)=1/(1+e^(-z)) g(z)=1/(1+e^(-z)) 这称为 S 型函数 (sigmoid function) 或逻辑函数 逻辑函数这个词 就是逻辑回归名字的由来 顺便说一下 S型函数和逻辑函数 基本上是同义词 意思是一样的 因此 这两个术语 基本上是可互换的 哪一个术语都可以 用来表示这个函数 g 如果我们 把这两个方程 合并到一起 这是我的假设 的另一种写法 也就是说 h(x)=1/(1+e^(-θ 转置乘以 x)) h(x)=1/(1+e^(-θ 转置乘以 x)) 我所做的是 把这个变量 z 这里 z 是一个实数 把 θ 的转置乘以 x 代入到这里 所以最后得到的是 θ 转置乘以 x 代替了这里的 z 最后 我们看一下S型函数是什么样的 我们在这儿绘制这个图形 S型函数 g(z) 也称为逻辑函数 看起来是这样的 它开始接近0 然后上升 直到在原点处达到0.5 然后它再次变平 像这样 所以这就是S型函数的样子 而且你注意S型函数 而且你注意S型函数 它渐近于1 然后随着横坐标 的反方向趋向于0 随着 z 趋于负无穷 随着 z 趋于负无穷 g(z) 趋近于零 随着 z 趋于正无穷 g(z) 趋近于1 因为 g(z) 的取值 因为 g(z) 的取值 在0和1之间 我们就得到 h(x) 的值 必在0和1之间 最后 有了这个假设函数 我们需要做的是 和之前一样 用参数θ拟合我们的数据 所以拿到一个训练集 我们需要给参数 θ 选定一个值 我们需要给参数 θ 选定一个值 然后用这个假设函数做出预测
### 分类问题的假设函数输出解释
![分类问题的假设函数输出解释](amWiki/images/001/03-Week3/1-Logistic Regression/7-分类问题的假设函数输出解释.jpg)  
稍后我们将讨论一个 用来拟合参数θ的学习算法 但是首先让我们讨论 一下这个模型的解释 这就是我对 假设函数 h(x) 的输出的解释 假设函数 h(x) 的输出的解释 当我的假设函数 输出某个数 我会认为这个数是 对于新输入样本 x 的 y 等于1的概率的估计值 我的意思是这样的 下面举个例子 比方说 我们来看肿瘤分类的例子 我们有一个特征向量 x 和平时一样 x0 等于 1 然后我们的特征变量 x1 是肿瘤的大小 假设我有一个病人来了 而且知道肿瘤的大小 而且知道肿瘤的大小 把他们的特征向量 x 代入我的假设函数 假如假设函数的输出为0.7 我将解释 我的假设如下 我要说 这个 假设告诉我 对于一个特征为 x 的患者 对于一个特征为 x 的患者 y 等于 1 的概率是0.7 换句话说 我要告诉我的病人 非常遗憾 肿瘤是恶性的可能性是70％或者说0.7 要更加正式的写出来 或者说写成数学表达式 我的假设函数等于 我的假设函数等于 P(y=1|x;θ) P(y=1|x;θ) P(y=1|x;θ) 对于熟悉概率的人 应该能看懂这个式子 如果你不太熟悉概率 可以这么看这个表达式 可以这么看这个表达式 在给定 x 的条件下 y=1 的概率 给定的 x 就是我的病人的特征 x 给定的 x 就是我的病人的特征 x 特征 x 代表了 我的病人特定的肿瘤大小 这个概率的参数是 θ 这个概率的参数是 θ 所以 我基本上可以认为 假设函数给出的估计 是 y=1 的概率 是 y=1 的概率 现在 因为这是一个 分类的任务 我们知道 y 必须是0或1 对不对？ 它们是 y 可能取到的 仅有的两个值 无论是在训练集中 或是对走进我的办公室 或在未来进入医生办公室的新患者 因此 有了 h(x) 我们也可以计算 y=0 的概率 具体地说 因为 y 必须是0或1 我们知道 y=0 的概率 加上 y=1 的概率 必须等于1 这第一个方程看起来 有点复杂 基本上就是说 给定参数 θ 对某个特征为 x 的病人 y=0 的概率 和给定参数 θ 时 对同一个特征为 x 的病人 y=1 的概率相加 必须等于1 如果觉得这个方程看到起来有点儿复杂 可以想象它没有 x 和 θ 这就是说 y=0 的概率 加上 y=1 的概率必须等于1 我们知道这是肯定的 因为 y 要么是0 要么是1 所以 y=0 的可能性 和 y=1 的可能性 它们俩相加肯定等于1 所以 如果你只是 把这一项 移到右边 你就会得到这个等式 就是说 y=0 的概率 等于1减去 y=1 的概率 因此 我们的 假设函数 h(x) 给出的是这一项 你可以简单地计算出这个概率 你可以简单地计算出这个概率 计算出 y=0 的概率的估计值 计算出 y=0 的概率的估计值 所以 你现在知道 逻辑回归的假设函数的表达式是什么 逻辑回归的假设函数的表达式是什么 我们看到了定义逻辑回归的 假设函数的数学公式 在接下来的视频中 我想试着让你 对假设函数是什么样子 有一个更直观的认识 我想告诉你 一个被称为判定边界 (decision) 的东西 一个被称为判定边界 (decision) 的东西 我们会一起看一些可视化的东西 可以更好地理解
### 内置习题
![内置习题_假设函数解释](amWiki/images/001/03-Week3/1-Logistic Regression/8-内置习题_假设函数解释.jpg)  
## English
### Classification of Logistic Regression for Hypothesis Representation
![Classification of Logistic Regression for Hypothesis Representation](amWiki/images/001/03-Week3/1-Logistic Regression/6-分类问题逻辑回归模型表达.jpg)  
Let's start talking about logistic regression. In this video, I'd like to show you the hypothesis representation. That is, what is the function we're going to use to represent our hypothesis when we have a classification problem?
0:15
Earlier, we said that we would like our classifier to output values that are between 0 and 1. So we'd like to come up with a hypothesis that satisfies this property, that is, predictions are maybe between 0 and 1. When we were using linear regression, this was the form of a hypothesis, where h(x) is theta transpose x. For logistic regression, I'm going to modify this a little bit and make the hypothesis g of theta transpose x. Where I'm going to define the function g as follows. G(z), z is a real number, is equal to one over one plus e to the negative z.
0:58
This is called the sigmoid function, or the logistic function, and the term logistic function, that's what gives rise to the name logistic regression. And by the way, the terms sigmoid function and logistic function are basically synonyms and mean the same thing. So the two terms are basically interchangeable, and either term can be used to refer to this function g. And if we take these two equations and put them together, then here's just an alternative way of writing out the form of my hypothesis. I'm saying that h(x) Is 1 over 1 plus e to the negative theta transpose x. And all I've do is I've taken this variable z, z here is a real number, and plugged in theta transpose x. So I end up with theta transpose x in place of z there. Lastly, let me show you what the sigmoid function looks like. We're gonna plot it on this figure here. The sigmoid function, g(z), also called the logistic function, it looks like this. It starts off near 0 and then it rises until it crosses 0.5 and the origin, and then it flattens out again like so. So that's what the sigmoid function looks like. And you notice that the sigmoid function, while it asymptotes at one and asymptotes at zero, as a z axis, the horizontal axis is z. As z goes to minus infinity, g(z) approaches zero. And as g(z) approaches infinity, g(z) approaches one. And so because g(z) upwards values are between zero and one, we also have that h(x) must be between zero and one. Finally, given this hypothesis representation, what we need to do, as before, is fit the parameters theta to our data. So given a training set we need to a pick a value for the parameters theta and this hypothesis will then let us make predictions.
### Classification for interpretation of Hypothesis Outputs
![Classification for interpretation of Hypothesis Outputs](amWiki/images/001/03-Week3/1-Logistic Regression/7-分类问题的假设函数输出解释.jpg)  
We'll talk about a learning algorithm later for fitting the parameters theta, but first let's talk a bit about the interpretation of this model.Here's how I'm going to interpret the output of my hypothesis, h(x).When my hypothesis outputs some number, I am going to treat that number as the estimated probability that y is equal to one on a new input, example x. Here's what I mean, here's an example. Let's say we're using the tumor classification example, so we may have a feature vector x, which is this x zero equals one as always. And then one feature is the size of the tumor.Suppose I have a patient come in and they have some tumor size and I feed their feature vector x into my hypothesis. And suppose my hypothesis outputs the number 0.7. I'm going to interpret my hypothesis as follows. I'm gonna say that this hypothesis is telling me that for a patient with features x, the probability that y equals 1 is 0.7. In other words, I'm going to tell my patient that the tumor, sadly, has a 70 percent chance, or a 0.7 chance of being malignant. To write this out slightly more formally, or to write this out in math, I'm going to interpret my hypothesis output as. P of y=1 given x parameterized by theta. So for those of you that are familiar with probability, this equation may make sense. If you're a little less familiar with probability, then here's how I read this expression. This is the probability that y is equal to one. Given x, given that my patient has features x, so given my patient has a particular tumor size represented by my features x. And this probability is parameterized by theta. So I'm basically going to count on my hypothesis to give me estimates of the probability that y is equal to 1. Now, since this is a classification task, we know that y must be either 0 or 1, right? Those are the only two values that y could possibly take on, either in the training set or for new patients that may walk into my office, or into the doctor's office in the future. So given h(x), we can therefore compute the probability that y = 0 as well, completely because y must be either 0 or 1. We know that the probability of y = 0 plus the probability of y = 1 must add up to 1. This first equation looks a little bit more complicated. It's basically saying that probability of y=0 for a particular patient with features x, and given our parameters theta.Plus the probability of y=1 for that same patient with features x and given theta parameters theta must add up to one. If this equation looks a little bit complicated, feel free to mentally imagine it without that x and theta. And this is just saying that the product of y equals zero plus the product of y equals one, must be equal to one. And we know this to be true because y has to be either zero or one, and so the chance of y equals zero, plus the chance that y is one. Those two must add up to one. And so if you just take this term and move it to the right hand side, then you end up with this equation. That says probability that y equals zero is 1 minus probability of y equals 1, and thus if our hypothesis feature of x gives us that term. You can therefore quite simply compute the probability or compute the estimated probability that y is equal to 0 as well. So, you now know what the hypothesis representation is for logistic regression and we're seeing what the mathematical formula is, defining the hypothesis for logistic regression. In the next video, I'd like to try to give you better intuition about what the hypothesis function looks like. And I wanna tell you about something called the decision boundary. And we'll look at some visualizations together to try to get a better sense of what this hypothesis function of logistic regression really looks like.
### Exam_in the video
![Exam_interpretation of Hypothesis Outputs](amWiki/images/001/03-Week3/1-Logistic Regression/8-内置习题_假设函数解释.jpg)  
