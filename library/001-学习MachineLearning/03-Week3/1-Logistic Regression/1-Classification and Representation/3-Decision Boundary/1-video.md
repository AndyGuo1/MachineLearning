# 分类问题_决策边界
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/03-Week3/1-Logistic Regression/3-Decision Boundary.mp4" type="video/mp4">
</video>
## 中文
### 分类问题逻辑回归假设函数特性
![分类问题逻辑回归假设函数特性](amWiki/images/001/03-Week3/1-Logistic Regression/10-分类问题逻辑回归假设函数特性.jpg)  
在过去的视频中 我们谈到 逻辑回归中假设函数的表示方法 现在 我想 告诉大家一个叫做 决策边界(decision boundary)的概念 这个概念能更好地帮助我们 理解逻辑回归的 假设函数在计算什么 让我们回忆一下 这是我们上次写下的公式 当时我们说 假设函数可以表示为 h(x)=g(θTx) 其中函数g 被称为S形函数（sigmoid function） 看起来是应该是这样的形状 它从零开始慢慢增加至1 逐渐逼近1 现在让我们 更进一步来理解 这个假设函数何时 会将y预测为1 什么时候又会将 y预测为0 让我们更好的理解 假设函数的应该是怎样的 特别是当我们的数据有多个特征时 具体地说 这个假设函数 输出的是 给定x时 y=1的概率 因此 如果我们想 预测y=1 还是等于0 我们可以这样做 只要该假设函数 输出y=1的概率 大于或等于0.5 那么这表示 y更有可能 等于1而不是0 因此 我们预测y=1 在另一种情况下 如果 预测y=1 的概率 小于0.5 那么我们应该预测y=0 在这里 我选择大于等于 在这里我选择小于 如果h(x)的值 正好等于0.5 那么 我们可以预测为1 也可以预测为0 但是这里我选择了大于等于 因此我们默认 如果h(x)等于0.5的话 预测选择为1 这只是一个细节 不用太在意 下面 我希望大家能够 清晰地理解 什么时候h(x) 将大于或等于 0.5 从而 我们最终预测y=1 如果我们看看 S形函数的曲线图 我们会注意到 S函数 只要z大于 或等于0时 g(z)就将大于 或等于0.5 因此 在曲线图的这半边 g的取值 大于或等于0.5 因为这个交点就是0.5 因此 当z大于0时 g(z) 也就是这个 S形函数 是大于或等于0.5的 由于逻辑回归的 假设函数h(x) 等于g(θTx) 因此 函数值将会 大于或等于0.5 只要θ转置乘以x 大于或等于0 因此 我们看到 因为这里θ转置x 取代了z的位置 所以我们看到 我们的假设函数 将会预测y=1 只要θ转置乘以x 大于或等于0 现在让我们来考虑 假设函数 预测y=0的情况 类似的 h(θ)将会 小于0.5 只要 g(z)小于0.5 这是因为 z的定义域上 导致g(z)取值 小于0.5的部分 是z小于0的部分 所以当g(z)小于0.5时 我们的假设函数将会预测 y=0 根据与之前 类似的原因 h(x)等于 g(θTx) 因此 只要 θ转置乘以x小于0 我们就预测y等于0 总结一下我们刚才所讲的 我们看到 如果我们要决定 预测y=1 还是y=0 取决于 y=1的概率 大于或等于0.5 还是小于0.5 这其实就等于说 我们将预测y=1 只需要θ转置乘以x 大于或等于0 另一方面我们将预测y=0 只需要θ转置乘以x 小于0 通过这些 我们能更好地 理解如何利用逻辑回归的假设函数 来进行预测
### 分类问题线性决策边界
![分类问题线性决策边界](amWiki/images/001/03-Week3/1-Logistic Regression/11-分类问题线性决策边界.jpg)  
现在假设我们有 一个训练集 就像幻灯片上的这个 接下来我们假设我们的假设函数是 h(x)等于g() 括号里面是θ0加上θ1x1 加上θ2乘以x2 目前我们还没有谈到 如何拟合此模型中的参数 我们将在下一个视频中讨论这个问题 但是假设我们 已经拟合好了参数 我们最终选择了如下值 比方说 我们选择θ0 等于-3 θ1 等于1 θ2等于1 因此 这意味着我的 参数向量将是 θ等于[-3 1 1] 这样 我们有了 这样的一个参数选择 让我们试着找出 假设函数何时将 预测y等于1 何时又将预测y等于0 使用我们在 在上一张幻灯片上展示的公式 我们知道 y更有可能是1 或者说 y等于1的概率 大于0.5 或者大于等于0.5 只要θ转置x 大于0 我刚刚加了下划线的 这个公式 -3加上x1再加上x2 当然就是θ转置x 这是当θ等于 我们选择的这个参数值时 θ转置乘以x的表达 因此 举例来说 对于任何样本 只要x1和x2满足 这个等式 也就是-3 加上x1再加x2 大于等于0 我们的假设函数就会认为 y等于1 的可能性较大 或者说将预测y=1 我们也可以 将-3放到不等式右边 并改写为x1 加号x2大于等于3 这样是等价的 我们发现 这一假设函数将预测 y=1 只要 x1+x2大于等于3 让我们来看看这在图上是什么意思 如果我写下等式 x1+x2等于3 这将定义一条直线 如果我画出这条直线 它将表示为 这样一条线 它通过 通过x1轴上的3 和x2轴上的3 因此 这部分的输入样本空间 这一部分的 X1-X2平面 对应x1加x2大于等于3 这将是上面这个半平面 也就是所有 上方和所有右侧的部分 相对我画的这条洋红色线来说 所以 我们的假设函数预测 y等于1的区域 就是这片区域 是这个巨大的区域 是右上方的这个半平面 让我把它写下来 我将称它为 y=1区域 与此相对 x1加x2 小于3的区域 也就是我们预测 y等于0的区域 是这一片区域 你看到 这也是一个半平面 左侧的这个半平面 是我们的假设函数预测y等于0的区域 我想给这条线一个名字 就是我刚刚画的这条洋红色线 这条线被称为 决策边界（decision boundary） 具体地说 这条直线 满足x1+x2=3 它对应一系列的点 它对应 h(x)等于 0.5的区域 决策边界 也就是 这条直线 将整个平面分成了两部分 其中一片区域假设函数预测y等于1 而另一片区域 假设函数预测y等于0 我想澄清一下 决策边界是 假设函数的一个属性 它包括参数θ0 θ1 θ2 在这幅图中 我画了一个训练集 我画了一组数据 让它更加可视化 但是 即使我们 去掉这个数据集 这条决策边界 和我们预测y等于1 与y等于0的区域 它们都是 假设函数的属性 决定于其参数 它不是数据集的属性 当然 我们后面还将讨论 如何拟合参数 那时 我们将 使用训练集 使用我们的数据 来确定参数的取值 但是 一旦我们有确定的参数取值 有确定的θ0 θ1 θ2 我们就将完全确定 决策边界 这时 我们实际上并不需要 在绘制决策边界的时候 绘制训练集
### 分类问题逻辑回归模型表达
![分类问题逻辑回归模型表达](amWiki/images/001/03-Week3/1-Logistic Regression/12-分类问题非线性决策边界.jpg)  
现在 让我们看一个 更复杂的例子 和往常一样 我使用十字 (X) 表示我的正样本 圆圈 (O) 的表示我的负样本 给定这样的一个训练集 我怎样才能使用逻辑回归 拟合这些数据呢？ 早些时候 当我们谈论 多项式回归 或线性回归时 我们谈到可以添加额外的 高阶多项式项 同样我们也可以对逻辑回归使用相同的方法 具体地说 假如我的假设函数是这样的 我已经添加了两个额外的特征 x1平方和x2平方 所以 我现在有5个参数 θ0 到 θ4 之前讲过 我们会 在下一个视频中讨论 如何自动选择 参数θ0到θ4的取值 但是 假设我 已经使用了这个方法 我最终选择θ0等于-1 θ1等于0 θ2等于0 θ3等于1 θ4等于1 这意味着 在这个参数选择下 我的参数向量 θ将是[-1 0 0 1 1] 根据我们前面的讨论 这意味着我的假设函数将预测 y=1 只要-1加x1平方 加x2平方大于等于0 也就是θ转置 我的θ转置 乘以特征变量大于等于0的时候 如果我将 -1放到不等式右侧 我可以说 我的假设函数将预测 y=1 只要x1平方加 x2的平方大于等于1 那么决策边界是什么样子的呢？ 好吧 如果我们绘制 x1平方加 x2的平方等于1的曲线 你们有些人已经 知道这个方程对应 半径为1 原点为中心的圆 所以 这就是我们的决策边界 圆外面的一切 我将预测 y=1 所以这里就是 y等于1的区域 我们在这里预测y=1 而在圆里面 我会预测y=0 因此 通过增加这些 复杂的多项式特征变量 我可以得到更复杂的决定边界 而不只是 用直线分开正负样本 在这个例子中 我可以得到 一个圆形的决策边界 再次强调 决策边界 不是训练集的属性 而是假设本身及其参数的属性 只要我们 给定了参数向量θ 圆形的决定边界 就确定了 我们不是用训练集来定义的决策边界 我们用训练集来拟合参数θ 以后我们将谈论如何做到这一点 但是 一旦你有 参数θ它就确定了决策边界 让我重新显示训练集 以方便可视化 最后 让我们来看看一个更复杂的例子 我们可以得到 更复杂的决策边界吗？ 如果我有 高阶多项式特征变量 比如x1平方 x1平方乘以x2 x1平方乘以x2平方 等等 如果我有更高阶 多项式 那么可以证明 你将得到 更复杂的决策边界 而逻辑回归 可以用于找到决策边界 例如 这样一个椭圆 或者参数不同的椭圆 也许你 可以得到一个不同的决定边界 像这个样子 一些有趣的形状 或者更为复杂的例子 你也可以得到决策边界 看起来这样 这样更复杂的形状 在这个区域 你预测y=1 在这个区域外面你预测y=0 因此 这些高阶多项式 特征变量 可以让你得到非常复杂的决策边界 因此 通过这些可视化图形 我希望告诉你 什么范围的假设函数 我们可以使用 逻辑回归来表示 现在我们知道了h(x)表示什么 在下一个视频中 我将介绍 如何自动选择参数θ 使我们能在给定一个训练集时 我们可以根据数据自动拟合参数
### 内置习题
![内置习题_决策边界](amWiki/images/001/03-Week3/1-Logistic Regression/13-内置习题_决策边界.jpg)  
## English
### 分类问题逻辑回归假设函数特性
![分类问题逻辑回归假设函数特性](amWiki/images/001/03-Week3/1-Logistic Regression/10-分类问题逻辑回归假设函数特性.jpg)  
In the last video, we talked about the hypothesis representation for logistic regression. What Id like to do now is tell you about something called the decision boundary, and this will give us a better sense of what the logistic regressions hypothesis function is computing.To recap, this is what we wrote out last time, where we said that the hypothesis is represented as h of x equals g of theta transpose x, where g is this function called the sigmoid function, which looks like this. It slowly increases from zero to one, asymptoting at one.What I want to do now is try to understand better when this hypothesis will make predictions that y is equal to 1 versus when it might make predictions that y is equal to 0. And understand better what hypothesis function looks like particularly when we have more than one feature. Concretely, this hypothesis is outputting estimates of the probability that y is equal to one, given x and parameterized by theta. So if we wanted to predict is y equal to one or is y equal to zero, here's something we might do. Whenever the hypothesis outputs that the probability of y being one is greater than or equal to 0.5, so this means that if there is more likely to be y equals 1 than y equals 0, then let's predict y equals 1. And otherwise, if the probability, the estimated probability of y being over 1 is less than 0.5, then let's predict y equals 0. And I chose a greater than or equal to here and less than here If h of x is equal to 0.5 exactly, then you could predict positive or negative, but I probably created a loophole here, so we default maybe to predicting positive if h of x is 0.5, but that's a detail that really doesn't matter that much.What I want to do is understand better when is it exactly that h of x will be greater than or equal to 0.5, so that we'll end up predicting y is equal to 1. If we look at this plot of the sigmoid function, we'll notice that the sigmoid function, g of z is greater than or equal to 0.5 whenever z is greater than or equal to zero. So is in this half of the figure that g takes on values that are 0.5 and higher. This notch here, that's 0.5, and so when z is positive, g of z, the sigmoid function is greater than or equal to 0.5.Since the hypothesis for logistic regression is h of x equals g of theta and transpose x, this is therefore going to be greater than or equal to 0.5, whenever theta transpose x is greater than or equal to 0. So what we're shown, right, because here theta transpose x takes the role of z.So what we're shown is that a hypothesis is gonna predict y equals 1 whenever theta transpose x is greater than or equal to 0. Let's now consider the other case of when a hypothesis will predict y is equal to 0. Well, by similar argument, h(x) is going to be less than 0.5 whenever g(z) is less than 0.5 because the range of values of z that cause g(z) to take on values less than 0.5, well, that's when z is negative. So when g(z) is less than 0.5, a hypothesis will predict that y is equal to 0. And by similar argument to what we had earlier, h(x) is equal to g of theta transpose x and so we'll predict y equals 0 whenever this quantity theta transpose x is less than 0.To summarize what we just worked out, we saw that if we decide to predict whether y=1 or y=0 depending on whether the estimated probability is greater than or equal to 0.5, or whether less than 0.5, then that's the same as saying that when we predict y=1 whenever theta transpose x is greater than or equal to 0. And we'll predict y is equal to 0 whenever theta transpose x is less than 0. Let's use this to better understand how the hypothesis of logistic regression makes those predictions
### 分类问题线性决策边界
![分类问题线性决策边界](amWiki/images/001/03-Week3/1-Logistic Regression/11-分类问题线性决策边界.jpg)  
Now, let's suppose we have a training set like that shown on the slide. And suppose a hypothesis is h of x equals g of theta zero plus theta one x one plus theta two x two.We haven't talked yet about how to fit the parameters of this model. We'll talk about that in the next video. But suppose that via a procedure to specified. We end up choosing the following values for the parameters. Let's say we choose theta 0 equals 3, theta 1 equals 1, theta 2 equals 1. So this means that my parameter vector is going to be theta equals minus 3, 1, 1.So, when given this choice of my hypothesis parameters, let's try to figure out where a hypothesis would end up predicting y equals one and where it would end up predicting y equals zero.Using the formulas that we were taught on the previous slide, we know that y equals one is more likely, that is the probability that y equals one is greater than or equal to 0.5, whenever theta transpose x is greater than zero. And this formula that I just underlined, -3 + x1 + x2, is, of course, theta transpose x when theta is equal to this value of the parameters that we just chose.So for any example, for any example which features x1 and x2 that satisfy this equation, that minus 3 plus x1 plus x2 is greater than or equal to 0, our hypothesis will think that y equals 1, the small x will predict that y is equal to 1.We can also take -3 and bring this to the right and rewrite this as x1+x2 is greater than or equal to 3, so equivalently, we found that this hypothesis would predict y=1 whenever x1+x2 is greater than or equal to 3.
Let's see what that means on the figure, if I write down the equation, X1 + X2 = 3, this defines the equation of a straight line and if I draw what that straight line looks like, it gives me the following line which passes through 3 and 3 on the x1 and the x2 axis.So the part of the infospace, the part of the X1 X2 plane that corresponds to when X1 plus X2 is greater than or equal to 3, that's going to be this right half thing, that is everything to the up and everything to the upper right portion of this magenta line that I just drew. And so, the region where our hypothesis will predict y = 1, is this region, just really this huge region, this half space over to the upper right. And let me just write that down, I'm gonna call this the y = 1 region. And, in contrast, the region where x1 + x2 is less than 3, that's when we will predict that y is equal to 0. And that corresponds to this region. And there's really a half plane, but that region on the left is the region where our hypothesis will predict y = 0. I wanna give this line, this magenta line that I drew a name. This line, there, is called the decision boundary.And concretely, this straight line, X1 plus X equals 3. That corresponds to the set of points, so that corresponds to the region where H of X is equal to 0.5 exactly and the decision boundary that is this straight line, that's the line that separates the region where the hypothesis predicts Y equals 1 from the region where the hypothesis predicts that y is equal to zero. And just to be clear, the decision boundary is a property of the hypothesis including the parameters theta zero, theta one, theta two. And in the figure I drew a training set, I drew a data set, in order to help the visualization. But even if we take away the data set this decision boundary and the region where we predict y =1 versus y = 0, that's a property of the hypothesis and of the parameters of the hypothesis and not a property of the data set.Later on, of course, we'll talk about how to fit the parameters and there we'll end up using the training set, using our data. To determine the value of the parameters. But once we have particular values for the parameters theta0, theta1, theta2 then that completely defines the decision boundary and we don't actually need to plot a training set in order to plot the decision boundary.
### 分类问题非线性决策边界
![分类问题非线性决策边界](amWiki/images/001/03-Week3/1-Logistic Regression/12-分类问题非线性决策边界.jpg)  
Let's now look at a more complex example where as usual, I have crosses to denote my positive examples and Os to denote my negative examples. Given a training set like this, how can I get logistic regression to fit the sort of data?Earlier when we were talking about polynomial regression or when we're talking about linear regression, we talked about how we could add extra higher order polynomial terms to the features. And we can do the same for logistic regression. Concretely, let's say my hypothesis looks like this where I've added two extra features, x1 squared and x2 squared, to my features. So that I now have five parameters, theta zero through theta four.As before, we'll defer to the next video, our discussion on how to automatically choose values for the parameters theta zero through theta four. But let's say that varied procedure to be specified, I end up choosing theta zero equals minus one, theta one equals zero, theta two equals zero, theta three equals one and theta four equals one.What this means is that with this particular choose of parameters, my parameter effect theta theta looks like minus one, zero, zero, one, one.Following our earlier discussion, this means that my hypothesis will predict that y=1 whenever -1 + x1 squared + x2 squared is greater than or equal to 0. This is whenever theta transpose times my theta transfers, my features is greater than or equal to zero. And if I take minus 1 and just bring this to the right, I'm saying that my hypothesis will predict that y is equal to 1 whenever x1 squared plus x2 squared is greater than or equal to 1. So what does this decision boundary look like? Well, if you were to plot the curve for x1 squared plus x2 squared equals 1 Some of you will recognize that, that is the equation for circle of radius one, centered around the origin. So that is my decision boundary.And everything outside the circle, I'm going to predict as y=1. So out here is my y equals 1 region, we'll predict y equals 1 out here and inside the circle is where I'll predict y is equal to 0. So by adding these more complex, or these polynomial terms to my features as well, I can get more complex decision boundaries that don't just try to separate the positive and negative examples in a straight line that I can get in this example, a decision boundary that's a circle.Once again, the decision boundary is a property, not of the trading set, but of the hypothesis under the parameters. So, so long as we're given my parameter vector theta, that defines the decision boundary, which is the circle. But the training set is not what we use to define the decision boundary. The training set may be used to fit the parameters theta. We'll talk about how to do that later. But, once you have the parameters theta, that is what defines the decisions boundary.Let me put back the training set just for visualization.And finally let's look at a more complex example.So can we come up with even more complex decision boundaries then this? If I have even higher polynomial terms so things like X1 squared, X1 squared X2, X1 squared equals squared and so on. And have much higher polynomials, then it's possible to show that you can get even more complex decision boundaries and the regression can be used to find decision boundaries that may, for example, be an ellipse like that or maybe a little bit different setting of the parameters maybe you can get instead a different decision boundary which may even look like some funny shape like that.Or for even more complete examples maybe you can also get this decision boundaries that could look like more complex shapes like that where everything in here you predict y = 1 and everything outside you predict y = 0. So this higher autopolynomial features you can a very complex decision boundaries. So, with these visualizations, I hope that gives you a sense of what's the range of hypothesis functions we can represent using the representation that we have for logistic regression.Now that we know what h(x) can represent, what I'd like to do next in the following video is talk about how to automatically choose the parameters theta so that given a training set we can automatically fit the parameters to our data.
### Exam_int the video
![Exam_Decision Boundary](amWiki/images/001/03-Week3/1-Logistic Regression/13-内置习题_决策边界.jpg)  
