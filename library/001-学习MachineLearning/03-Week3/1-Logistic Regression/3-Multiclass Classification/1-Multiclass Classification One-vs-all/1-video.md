# 多类别分类问题_一对多
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/03-Week3/1-Logistic Regression/7-Multiclass Classification_One-vs-all.mp4" type="video/mp4">
</video>
## 中文
### 举例说明多类别分类问题
![举例说明多类别分类问题](amWiki/images/001/03-Week3/1-Logistic Regression/35-举例说明多类别分类问题.jpg)  
在本节视频中 我们将谈到如何使用逻辑回归 (logistic regression) 来解决多类别分类问题 具体来说 我想通过一个叫做"一对多" (one-vs-all) 的分类算法 让你了解什么是多类别分类问题 先看这样一些例子 假如说你现在需要 一个学习算法 能自动地 将邮件归类到不同的文件夹里 或者说可以自动地加上标签 那么 你也许需要一些不同的文件夹 或者不同的标签来完成这件事 来区分开来自工作的邮件、来自朋友的邮件 来自家人的邮件或者是有关兴趣爱好的邮件 那么 我们就有了 这样一个分类问题 其类别有四个 分别用y=1、y=2、y=3、 y=4 来代表 另一个例子是有关药物诊断的 如果一个病人 因为鼻塞 来到你的诊所 他可能并没有生病 用 y=1 这个类别来代表 或者患了感冒 用 y=2 来代表 或者得了流感 y=3 第三个例子 也是最后一个例子 如果你正在做有关 天气的机器学习分类问题 那么你可能想要区分 哪些天是晴天、多云、雨天、 或者下雪天 对上述所有的例子 y 可以取 一个很小的数值 一个相对"谨慎"的数值 比如1到3、1到4或者其它数值 以上说的都是多类分类问题
### 画图说明多类别分类问题
![画图说明多类别分类问题](amWiki/images/001/03-Week3/1-Logistic Regression/36-画图说明多类别分类问题.jpg)  
顺便一提的是 对于下标是 0 1 2 3 还是 1 2 3 4 都不重要 我更喜欢将分类 从 1 开始标而不是 0 其实怎样标注都不会影响最后的结果 然而对于之前的一个 二元分类问题 我们的数据看起来可能是像这样 对于一个多类分类问题 我们的数据集 或许看起来像这样 我用三种不同的符号来代表三个类别 问题就是 给出三个类型的数据集 这是一个类别中的样本 而这个样本是属于 另一个类别 而这个样本属于第三个类别 我们如何得到一个学习算法来进行分类呢？ 我们现在已经知道如何 进行二元分类 可以使用逻辑斯特回归 对于直线或许你也知道 可以将数据集一分为二为正类和负类 用一对多的 分类思想 我们可以 将其用在多类分类问题上
### 多类别分类问题处理方式-一对多
![多类别分类问题处理方式-一对多](amWiki/images/001/03-Week3/1-Logistic Regression/37-多类别分类问题处理方式-一对多.jpg)  
顺便一提的是 对于下标是 0 1 2 3 还是 1 2 3 4 都不重要 我更喜欢将分类 从 1 开始标而不是 0 其实怎样标注都不会影响最后的结果 然而对于之前的一个 二元分类问题 我们的数据看起来可能是像这样 对于一个多类分类问题 我们的数据集 或许看起来像这样 我用三种不同的符号来代表三个类别 问题就是 给出三个类型的数据集 这是一个类别中的样本 而这个样本是属于 另一个类别 而这个样本属于第三个类别 我们如何得到一个学习算法来进行分类呢？ 我们现在已经知道如何 进行二元分类 可以使用逻辑斯特回归 对于直线或许你也知道 可以将数据集一分为二为正类和负类 用一对多的 分类思想 我们可以 将其用在多类分类问题上 下面将介绍如何进行一对多的分类工作 有时这个方法也被称为"一对余"方法 现在我们有一个训练集 好比左边表示的 有三个类别 我们用三角形表示 y=1 方框表示 y=2 叉叉表示 y=3 我们下面要做的就是 使用一个训练集 将其分成三个二元分类问题 所以我将它分成三个 二元分类问题 我们先从用三角形代表的类别1开始 实际上我们可以创建一个 新的"伪"训练集 类型2和类型3 定为负类 类型1 设定为正类 我们创建一个新的 训练集 如右侧所示的那样 我们要拟合出一个合适的分类器 我们称其为 h 下标 θ 上标(1) (x) 这里的三角形是正样本 而圆形代表负样本 可以这样想 设置三角形的值为1 圆形的值为0 下面我们来训练一个标准的 逻辑回归分类器 这样我们就得到一个正边界 对吧? 这里上标(1)表示类别1 我们可以像这样对三角形类别这么做 下面 我们将为类别2做同样的工作 取这些方块样本 然后将这些方块 作为正样本 设其它的为三角形和叉形类别为负样本 这样我们找到第二个合适的逻辑回归分类器 我们称为 h 下标 θ 上标(2) (x) 其中上标(2)表示 是类别2 所以我们做的就是 把方块类当做正样本 我们可能便会得到这样的一个分类器 最后 同样地 我们对第三个类别采用同样的方法 并找出 第三个分类器 h 下标 θ 上标(3) (x) 或许这么做 可以给出一个像这样的 判别边界 或者说分类器 能这样分开正负样本 总而言之 我们已经拟合出三个分类器 对于 i 等于1、2、3 我们都找到了一个分类器 h 上标(i) 下标θ 括号 x 通过这样来尝试 估计出 给出 x 和先验 θ 时 y的值等于 i 的概率 对么？ 在一开始 对于第一个在这里的 分类器 完成了对三角形的识别 把三角形当做是正类别 所以 h(1) 实际上是在计算 给定x 以 θ 为参数时 y的值为1的 概率是多少 概率是多少 同样地 这个也是这么处理 矩形类型当做一个正类别 同样地 可以计算出 y=2 的概率和其它的概率值来 现在我们便有了三个分类器 且每个分类器都作为其中一种情况进行训练
### 多类别分类问题-分类器
![多类别分类问题-分类器](amWiki/images/001/03-Week3/1-Logistic Regression/38-多类别分类问题-分类器.jpg)  
总之 我们已经把要做的做完了 现在要做的就是训练这个 逻辑回归分类器 h(i) 逻辑回归分类器 h(i) 其中 i 对应每一个可能的 y=i 最后 为了做出预测 我们给出输入一个新的 x 值 用这个做预测 我们要做的 就是 在我们三个分类器 里面输入 x 然后 我们选择一个让 h 最大的 i 你现在知道了 基本的挑选分类器的方法 选择出哪一个分类器是 可信度最高效果最好的 那么就可认为得到一个正确的分类 无论i值是多少 我们都有最高的概率值 我们预测 y 就是那个值 这就是多类别分类问题 以及一对多的方法 通过这个小方法 你现在也可以将 逻辑回归分类器 用在多类分类的问题上
### 内置习题
![内置习题_多类别分类问题分类器](amWiki/images/001/03-Week3/1-Logistic Regression/39-内置习题_多类别分类问题分类器.jpg)
## English
### Example_Multiclass Classification
![Example_Multiclass Classification](amWiki/images/001/03-Week3/1-Logistic Regression/35-举例说明多类别分类问题.jpg)  
In this video we'll talk about how to get logistic regression to work for multiclass classification problems. And in particular I want to tell you about an algorithm called one-versus-all classification.
0:12
What's a multiclass classification problem? Here are some examples. Lets say you want a learning algorithm to automatically put your email into different folders or to automatically tag your emails so you might have different folders or different tags for work email, email from your friends, email from your family, and emails about your hobby. And so here we have a classification problem with four classes which we might assign to the classes y = 1, y =2, y =3, and y = 4 too. And another example, for medical diagnosis, if a patient comes into your office with maybe a stuffy nose, the possible diagnosis could be that they're not ill. Maybe that's y = 1. Or they have a cold, 2. Or they have a flu.
0:59
And a third and final example if you are using machine learning to classify the weather, you know maybe you want to decide that the weather is sunny, cloudy, rainy, or snow, or if it's gonna be snow, and so in all of these examples, y can take on a small number of values, maybe one to three, one to four and so on, and these are multiclass classification problems.
### Plot to Explain Multiclass Classification
![Plot to Explain Multiclass Classification](amWiki/images/001/03-Week3/1-Logistic Regression/36-画图说明多类别分类问题.jpg)  
And by the way, it doesn't really matter whether we index is at 0, 1, 2, 3, or as 1, 2, 3, 4. I tend to index my classes starting from 1 rather than starting from 0, but either way we're off and it really doesn't matter. Whereas previously for a binary classification problem, our data sets look like this. For a multi-class classification problem our data sets may look like this where here I'm using three different symbols to represent our three classes. So the question is given the data set with three classes where this is an example of one class, that's an example of a different class, and that's an example of yet a third class. How do we get a learning algorithm to work for the setting? We already know how to do binary classification using a regression. We know how to you know maybe fit a straight line to set for the positive and negative classes. You see an idea called one-vs-all classification. We can then take this and make it work for multi-class classification as well.
### Solution for Multiclass Classification-one-vs-all
![Solution for Multiclass Classification-one-vs-all](amWiki/images/001/03-Week3/1-Logistic Regression/37-多类别分类问题处理方式-一对多.jpg)  
Here's how a one-vs-all classification works. And this is also sometimes called one-vs-rest. Let's say we have a training set like that shown on the left, where we have three classes of y equals 1, we denote that with a triangle, if y equals 2, the square, and if y equals three, then the cross. What we're going to do is take our training set and turn this into three separate binary classification problems. I'll turn this into three separate two class classification problems. So let's start with class one which is the triangle. We're gonna essentially create a new sort of fake training set where classes two and three get assigned to the negative class. And class one gets assigned to the positive class. You want to create a new training set like that shown on the right, and we're going to fit a classifier which I'm going to call h subscript theta superscript one of x where here the triangles are the positive examples and the circles are the negative examples. So think of the triangles being assigned the value of one and the circles assigned the value of zero. And we're just going to train a standard logistic regression classifier and maybe that will give us a position boundary that looks like that. Okay?
3:34
This superscript one here stands for class one, so we're doing this for the triangles of class one. Next we do the same thing for class two. Gonna take the squares and assign the squares as the positive class, and assign everything else, the triangles and the crosses, as a negative class. And then we fit a second logistic regression classifier and call this h of x superscript two, where the superscript two denotes that we're now doing this, treating the square class as the positive class. And maybe we get classified like that. And finally, we do the same thing for the third class and fit a third classifier h super script three of x, and maybe this will give us a decision bounty of the visible cross fire. This separates the positive and negative examples like that.
4:22
So to summarize, what we've done is, we've fit three classifiers. So, for i = 1, 2, 3, we'll fit a classifier x super script i subscript theta of x. Thus trying to estimate what is the probability that y is equal to class i, given x and parametrized by theta. Right? So in the first instance for this first one up here, this classifier was learning to recognize the triangles. So it's thinking of the triangles as a positive clause, so x superscript one is essentially trying to estimate what is the probability that the y is equal to one, given that x is parametrized by theta. And similarly, this is treating the square class as a positive class and so it's trying to estimate the probability that y = 2 and so on. So we now have three classifiers, each of which was trained to recognize one of the three classes.
### Multiclass Classification-classifier
![Multiclass Classification-classifier](amWiki/images/001/03-Week3/1-Logistic Regression/38-多类别分类问题-分类器.jpg)  
Just to summarize, what we've done is we want to train a logistic regression classifier h superscript i of x for each class i to predict the probability that y is equal to i. Finally to make a prediction, when we're given a new input x, and we want to make a prediction. What we do is we just run all three of our classifiers on the input x and we then pick the class i that maximizes the three. So we just basically pick the classifier, I think whichever one of the three classifiers is most confident and so the most enthusiastically says that it thinks it has the right clause. So whichever value of i gives us the highest probability we then predict y to be that value.
6:02
So that's it for multi-class classification and one-vs-all method. And with this little method you can now take the logistic regression classifier and make it work on multi-class classification problems as well
### Exam_in the video
![Exam_The Classifier of Multiclass Classification](amWiki/images/001/03-Week3/1-Logistic Regression/39-内置习题_多类别分类问题分类器.jpg)
