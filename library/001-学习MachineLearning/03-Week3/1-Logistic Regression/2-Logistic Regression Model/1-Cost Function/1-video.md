# 分类问题_代价函数
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/03-Week3/1-Logistic Regression/4-Cost Function.mp4" type="video/mp4">
</video>
## 中文
### 如何选择参数θ?
![如何选择参数θ](amWiki/images/001/03-Week3/1-Logistic Regression/15-如何选择参数θ.jpg)  
在这个视频中，我们将讨论如何将θ的参数用于逻辑压缩。具体来说，我想定义优化目标，或者我们用来适应参数的成本函数。0:15这是拟合逻辑回归模型的引导性学习问题。我们有一个m训练样本的训练集，和往常一样，我们的每一个例子都是由一个n加一个维度，0:32，和往常一样我们有x o = 1。第一个特征或零特征总是等于一个。因为这是一个计算问题，我们的训练集有一个属性，每个标签y都是0或1。这是一个假设，假设的参数是这个θ。我想讨论的问题是，给定这个训练集，我们如何选择，或者我们如何适应参数的θ?
### 逻辑回归中的代价函数方程
![逻辑回归中的代价函数方程](amWiki/images/001/03-Week3/1-Logistic Regression/16-逻辑回归中的代价函数方程.jpg)  
在我们开发线性回归模型的时候，我们使用了以下的成本函数。我写得有点不一样，我用了1 / 2，把它放在求和里。现在我想用另一种方法来写出这个成本函数。也就是说，这里写的不是这个返回的平方，而是这里的h(x)的成本，我将定义，h(x)的总成本，等于这个。就等于这个平方误差的一半。现在我们可以更清楚地看到，成本函数是我的训练集的总和，是1 / n乘以这里的成本项的训练集的总和。为了简化这个方程，把这些超级脚本去掉是很方便的。所以只要定义h(x,y)的成本等于这个平方误差的一半。对于这个成本函数的解释是，这是我想让学习算法付出的代价如果它输出那个值，如果它的预测是h(x)，而实际的标签是y。所以把这些超级脚本划掉，对，对线性回归没有惊讶的是我们定义的成本是，或者说，这是1 / 2乘以我所预测的和我们所拥有的实际值之间的平方差。这个成本函数对线性回归很有效。但这里，我们对逻辑回归感兴趣。如果我们能把这个成本函数最小化，它就可以在这里输入J，这就没问题了。但结果是，如果我们使用这个特殊的成本函数，这将是参数的数据的非凸函数。这就是我所说的非凸的意思。有一些交叉函数jθ和逻辑回归，这个函数h 它的非线性是1 / 1加上e的-θ转置。这是一个非常复杂的非线性函数。如果你取这个函数，代入这里。然后取这个成本函数然后代入这里然后画出jθ的样子。你会发现jθ看起来就像这个3。33和很多局部最优的函数。正式的术语是，这是一个非凸函数。你可以说，如果你要在这种函数上运行梯度下降它不能保证收敛到全局最小值。相反，我们想要的是一个函数j(θ)是凸的，这是一个单弓的函数看起来是这样的，如果你运行θ，我们就能保证4:01将收敛到全局最小值。使用这个巨大的成本函数的问题是由于这个非常非线性的函数出现在中间，Jθ最终是一个非凸函数，如果你把它定义为一个正方形的成本函数。所以我们想做的是，而不是提出一个不同的成本函数，那是凸的，这样我们就可以应用一个很棒的算法，比如梯度下降，并保证能找到全局最小值。
### y=1时，逻辑回归中的代价函数
![y=1时，逻辑回归中的代价函数](amWiki/images/001/03-Week3/1-Logistic Regression/17-y=1,逻辑回归中的代价函数方程.jpg)  
这是逻辑回归的成本函数。我们说代价，或者说算法支付的惩罚，如果它的值是h(x)的值，所以如果这是某个数字，比如0。7，它可以预测x的值。实际的成本标签原来是y。如果y = 1和- log(1 - h(x))y = 0，代价将是- log(h(x))这看起来是一个非常复杂的函数，但是让我们画出这个函数来获得一些关于它的作用的直觉。我们从y = 1开始。如果y = 1，那么成本函数就是- log(h(x))如果我们把它画出来，假设横轴是h(x)，所以我们知道一个假设输出的值是0到1之间。对，h(x)在0和1之间变化。如果你画出这个成本函数的样子，你会发现它看起来是这样的。有一种方法可以看出为什么这个图看起来是这样的，因为如果你把logz和z放在横轴上，那么这就是这个样子。它接近负无穷，对吧?这就是对数函数的样子。这是0，这是1。这里，z当然扮演了h(x)的角色。所以- logz就像这样。只要翻转符号，负对数z，我们只关心这个函数在0到1之间的范围，所以把它消掉。我们只剩下了，这部分的曲线，这就是左边这条曲线的样子。现在，这个成本函数有一些有趣和令人满意的特性。首先，你注意到如果y等于1 h(x)等于1，换句话说，如果假设准确地预测h等于1 y等于它的预测，那么代价等于0，对吧?如果h(x)= 1，我们只考虑y = 1的情况。但是如果h(x)= 1，那么这里的成本是0。这就是我们想要的结果因为如果我们正确地预测了输出y，那么成本是0。但是现在注意到，当h(x)趋于0时，假设假设的输出趋于0，成本就会上升，然后趋于无穷。它的作用是，它抓住了一种直觉，如果假设为0，那就像说一个假设说y = 1的概率等于0。这有点像我们的医疗病人说的是y的概率所以，你的肿瘤绝对不可能是恶性的。7:55但是如果结果是肿瘤，病人的肿瘤，实际上是恶性的，所以如果y等于1，即使我们告诉他们，它发生的概率是0。所以这绝对不可能是恶性的。但是如果我们把这种确定性告诉他们，结果是错的，那么我们就会用非常非常大的成本来惩罚学习算法。当y = 1和h(x)趋于0时，这个代价趋于无穷。这张幻灯片考虑y = 1的情况。
### y=0时，逻辑回归中的代价函数
![y=0时，逻辑回归中的代价函数](amWiki/images/001/03-Week3/1-Logistic Regression/18-y=0,逻辑回归中的代价函数方程.jpg)  
我们来看看y = 0的成本函数。如果y等于0，那么代价是这样的，它看起来就像这个表达式，如果你画一个函数，- log(1 - z)，你得到的是这个函数实际上是这样的。它从0到1，像这样，如果你画出y = 0的价格函数，你会发现它是这样的。这条曲线的作用是，它现在上升，它趋于+∞h(x)趋于1因为正如我说的，如果y等于0。但是我们预测y等于1，几乎可以肯定，可能是1，最后我们支付了很大的代价。我们来画出y = 0的价格函数。如果y = 0，那将是我们的成本函数，如果你看这个表达式和你的绘图- log(1 - z)，如果你算出它的样子，你会得到一个看起来像这样的图形从0到a和横轴的z轴。所以如果你取这个成本函数并把它标为y = 0的情况，你得到的是成本函数。它是像这样表示的,什么这个成本函数是上升或者是正无穷h(x)→1,这吸引了直觉,如果一个假说预测,与确定性h(x)= 1,与可能的人,绝对会y = 1。但是如果y等于0，那么做这个假设是有意义的。所以这里的学习算法成本很高。反过来，如果h(x)等于0 y等于0，那么这个假设就会融化。z的受保护y等于0，结果y等于0，所以在这一点上，代价函数是0。在这个视频中，我们将定义一个单一的火车实例的成本函数。关于凸性分析的主题已经超出了本课程的范围，但有可能表明，在成本函数的特定选择下，这将给出一个凸优化问题。总成本函数jθ将是凸的，局部的最优化是自由的。下一节我们将把这些想法成本函数的一个训练的例子和进一步发展,并定义整个训练集的成本函数。,我们也会找出一种更简单的方法写比到目前为止我们一直在使用它,并根据我们评分下降,这将给我们逻辑回归收敛算法。
### 内置习题
![内置习题_代价函数是凸函数](amWiki/images/001/03-Week3/1-Logistic Regression/19-内置习题_代价函数是凸函数.jpg)
![内置习题_逻辑回归模型中代价函数](amWiki/images/001/03-Week3/1-Logistic Regression/20-内置习题_逻辑回归模型中代价函数.jpg)
## English
### How to choose parameter's θ?
![How to choose parameter's θ](amWiki/images/001/03-Week3/1-Logistic Regression/15-如何选择参数θ.jpg)  
In this video, we'll talk about how to fit the parameters of theta for the logistic compression. In particular, I'd like to define the optimization objective, or the cost function that we'll use to fit the parameters.Here's the supervised learning problem of fitting logistic regression model. We have a training set of m training examples and as usual, each of our examples is represented by a that's n plus one dimensional,and as usual we have x o equals one. First feature or a zero feature is always equal to one. And because this is a computation problem, our training set has the property that every label y is either 0 or 1. This is a hypothesis, and the parameters of a hypothesis is this theta over here. And the question that I want to talk about is given this training set, how do we choose, or how do we fit the parameter's theta?
### Logistic Regression of Cost Function
![Logistic Regression of Cost Function](amWiki/images/001/03-Week3/1-Logistic Regression/16-逻辑回归中的代价函数方程.jpg)  
Back when we were developing the linear regression model, we used the following cost function. I've written this slightly differently where instead of 1 over 2m, I've taken a one-half and put it inside the summation instead. Now I want to use an alternative way of writing out this cost function. Which is that instead of writing out this square of return here, let's write in here costs of h of x, y and I'm going to define that total cost of h of x, y to be equal to this. Just equal to this one-half of the squared error. So now we can see more clearly that the cost function is a sum over my training set, which is 1 over n times the sum of my training set of this cost term here.And to simplify this equation a little bit more, it's going to be convenient to get rid of those superscripts. So just define cost of h of x comma y to be equal to one half of this squared error. And interpretation of this cost function is that, this is the cost I want my learning algorithm to have to pay if it outputs that value, if its prediction is h of x, and the actual label was y. So just cross off the superscripts, right, and no surprise for linear regression the cost we've defined is that or the cost of this is that is one-half times the square difference between what I predicted and the actual value that we have, 0 for y. Now this cost function worked fine for linear regression. But here, we're interested in logistic regression. If we could minimize this cost function that is plugged into J here, that will work okay. But it turns out that if we use this particular cost function, this would be a non-convex function of the parameter's data. Here's what I mean by non-convex. Have some cross function j of theta and for logistic regression, this function h here has a nonlinearity that is one over one plus e to the negative theta transpose. So this is a pretty complicated nonlinear function. And if you take the function, plug it in here. And then take this cost function and plug it in there and then plot what j of theta looks like. You find that j of theta can look like a function that's like this with many local optima. And the formal term for this is that this is a non-convex function. And you can kind of tell, if you were to run gradient descent on this sort of function It is not guaranteed to converge to the global minimum. Whereas in contrast what we would like is to have a cost function j of theta that is convex, that is a single bow-shaped function that looks like this so that if you run theta in the we would be guaranteed that would converge to the global minimum. And the problem with using this great cost function is that because of this very nonlinear function that appears in the middle here, J of theta ends up being a nonconvex function if you were to define it as a square cost function. So what we'd like to do is, instead of come up with a different cost function, that is convex, and so that we can apply a great algorithm, like gradient descent and be guaranteed to find the global minimum.
### y=1,Logistic Regression of Cost Function
![y=1,Logistic Regression of Cost Function](amWiki/images/001/03-Week3/1-Logistic Regression/17-y=1,逻辑回归中的代价函数方程.jpg)  
And the problem with using this great cost function is that because of this very nonlinear function that appears in the middle here, J of theta ends up being a nonconvex function if you were to define it as a square cost function. So what we'd like to do is, instead of come up with a different cost function, that is convex, and so that we can apply a great algorithm, like gradient descent and be guaranteed to find the global minimum. Here's the cost function that we're going to use for logistic regression. We're going to say that the cost, or the penalty that the algorithm pays, if it upwards the value of h(x), so if this is some number like 0.7, it predicts the value h of x. And the actual cost label turns out to be y. The cost is going to be -log(h(x)) if y = 1 and -log(1- h(x)) if y = 0. This looks like a pretty complicated function, but let's plot this function to gain some intuition about what it's doing. Let's start off with the case of y = 1. If y = 1, then the cost function is -log(h(x)). And if we plot that, so let's say that the horizontal axis is h(x), so we know that a hypothesis is going to output a value between 0 and 1. Right, so h(x), that varies between 0 and 1. If you plot what this cost function looks like, you find that it looks like this. One way to see why the plot looks like this is because if you were to plot log z with z on the horizontal axis, then that looks like that. And it approaches minus infinity, right? So this is what the log function looks like. And this is 0, this is 1. Here, z is of course playing the role of h of x. And so -log z will look like this.Just flipping the sign, minus log z, and we're interested only in the range of when this function goes between zero and one, so get rid of that. And so we're just left with, you know, this part of the curve, and that's what this curve on the left looks like. Now, this cost function has a few interesting and desirable properties. First, you notice that if y is equal to 1 and h(x) is equal to 1, in other words, if the hypothesis exactly predicts h equals 1 and y is exactly equal to what it predicted, then the cost = 0 right? That corresponds to the curve doesn't actually flatten out. The curve is still going. First, notice that if h(x) = 1, if that hypothesis predicts that y = 1 and if indeed y = 1 then the cost = 0. That corresponds to this point down here, right? If h(x) = 1 and we're only considering the case of y = 1 here. But if h(x) = 1 then the cost is down here, is equal to 0. And that's where we'd like it to be because if we correctly predict the output y, then the cost is 0. But now notice also that as h(x) approaches 0, so as the output of a hypothesis approaches 0, the cost blows up and it goes to infinity. And what this does is this captures the intuition that if a hypothesis of 0, that's like saying a hypothesis saying the chance of y equals 1 is equal to 0. It's kinda like our going to our medical patients and saying the probability that you have a malignant tumor, the probability that y=1, is zero. So, it's like absolutely impossible that your tumor is malignant.But if it turns out that the tumor, the patient's tumor, actually is malignant, so if y is equal to one, even after we told them, that the probability of it happening is zero. So it's absolutely impossible for it to be malignant. But if we told them this with that level of certainty and we turn out to be wrong, then we penalize the learning algorithm by a very, very large cost. And that's captured by having this cost go to infinity if y equals 1 and h(x) approaches 0. This slide consider the case of y equals 1.
### y=0,Logistic Regression of Cost Function
![y=0,Logistic Regression of Cost Function](amWiki/images/001/03-Week3/1-Logistic Regression/18-y=0,逻辑回归中的代价函数方程.jpg)  
Let's look at what the cost function looks like for y equals 0.If y is equal to 0, then the cost looks like this, it looks like this expression over here, and if you plot the function, -log(1-z), what you get is the cost function actually looks like this. So it goes from 0 to 1, something like that and so if you plot the cost function for the case of y equals 0, you find that it looks like this. And what this curve does is it now goes up and it goes to plus infinity as h of x goes to 1 because as I was saying, that if y turns out to be equal to 0. But we predicted that y is equal to 1 with almost certainly, probably 1, then we end up paying a very large cost.Let's plot the cost function for the case of y=0. So if y=0, that's going to be our cost function, if you look at this expression and you plot -log(1-z), if you figure out what that looks like, you get a figure that looks like this which goes from 0 to a with the z axis on the horizontal axis. So if you take this cost function and plot it for the case of y=0, what you get is that the cost function.And, what this cost function does is that it goes up or it goes to a positive infinity as h of x goes to 1, and this catches the intuition that if a hypothesis predicted that your h of x is equal to 1 with certainty, with probably ones, absolutely gonna be y equals 1. But if y turns out to be equal to 0, then it makes sense to make the hypothesis. So the make the learning algorithm up here a very large cost. And conversely, if h of x is equal to 0 and y equals 0, then the hypothesis melted. The protected y of z is equal to 0, and it turns out y is equal to 0, so at this point, the cost function is going to be 0.In this video, we will define the cost function for a single train example. The topic of convexity analysis is now beyond the scope of this course, but it is possible to show that with a particular choice of cost function, this will give a convex optimization problem.Overall cost function j of theta will be convex and local optima free.In the next video we're gonna take these ideas of the cost function for a single training example and develop that further, and define the cost function for the entire training set. And we'll also figure out a simpler way to write it than we have been using so far, and based on that we'll work out grading descent, and that will give us logistic compression algorithm.
### Exam_in the video
![Exam_Convex Function](amWiki/images/001/03-Week3/1-Logistic Regression/19-内置习题_代价函数是凸函数.jpg)
![Exam_Logistic Regression of Cost Function](amWiki/images/001/03-Week3/1-Logistic Regression/20-内置习题_逻辑回归模型中代价函数.jpg)
