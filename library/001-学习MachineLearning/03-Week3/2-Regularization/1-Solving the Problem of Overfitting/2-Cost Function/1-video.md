# 正则化代价函数
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/03-Week3/2-Regularization/2-Cost Function.mp4" type="video/mp4">
</video>
## 中文
### 正则化代价函数直观感受
![正则化代价函数直观感受](amWiki/images/001/03-Week3/2-Regularization/7-正则化代价函数直观感受.jpg)  
在这段视频中 传达给你一个直观的感受 告诉你正规化是如何进行的 而且 我们还要写出 我们使用正规化时 需要使用的代价函数 根据我们幻灯片上的 这些例子 我想我可以给你一个直观的感受 但是 一个更好的 让你自己去理解正规化 如何工作的方法是  你自己亲自去实现它 并且看看它是如何工作的 如果在这节课后 你进行一些适当的练习 你就有机会亲自体验一下 正规化到底是怎么工作的 那么 这里就是一些直观解释 在前面的视频中 我们看到了 如果说我们要 用一个二次函数来 拟合这些数据 它给了我们一个对数据很好的拟合 然而 如果我们 用一个更高次的 多项式去拟合 我们最终 可能得到一个曲线 能非常好地拟合训练集 但是 这真的不是一个好的结果 它过度拟合了数据 因此 一般性并不是很好 让我们考虑下面的假设 我们想要加上惩罚项 从而使 参数 θ3 和 θ4 足够的小 这里我的意思就是 这是我们的优化目标 或者客观的说 这就是我们需要 优化的问题 我们需要尽量减少 代价函数的均方误差 对于这个函数 我们对它进行一些 添加一些项 加上 1000 乘以 θ3 的平方 再加上 1000 乘以 θ4 的平方 1000 只是我随便写的某个较大的数字而已 现在 如果我们要 最小化这个函数 为了使这个 新的代价函数最小化 我们要让 θ3 和 θ4 尽可能小 对吧？ 因为 如果你有 1000 乘以 θ3 这个 新的代价函数将会是很大的 所以 当我们最小化 这个新的函数时 我们将使 θ3 的值 接近于0 θ4 的值也接近于0 就像我们忽略了 这两个值一样 如果我们做到这一点 如果 θ3 和 θ4  接近0 那么我们 将得到一个近似的二次函数 所以 我们最终 恰当地拟合了数据 你知道 二次函数加上一些项 这些很小的项 贡献很小 因为 θ3 θ4 它们是非常接近于0的 所以 我们最终得到了 实际上 很好的一个二次函数 因为这是一个 更好的假设 在这个具体的例子中 我们看到了 惩罚这两个 大的参数值的效果
### 正则化背后的思路
![正则化背后的思路](amWiki/images/001/03-Week3/2-Regularization/8-正则化背后的思路.jpg)  
更一般地 这里给出了正规化背后的思路 这种思路就是 如果我们 的参数值 对应一个较小值的话 就是说 参数值比较小 那么往往我们会得到一个 形式更简单的假设 所以 我们最后一个例子中 我们惩罚的只是 θ3 和 θ4 使这两个 值均接近于零 我们得到了一个更简单的假设 也即这个假设大抵上是一个二次函数 但更一般地说 如果我们就像这样 惩罚的其它参数 通常我们 可以把它们都想成是 得到一个更简单的假设 因为你知道 当这些参数越接近这个例子时 假设的结果越接近 一个二次函数 但更一般地 可以表明 这些参数的值越小 通常对应于越光滑的函数 也就是更加简单的函数 因此 就不易发生过拟合的问题 我知道 为什么要所有的部分参数变小的这些原因 为什么越小的参数对应于一个简单的假设 我知道这些原因 对你来说现在不一定完全理解 但现在解释起来确实比较困难 除非你自己实现一下 自己亲自运行了这部分 但是我希望 这个例子中  使 θ3 和 θ4 很小 并且这样做 能给我们一个更加简单的 假设 我希望这个例子 有助于解释原因 至少给了 我们一些直观感受 为什么这应该是这样的 来让我们看看具体的例子 对于房屋价格预测我们 可能有上百种特征 我们谈到了一些可能的特征 比如说 x1 是房屋的尺寸 x2 是卧室的数目 x3 是房屋的层数等等 那么我们可能就有一百个特征 跟前面的多项式例子不同 我们是不知道的 对吧 我们不知道 θ3 θ4 是高阶多项式的项 所以 如果我们有一个袋子 如果我们有一百个特征 在这个袋子里 我们是很难 提前选出那些 关联度更小的特征的 也就是说如果我们有一百或一百零一个参数 我们不知道 挑选哪一个 我们并不知道 如何选择参数 如何缩小参数的数目 因此在正规化里 我们要做的事情 就是把我们的 代价函数 这里就是线性回归的代价函数 接下来我度量 来修改这个代价函数 从而缩小 我所有的参数值 因为你知道 我不知道是哪个 哪一个或两个要去缩小 所以我就修改我的 代价函数 在这后面添加一项 就像我们在方括号里的这项 当我添加一个额外的 正则化项的时候 我们收缩了每个 参数 并且因此 我们会使 我们所有的参数 θ1 θ2 θ3 直到 θ100 的值变小 顺便说一下 按照惯例来讲 我们从第一个这里开始 所以我实际上没有去惩罚 θ0 因此 θ0 的值是大的 这就是一个约定 从1到 n 的求和 而不是从0到 n 的求和 但其实在实践中 这只会有非常小的差异 无论你是否包括这项 就是 θ0 这项 实际上 结果只有非常小的差异 但是按照惯例 通常情况下我们还是只 从 θ1 到 θ100 进行正规化 这里我们写下来 我们的正规化优化目标
### 正则化代价函数表达式
![正则化代价函数表达式](amWiki/images/001/03-Week3/2-Regularization/9-正则化代价函数表达式.jpg)  
我们的正规化后的代价函数 就是这样的 J(θ) 这个项 右边的这项就是一个正则化项 并且 λ 在这里我们称做正规化参数  λ 要做的就是控制 在两个不同的目标中 的一个平衡关系 第一个目标 第一个需要抓住的目标 就是我们想要训练 使假设更好地拟合训练数据 我们希望假设能够很好的适应训练集 而第二个目标是 我们想要保持参数值较小 这就是第二项的目标 通过正则化目标函数 这就是λ 这个正则化 参数需要控制的 它会这两者之间的平衡 目标就是平衡拟合训练的目的 和 保持参数值较小的目的 从而来保持假设的形式相对简单 来避免过度的拟合 对于我们的房屋价格预测来说 这个例子 尽管我们之前有 我们已经用非常高的 高阶多项式来拟合 我们将会 得到一个 非常弯曲和复杂的曲线函数 就像这个 如果你还是用高阶多项式拟合 就是用这里所有的多项式特征来拟合的话 但现在我们不这样了 你只需要确保使用了 正规化目标的方法 那么你就可以得到 实际上是一个曲线 但这个曲线不是 一个真正的二次函数 而是更加的流畅和简单 也许就像这条紫红色的曲线一样 那么 你知道的 这样就得到了对于这个数据更好的假设 再一次说明下 我了解这部分有点难以明白 为什么加上 参数的影响可以具有 这种效果 但如果你 亲自实现了正规化 你将能够看到 这种影响的最直观的感受
### 正则化参数过大带来的问题
![正则化参数过大带来的问题](amWiki/images/001/03-Week3/2-Regularization/10-正则化参数过大带来的问题.jpg)  
在正规化线性回归中 如果 正规化参数值 被设定为非常大 那么将会发生什么呢？ 我们将会非常大地惩罚 参数θ1 θ2 θ3 θ4 也就是说  如果我们的假设是底下的这个 如果我们最终惩罚 θ1 θ2 θ3 θ4 在一个非常大的程度 那么我们 会使所有这些参数接近于零的 对不对？ θ1 将接近零 θ2 将接近零 θ3 和 θ4 最终也会接近于零 如果我们这么做 那么就是 我们的假设中 相当于去掉了这些项 并且使 我们只是留下了一个简单的假设
### 举例说明正则化参数过大导致欠拟合问题
![举例说明正则化参数过大导致欠拟合问题](amWiki/images/001/03-Week3/2-Regularization/11-举例说明正则化参数过大导致欠拟合问题.jpg)  
这个假设只能表明 那就是 房屋价格 就等于 θ0 的值 那就是类似于拟合了 一条水平直线 对于数据来说 这就是一个 欠拟合 (underfitting) 这种情况下这一假设 它是条失败的直线 对于训练集来说 这只是一条平滑直线 它没有任何趋势 它不会去趋向大部分训练样本的任何值 这句话的另​​一种方式来表达就是 这种假设有 过于强烈的"偏见" 或者 过高的偏差 (bais) 认为预测的价格只是 等于 θ0 并且 尽管我们的数据集 选择去拟合一条 扁平的直线 仅仅是一条 扁平的水平线 我画得不好 对于数据来说  这只是一条水平线 因此 为了使正则化运作良好 我们应当注意一些方面 应该去选择一个不错的 正则化参数 λ 并且当我们以后讲到多重选择时 在后面的课程中 我们将讨论 一种方法 一系列的方法来自动选择 正则化参数 λ 所以 这就是高度正则化的思路 回顾一下代价函数 为了使用正则化 在接下来的两段视频中 让我们 把这些概念 应用到 到线性回归和 逻辑回归中去 那么我们就可以让他们 避免过度拟合了
### 内置习题
![内置习题_正规化参数过大的影响](amWiki/images/001/03-Week3/2-Regularization/12-内置习题_正规化参数过大的影响.jpg)
## English
### Regularization Cost Function  Intuition
![Regularization Cost Function  Intuition](amWiki/images/001/03-Week3/2-Regularization/7-正则化代价函数直观感受.jpg)  
In this video, I'd like to convey to you, the main intuitions behind how regularization works. And, we'll also write down the cost function that we'll use, when we were using regularization. With the hand drawn examples that we have on these slides, I think I'll be able to convey part of the intuition. But, an even better way to see for yourself, how regularization works, is if you implement it, and, see it work for yourself. And, if you do the appropriate exercises after this, you get the chance to self see regularization in action for yourself. So, here is the intuition. In the previous video, we saw that, if we were to fit a quadratic function to this data, it gives us a pretty good fit to the data. Whereas, if we were to fit an overly high order degree polynomial, we end up with a curve that may fit the training set very well, but, really not be a, but overfit the data poorly, and, not generalize well. Consider the following, suppose we were to penalize, and, make the parameters theta 3 and theta 4 really small. Here's what I mean, here is our optimization objective, or here is our optimization problem, where we minimize our usual squared error cause function. Let's say I take this objective and modify it and add to it, plus 1000 theta 3 squared, plus 1000 theta 4 squared. 1000 I am just writing down as some huge number. Now, if we were to minimize this function, the only way to make this new cost function small is if theta 3 and data 4 are small, right? Because otherwise, if you have a thousand times theta 3, this new cost functions gonna be big. So when we minimize this new function we are going to end up with theta 3 close to 0 and theta 4 close to 0, and as if we're getting rid of these two terms over there. And if we do that, well then, if theta 3 and theta 4 close to 0 then we are being left with a quadratic function, and, so, we end up with a fit to the data, that's, you know, quadratic function plus maybe, tiny contributions from small terms, theta 3, theta 4, that they may be very close to 0. And, so, we end up with essentially, a quadratic function, which is good. Because this is a much better hypothesis. In this particular example, we looked at the effect of penalizing two of the parameter values being large.
### The idea behind Regularization
![The idea behind Regularization](amWiki/images/001/03-Week3/2-Regularization/8-正则化背后的思路.jpg)  
More generally, here is the idea behind regularization. The idea is that, if we have small values for the parameters, then, having small values for the parameters, will somehow, will usually correspond to having a simpler hypothesis. So, for our last example, we penalize just theta 3 and theta 4 and when both of these were close to zero, we wound up with a much simpler hypothesis that was essentially a quadratic function.But more broadly, if we penalize all the parameters usually that, we can think of that, as trying to give us a simpler hypothesis as well because when, you know, these parameters are as close as you in this example, that gave us a quadratic function. But more generally, it is possible to show that having smaller values of the parameters corresponds to usually smoother functions as well for the simpler. And which are therefore, also, less prone to overfitting. I realize that the reasoning for why having all the parameters be small. Why that corresponds to a simpler hypothesis; I realize that reasoning may not be entirely clear to you right now. And it is kind of hard to explain unless you implement yourself and see it for yourself. But I hope that the example of having theta 3 and theta 4 be small and how that gave us a simpler hypothesis, I hope that helps explain why, at least give some intuition as to why this might be true. Lets look at the specific example. For housing price prediction we may have our hundred features that we talked about where may be x1 is the size, x2 is the number of bedrooms, x3 is the number of floors and so on. And we may we may have a hundred features. And unlike the polynomial example, we don't know, right, we don't know that theta 3, theta 4, are the high order polynomial terms. So, if we have just a bag, if we have just a set of a hundred features, it's hard to pick in advance which are the ones that are less likely to be relevant. So we have a hundred or a hundred one parameters. And we don't know which ones to pick, we don't know which parameters to try to pick, to try to shrink. So, in regularization, what we're going to do, is take our cost function, here's my cost function for linear regression. And what I'm going to do is, modify this cost function to shrink all of my parameters, because, you know, I don't know which one or two to try to shrink. So I am going to modify my cost function to add a term at the end. Like so we have square brackets here as well. When I add an extra regularization term at the end to shrink every single parameter and so this term we tend to shrink all of my parameters theta 1, theta 2, theta 3 up to theta 100.By the way, by convention the summation here starts from one so I am not actually going penalize theta zero being large. That sort of the convention that, the sum I equals one through N, rather than I equals zero through N. But in practice, it makes very little difference, and, whether you include, you know, theta zero or not, in practice, make very little difference to the results. But by convention, usually, we regularize only theta through theta 100. Writing down our regularized optimization objective, our regularized cost function again.
### Regularization Cost Function Expression
![Regularization Cost Function Expression](amWiki/images/001/03-Week3/2-Regularization/9-正则化代价函数表达式.jpg)  
Here it is. Here's J of theta where, this term on the right is a regularization term and lambda here is called the regularization parameter and what lambda does, is it controls a trade off between two different goals. The first goal, capture it by the first goal objective, is that we would like to train, is that we would like to fit the training data well. We would like to fit the training set well. And the second goal is, we want to keep the parameters small, and that's captured by the second term, by the regularization objective. And by the regularization term. And what lambda, the regularization parameter does is the controls the trade of between these two goals, between the goal of fitting the training set well and the goal of keeping the parameter plan small and therefore keeping the hypothesis relatively simple to avoid overfitting. For our housing price prediction example, whereas, previously, if we had fit a very high order polynomial, we may have wound up with a very, sort of wiggly or curvy function like this. If you still fit a high order polynomial with all the polynomial features in there, but instead, you just make sure, to use this sole of regularized objective, then what you can get out is in fact a curve that isn't quite a quadratic function, but is much smoother and much simpler and maybe a curve like the magenta line that, you know, gives a much better hypothesis for this data. Once again, I realize it can be a bit difficult to see why strengthening the parameters can have this effect, but if you implement yourselves with regularization you will be able to see this effect firsthand.
### What if regularization parameter λ large?
![What if regularization parameter λ large](amWiki/images/001/03-Week3/2-Regularization/10-正则化参数过大带来的问题.jpg)  
In regularized linear regression, if the regularization parameter monitor is set to be very large, then what will happen is we will end up penalizing the parameters theta 1, theta 2, theta 3, theta 4 very highly. That is, if our hypothesis is this is one down at the bottom. And if we end up penalizing theta 1, theta 2, theta 3, theta 4 very heavily, then we end up with all of these parameters close to zero, right? Theta 1 will be close to zero; theta 2 will be close to zero. Theta three and theta four will end up being close to zero. And if we do that, it's as if we're getting rid of these terms in the hypothesis so that we're just left with a hypothesis that will say that.
### Example-Regularization Parameter λ large lead to Overfitting
![Example-Regularization Parameter λ large lead to Overfitting](amWiki/images/001/03-Week3/2-Regularization/11-举例说明正则化参数过大导致欠拟合问题.jpg)  
It says that, well, housing prices are equal to theta zero, and that is akin to fitting a flat horizontal straight line to the data. And this is an example of underfitting, and in particular this hypothesis, this straight line it just fails to fit the training set well. It's just a fat straight line, it doesn't go, you know, go near. It doesn't go anywhere near most of the training examples. And another way of saying this is that this hypothesis has too strong a preconception or too high bias that housing prices are just equal to theta zero, and despite the clear data to the contrary, you know chooses to fit a sort of, flat line, just a flat horizontal line. I didn't draw that very well. This just a horizontal flat line to the data. So for regularization to work well, some care should be taken, to choose a good choice for the regularization parameter lambda as well. And when we talk about multi-selection later in this course, we'll talk about a way, a variety of ways for automatically choosing the regularization parameter lambda as well. So, that's the idea of the high regularization and the cost function reviews in order to use regularization In the next two videos, lets take these ideas and apply them to linear regression and to logistic regression, so that we can then get them to avoid overfitting.
### Exam_in the video
![Exam_Regularization Parameter λ large lead to Overfitting](amWiki/images/001/03-Week3/2-Regularization/12-内置习题_正规化参数过大的影响.jpg)
