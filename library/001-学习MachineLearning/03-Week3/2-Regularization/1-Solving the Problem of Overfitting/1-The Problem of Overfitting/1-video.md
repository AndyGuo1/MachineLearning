# 过拟合问题
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/03-Week3/2-Regularization/1-The Problem of Overfitting.mp4" type="video/mp4">
</video>
## 中文
### 举例线性回归中的过拟合问题
![举例线性回归中的过拟合问题](amWiki/images/001/03-Week3/2-Regularization/1-举例线性回归中的过拟合问题.jpg)  
到现在为止 你已经见识了 几种不同的学习算法 包括线性回归和逻辑回归 它们能够有效地解决许多问题 但是当将它们应用到 某些特定的机器学习应用时 会遇到过度拟合(over-fitting)的问题 可能会导致它们效果很差 在这段视频中 我将为你解释 什么是过度拟合问题 并且 在此之后接下来的几个视频中 我们将谈论一种 称为正则化(regularization)的技术 它可以改善或者 减少过度拟合问题 以使学习算法更好实现 那么什么是过度拟合呢？ 让我们继续使用 那个用线性回归 来预测房价的例子 我们通过建立 以住房面积为自变量的函数来预测房价 我们可以 对该数据做线性回归 如果这么做 我们也许能够获得 拟合数据的这样一条直线 但是 这不是一个很好的模型 我们看看这些数据 很明显 随着房子面积增大 住房价格的变化趋于稳定 或者越往右越平缓 因此该算法 没有很好拟合训练数据 我们把这个问题称为欠拟合(underfitting) 这个问题的另一个术语叫做 高偏差(bias) 这两种说法大致相似 意思是它只是没有很好地拟合训练数据 这个词是 过去传下来的一个专业名词 它的意思是 如果拟合一条直线 到训练数据 就好像算法 有一个很强的偏见 或者说非常大的偏差 因为该算法认为房子价格与面积仅仅线性相关 尽管与该数据的事实相反 尽管相反的证据 被事前定义为 偏差 它还是接近于 拟合一条直线 而此法最终导致拟合数据效果很差 我们现在可以在中间 加入一个二次项 在这组数据中 我们用二次函数来拟合它 然后可以拟合出一条曲线 事实证明这个拟合效果很好 另一个极端情况是 如果我们拟合一个四次多项式 因此在这里我们有五个参数 θ0到θ4 这样我们可以拟合一条曲线 通过我们的五个训练样本 你可以得到看上去如此的一条曲线 一方面 似乎 对训练数据 做了一个很好的拟合 因为这条曲线通过了所有的训练实例 但是 这仍然是一条扭曲的曲线 对吧？ 它不停上下波动 因此事实上 我们并不认为它是一个预测房价的好模型 所以 这个问题我们把他叫做 过度拟合或过拟合(overfitting) 另一个描述该问题的术语是 高方差(variance) 高方差是另一个 历史上的叫法 但是 从第一印象上来说 如果我们拟合一个 高阶多项式 那么 这个函数能很好的拟合训练集 能拟合几乎所有的 训练数据 这就面临可能函数太过庞大的问题 变量太多 同时如果我们没有足够的数据 去约束这个变量过多的模型 那么这就是过度拟合 在两者之间的情况 叫"刚好合适" 这并不是一个真正的名词 我只是把它写在这里 这个二次多项式 二次函数 可以说是恰好拟合这些数据 概括地说 过度拟合的问题 将会在变量过多的时候 发生 这种时候训练出的方程总能很好的拟合训练数据 所以 你的代价函数 实际上可能非常接近于0 或者 就是0 但是 这样的曲线 它千方百计的拟合于训练数据 这样导致 它无法泛化到 新的数据样本中 以至于无法预测新样本价格 在这里 术语"泛化" 指的是一个假设模型能够应用到新样本的能力 新样本数据是 没有出现在训练集中的房子 在这张幻灯片上 我们看到了 线性回归情况下的过拟合
### 举例逻辑回归中的过拟合问题
![举例逻辑回归中的过拟合问题](amWiki/images/001/03-Week3/2-Regularization//2-举例逻辑回归中的过拟合问题.jpg)  
类似的方法同样可以应用到逻辑回归 这里是一个以x1与x2为变量的 逻辑回归 我们可以做的就是 用这样一个简单的假设模型 来拟合逻辑回归 和以前一样 字母g代表S型函数 如果这样做 你会得到一个假设模型 这个假设模型是一条直线 它直接分开了正样本和负样本 但这个模型并不能够很好的拟合数据 因此 这又是一个欠拟合的例子 或者说假设模型具有高偏差 相比之下 如果 如果再加入一些变量 比如这些二次项 那么你可以得到一个判定边界 像这样 这样就很好的拟合了数据 这很可能 是训练集的最好拟合结果 最后 在另一种极端情况下 如果你用高阶多项式来拟合数据 你加入了很多 高阶项 那么逻辑回归可能发生自身扭曲 它千方百计的 形成这样一个 判定边界 来拟合你的训练数据 以至于成为一条扭曲的曲线 使其能够拟合每一个训练集中的样本 而且 如果x1和x2 能够预测 癌症 你知道 癌症是一种恶性肿瘤 同时肿瘤也可能是良性 确实 这个假设模型不是一个很好的预测 因此 这又是一个过拟合例子 是一个 有高方差的假设模型 并且不能够很好泛化到新样本
### 识别过拟合
![识别过拟合](amWiki/images/001/03-Week3/2-Regularization//3-识别过拟合.jpg)  
在今后课程中 我们会讲到调试和诊断 诊断出导致学习算法故障的东西 我们告诉你如何用 专门的工具来识别 过拟合 和可能发生的欠拟合 但是 现在 让我们谈谈 过拟合 的问题 我们怎么样解决呢 在前面的例子中 当我们使用一维或二维数据时 我们可以通过绘出假设模型的图像来研究问题所在 再选择合适的多项式来拟合数据 因此 以之前的房屋价格为例 我们可以 绘制假设模型的图像 就能看到 模型的曲线 非常扭曲并通过所有样本房价 我们可以通过绘制这样的图形 来选择合适的多项式阶次 因此绘制假设模型曲线 可以作为决定多项式阶次 的一种方法 但是这并不是总是有用的 而且事实上更多的时候我们 会遇到有很多变量的假设模型 并且 这不仅仅是选择多项式阶次的问题 事实上 当我们 有这么多的特征变量 这也使得绘图变得更难 并且 更难使其可视化 因此并不能通过这种方法决定保留哪些特征变量 具体地说 如果我们试图 预测房价 同时又拥有这么多特征变量 这些变量看上去都很有用 但是 如果我们有 过多的变量 同时 只有非常少的训练数据 就会出现过度拟合的问题
### 解决过拟合问题方法
![解决过拟合问题方法](amWiki/images/001/03-Week3/2-Regularization//4-解决过拟合问题方法.jpg)  
为了解决过度拟合 有两个办法 来解决问题 第一个办法是要尽量 减少选取变量的数量 具体而言 我们可以人工检查 变量的条目 并以此决定哪些变量更为重要 然后 决定保留哪些特征变量 哪些应该舍弃 在今后的课程中 我们会提到模型选择算法 这种算法是为了自动选择 采用哪些特征变量 自动舍弃不需要的变量 这种减少特征变量 的做法是非常有效的 并且可以减少过拟合的发生 当我们今后讲到模型选择时 我们将深入探讨这个问题 但是其缺点是 舍弃一部分特征变量 你也舍弃了 问题中的一些信息 例如 也许所有的 特征变量 对于预测房价都是有用的 我们实际上并不想 舍弃一些信息 或者舍弃这些特征变量 第二个选择 我们将在接下来的视频中讨论 就是正则化 正则化中我们将保留 所有的特征变量 但是数量级 或参数数值的大小 θ(j) 这个方法非常有效 当我们有很多特征变量时 其中每一个变量 都能对预测产生一点影响 y的值 正如我们在房价的例子中看到的那样 在那里我们可以有很多特征变量 其中每一个变量 都是有用的 因此我们不希望把它们删掉 这就导致了 正则化概念的发生 我知道 这些东西你们现在可能还听不懂 但是在接下来的视频中 我们将开始详细讲述 怎样应用正则化和什么叫做正则化均值 然后我们将开始 讲解怎样使用正则化
### 内置习题
![内置习题_过拟合问题](amWiki/images/001/03-Week3/2-Regularization/5-内置习题_过拟合问题.jpg)
## English
### Example for Linear Regression Overfitting Problem
![Example for Linear Regression Overfitting Problem](amWiki/images/001/03-Week3/2-Regularization/1-举例线性回归中的过拟合问题.jpg)  
By now, you've seen a couple different learning algorithms, linear regression and logistic regression. They work well for many problems, but when you apply them to certain machine learning applications, they can run into a problem called overfitting that can cause them to perform very poorly. What I'd like to do in this video is explain to you what is this overfitting problem, and in the next few videos after this, we'll talk about a technique called regularization, that will allow us to ameliorate or to reduce this overfitting problem and get these learning algorithms to maybe work much better. So what is overfitting? Let's keep using our running example of predicting housing prices with linear regression where we want to predict the price as a function of the size of the house. One thing we could do is fit a linear function to this data, and if we do that, maybe we get that sort of straight line fit to the data. But this isn't a very good model. Looking at the data, it seems pretty clear that as the size of the housing increases, the housing prices plateau, or kind of flattens out as we move to the right and so this algorithm does not fit the training and we call this problem underfitting, and another term for this is that this algorithm has high bias. Both of these roughly mean that it's just not even fitting the training data very well. The term is kind of a historical or technical one, but the idea is that if a fitting a straight line to the data, then, it's as if the algorithm has a very strong preconception, or a very strong bias that housing prices are going to vary linearly with their size and despite the data to the contrary. Despite the evidence of the contrary is preconceptions still are bias, still closes it to fit a straight line and this ends up being a poor fit to the data. Now, in the middle, we could fit a quadratic functions enter and, with this data set, we fit the quadratic function, maybe, we get that kind of curve and, that works pretty well. And, at the other extreme, would be if we were to fit, say a fourth other polynomial to the data. So, here we have five parameters, theta zero through theta four, and, with that, we can actually fill a curve that process through all five of our training examples. You might get a curve that looks like this.That, on the one hand, seems to do a very good job fitting the training set and, that is processed through all of my data, at least. But, this is still a very wiggly curve, right? So, it's going up and down all over the place, and, we don't actually think that's such a good model for predicting housing prices. So, this problem we call overfitting, and, another term for this is that this algorithm has high variance.. The term high variance is another historical or technical one. But, the intuition is that, if we're fitting such a high order polynomial, then, the hypothesis can fit, you know, it's almost as if it can fit almost any function and this face of possible hypothesis is just too large, it's too variable. And we don't have enough data to constrain it to give us a good hypothesis so that's called overfitting. And in the middle, there isn't really a name but I'm just going to write, you know, just right. Where a second degree polynomial, quadratic function seems to be just right for fitting this data. To recap a bit the problem of over fitting comes when if we have too many features, then to learn hypothesis may fit the training side very well. So, your cost function may actually be very close to zero or may be even zero exactly, but you may then end up with a curve like this that, you know tries too hard to fit the training set, so that it even fails to generalize to new examples and fails to predict prices on new examples as well, and here the term generalized refers to how well a hypothesis applies even to new examples. That is to data to houses that it has not seen in the training set. On this slide, we looked at over fitting for the case of linear regression.
### Example for Logistic Regression Overfitting Problem
![Example for Logistic Regression Overfitting Problem](amWiki/images/001/03-Week3/2-Regularization/2-举例逻辑回归中的过拟合问题.jpg)  
A similar thing can apply to logistic regression as well. Here is a logistic regression example with two features X1 and x2. One thing we could do, is fit logistic regression with just a simple hypothesis like this, where, as usual, G is my sigmoid function. And if you do that, you end up with a hypothesis, trying to use, maybe, just a straight line to separate the positive and the negative examples. And this doesn't look like a very good fit to the hypothesis. So, once again, this is an example of underfitting or of the hypothesis having high bias. In contrast, if you were to add to your features these quadratic terms, then, you could get a decision boundary that might look more like this. And, you know, that's a pretty good fit to the data. Probably, about as good as we could get, on this training set. And, finally, at the other extreme, if you were to fit a very high-order polynomial, if you were to generate lots of high-order polynomial terms of speeches, then, logistical regression may contort itself, may try really hard to find a decision boundary that fits your training data or go to great lengths to contort itself, to fit every single training example well. And, you know, if the features X1 and X2 offer predicting, maybe, the cancer to the, you know, cancer is a malignant, benign breast tumors. This doesn't, this really doesn't look like a very good hypothesis, for making predictions. And so, once again, this is an instance of overfitting and, of a hypothesis having high variance and not really, and, being unlikely to generalize well to new examples.
### Address Overfitting
![Address Overfitting](amWiki/images/001/03-Week3/2-Regularization/3-识别过拟合.jpg)  
Later, in this course, when we talk about debugging and diagnosing things that can go wrong with learning algorithms, we'll give you specific tools to recognize when overfitting and, also, when underfitting may be occurring. But, for now, lets talk about the problem of, if we think overfitting is occurring, what can we do to address it? In the previous examples, we had one or two dimensional data so, we could just plot the hypothesis and see what was going on and select the appropriate degree polynomial. So, earlier for the housing prices example, we could just plot the hypothesis and, you know, maybe see that it was fitting the sort of very wiggly function that goes all over the place to predict housing prices. And we could then use figures like these to select an appropriate degree polynomial. So plotting the hypothesis, could be one way to try to decide what degree polynomial to use. But that doesn't always work. And, in fact more often we may have learning problems that where we just have a lot of features. And there is not just a matter of selecting what degree polynomial. And, in fact, when we have so many features, it also becomes much harder to plot the data and it becomes much harder to visualize it, to decide what features to keep or not. So concretely, if we're trying predict housing prices sometimes we can just have a lot of different features. And all of these features seem, you know, maybe they seem kind of useful. But, if we have a lot of features, and, very little training data, then, over fitting can become a problem.
### Solutions of slove Overfitting
![Solutions of slove Overfitting](amWiki/images/001/03-Week3/2-Regularization/4-解决过拟合问题方法.jpg)  
In order to address over fitting, there are two main options for things that we can do. The first option is, to try to reduce the number of features. Concretely, one thing we could do is manually look through the list of features, and, use that to try to decide which are the more important features, and, therefore, which are the features we should keep, and, which are the features we should throw out. Later in this course, where also talk about model selection algorithms. Which are algorithms for automatically deciding which features to keep and, which features to throw out. This idea of reducing the number of features can work well, and, can reduce over fitting. And, when we talk about model selection, we'll go into this in much greater depth. But, the disadvantage is that, by throwing away some of the features, is also throwing away some of the information you have about the problem. For example, maybe, all of those features are actually useful for predicting the price of a house, so, maybe, we don't actually want to throw some of our information or throw some of our features away. The second option, which we'll talk about in the next few videos, is regularization. Here, we're going to keep all the features, but we're going to reduce the magnitude or the values of the parameters theta J. And, this method works well, we'll see, when we have a lot of features, each of which contributes a little bit to predicting the value of Y, like we saw in the housing price prediction example. Where we could have a lot of features, each of which are, you know, somewhat useful, so, maybe, we don't want to throw them away. So, this subscribes the idea of regularization at a very high level. And, I realize that, all of these details probably don't make sense to you yet. But, in the next video, we'll start to formulate exactly how to apply regularization and, exactly what regularization means. And, then we'll start to figure out, how to use this, to make how learning algorithms work well and avoid overfitting.
### Exam_in the video
![Exam_Overfitting](amWiki/images/001/03-Week3/2-Regularization/5-内置习题_过拟合问题.jpg)
