# 正则化线性回归
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/03-Week3/2-Regularization/3-Regularized Linear Regression.mp4" type="video/mp4">
</video>
## 中文
### 正则化线性回归代价函数表达式
![正则化线性回归代价函数表达式](amWiki/images/001/03-Week3/2-Regularization/14-正则化线性回归代价函数表达式.jpg)  
对于线性回归的求解 我们之前 推导了两种学习算法 一种基于梯度下降 一种基于正规方程 在这段视频中 我们将继续学习 这两个算法 并把它们推广 到正则化线性回归中去 这是我们上节课推导出的 正则化线性回归的 优化目标 前面的第一部分是 一般线性回归的目标函数 而现在我们有这个额外的 正则化项 其中 λ 是正则化参数 我们想找到参数 θ 能最小化代价函数 即这个正则化代价函​​数 J(θ)
### 正则化线性回归梯度下降算法
![正则化线性回归梯度下降算法](amWiki/images/001/03-Week3/2-Regularization/15-正则化线性回归梯度下降算法.jpg)  
之前 我们使用 梯度下降求解原来 没有正则项的代价函数 我们用 下面的算法求解常规的 没有正则项的线性回归 我们会如此反复更新 参数 θj 其中 j=0, 1, 2...n 让我 照这个把 j=0 即 θ0 的情况单独写出来 我只是把 θ0 的更新 分离出来 剩下的这些参数θ1, θ2 到θn的更新 作为另一部分 所以 这样做其实没有什么变化 对吧？ 这只是把 θ0 的更新 这只是把 θ0 的更新 和 θ1 θ2 到 θn 的更新分离开来 和 θ1 θ2 到 θn 的更新分离开来 我这样做的原因是 你可能还记得 对于正则化的线性回归 我们惩罚参数θ1 θ2...一直到 θn 但是我们不惩罚θ0 所以 当我们修改这个 正则化线性回归的算法时 我们将对 θ0 的方式将有所不同 具体地说 如果我们 要对这个算法进行 修改 并用它 求解正则化的目标函数 我们 需要做的是 把下边的这一项做如下的修改 我们要在这一项上添加一项: λ 除以 m 再乘以 θj  如果这样做的话 那么你就有了 用于最小化 正则化代价函数 J(θ)  的梯度下降算法 我不打算用 微积分来证明这一点 但如果你看这一项 方括号里的这一项 如果你知道微积分 应该不难证明它是 J(θ) 对 θj 的偏导数 这里的 J(θ) 是用的新定义的形式 它的定义中 包含正则化项 而另一项 上面的这一项 我用青色的方框 圈出来的这一项 这也一个是偏导数 是 J(θ)对 θ0 的偏导数 如果你仔细看 θj 的更新 你会发现一些 有趣的东西 具体来说 θj 的每次更新 都是 θj 自己减去 α 乘以原来的无正则项 然后还有这另外的一项 这一项的大小也取决于 θj 所以 如果你 把所有这些 取决于 θj 的合在一起的话 可以证明 这个更新 可以等价地写为 如下的形式 具体来讲 上面的 θj 对应下面的 θj 乘以括号里的1 而这一项是 λ 除以 m 还有一个α 把它们合在一起 所以你最终得到 α 乘以 λ 再除以 m 然后合在一起 乘以 θj 而这一项 1 减去 α 乘以 λ 除以 m 这一项很有意思 具体来说 这一项 1 减去 α 乘以 λ 除以 m 这一项的值 通常是一个具体的实数 而且小于1 对吧？由于 α 乘以 λ 除以 m 通常情况下是正的 如果你的学习速率小 而 m 很大的话 (1 - αλ/m) 这一项通常是很小的 所以这里的一项 一般来说将是一个比1小一点点的值 所以我们可以把它想成 一个像0.99一样的数字 所以 对 θj 更新的结果 我们可以看作是 被替换为 θj 的0.99倍 也就是 θj 乘以0.99 把 θj 向 0 压缩了一点点 所以这使得 θj 小了一点 更正式地说 θj 的平方范数 更小了 另外 这一项后边的第二项 这实际上 与我们原来的 梯度下降更新完全一样 跟我们加入了正则项之前一样 好的 现在你应该对这个 梯度下降的更新没有疑问了 当我们使用正则化线性回归时 我们需要做的就是 在每一个被正规化的参数 θj 上 乘以了一个 比1小一点点的数字 也就是把参数压缩了一点 然后 我们执行跟以前一样的更新 当然 这仅仅是 从直观上认识 这个更新在做什么 从数学上讲 它就是带有正则化项的 J(θ)  的梯度下降算法 我们在之前的幻灯片 给出了定义 梯度下降只是 我们拟合线性回归模型的两种算法的其中一个
### 正则化线性回归正规方程表达式
![正则化线性回归正规方程表达式](amWiki/images/001/03-Week3/2-Regularization/16-正则化线性回归正规方程表达式.jpg)  
第二种算法是 使用正规方程 我们的做法 是建立这个 设计矩阵 X 其中每一行 对应于一个单独的训练样本 然后创建了一个向量 y 向量 y 是一个 m 维的向量 m 维的向量 包含了所有训练集里的标签 所以 X 是一个 m × (n+1) 维矩阵 y 是一个 m 维向量 y 是一个 m 维向量 为了最小化代价函数 J 我们发现 一个办法就是 一个办法就是 让 θ 等于这个式子 即 X 的转置乘以 X 再对结果取逆 再乘以 X 的转置乘以Y 我在这里留点空间 等下再填满 这个 θ 的值 其实就是最小化 代价函数 J(θ) 的θ值 这时的代价函数J(θ)没有正则项 现在如果我们用了是正则化 我们想要得到最小值 我们想要得到最小值 我们来看看应该怎么得到 我们来看看应该怎么得到 推导的方法是 取 J 关于各个参数的偏导数 并令它们 等于0 然后做些 数学推导 你可以 得到这样的一个式子 它使得代价函数最小 具体的说 如果你 使用正则化 那么公式要做如下改变 括号里结尾添这样一个矩阵 0 1 1 1 等等  直到最后一行 所以这个东西在这里是 一个矩阵 它的左上角的元素是0 其余对角线元素都是1 剩下的元素也都是 0 我画的比较随意 可以举一个例子 如果 n 等于2 那么这个矩阵 将是一个3 × 3 矩阵 更一般地情况 该矩阵是 一个 (n+1) × (n+1) 维的矩阵 一个 (n+1) × (n+1) 维的矩阵 因此 n 等于2时 矩阵看起来会像这样 左上角是0  然后其他对角线上是1 其余部分都是0 同样地 我不打算对这些作数学推导 坦白说这有点费时耗力 但可以证明 如果你采用新定义的 J(θ) 如果你采用新定义的 J(θ) 包含正则项的目标函数 那么这个计算 θ 的式子 能使你的 J(θ)  达到全局最小值
### 正则化可解决线性回归代价函数导数项【X的转置*X】不可逆问题【奇异矩阵/退化矩阵】
![正则化可解决线性回归代价函数导数项【X的转置*X】不可逆问题【奇异矩阵/退化矩阵】](amWiki/images/001/03-Week3/2-Regularization/17-正则化可解决线性回归代价函数导数项【X的转置X】不可逆问题【奇异矩阵退化矩阵】.jpg)  
所以最后 我想快速地谈一下不可逆性的问题 这部分是比较高阶的内容 所以这一部分还是作为选学 你可以跳过去 或者你也可以听听 如果听不懂的话 也没有关系 之前当我讲正规方程的时候 我们也有一段选学视频 讲不可逆的问题 所以这是另一个选学内容 可以作为上次视频的补充 可以作为上次视频的补充 现在考虑 m 即样本总数 小与或等于特征数量 n 如果你的样本数量 比特征数量小的话 那么这个矩阵 X 转置乘以 X 将是 不可逆或奇异的(singluar)  或者用另一种说法是 这个矩阵是 退化(degenerate)的 如果你在 Octave 里运行它 无论如何 你用函数 pinv 取伪逆矩阵 这样计算 理论上方法是正确的 但实际上 你不会得到一个很好的假设 尽管 Ocatve 会 用 pinv 函数 给你一个数值解 看起来还不错 但是 如果你是在一个不同的编程语言中 如果在 Octave 中 你用 inv 来取常规逆 你用 inv 来取常规逆 也就是我们要对 X 转置乘以 X 取常规逆 然后在这样的情况下 你会发现 X 转置乘以 X 是奇异的 是不可逆的 即使你在不同的 编程语言里计算 并使用一些 线性代数库 试图计算这个矩阵的逆矩阵 都是不可行的 因为这个矩阵是不可逆的或奇异的 幸运的是 正规化也 为我们解决了这个问题 具体地说 只要正则参数是严格大于0的 实际上 可以 证明该矩阵 X 转置 乘以 X 加上 λ 乘以 这里这个矩阵 可以证明 这个矩阵将不是奇异的 即该矩阵将是可逆的 因此 使用正则化还可以 照顾一些 X 转置乘以 X 不可逆的问题 照顾一些 X 转置乘以 X 不可逆的问题 好的 你现在知道了如何实现正则化线性回归 利用它 你就可以 避免过度拟合 即使你在一个相对较小的训练集里有很多特征 这应该可以让你 在很多问题上更好地运用线性回归 在接下来的视频中 我们将 把这种正则化的想法应用到逻辑回归 这样你就可以 让逻辑回归也避免过度拟合 并让它表现的更好
### 内置习题
![内置习题_正规化参数过大的影响](amWiki/images/001/03-Week3/2-Regularization/18-内置习题_线性回归正则化.jpg)
## English
### Regularization Linear Regression of Cost Function
![Regularization Linear Regression of Cost Function](amWiki/images/001/03-Week3/2-Regularization/14-正则化线性回归代价函数表达式.jpg)  
For linear regression, we have previously worked out two learning algorithms. One based on gradient descent and one based on the normal equation. In this video, we'll take those two algorithms and generalize them to the case of regularized linear regression.Here's the optimization objective that we came up with last time for regularized linear regression. This first part is our usual objective for linear regression. And we now have this additional regularization term, where lambda is our regularization parameter, and we like to find parameters theta that minimizes this cost function, this regularized cost function, J of theta.
### Regularization Linear Regression of Gradient Descent
![Regularization Linear Regression of Gradient Descent](amWiki/images/001/03-Week3/2-Regularization/15-正则化线性回归梯度下降算法.jpg)  
Previously, we were using gradient descent for the original cost function without the regularization term. And we had the following algorithm, for regular linear regression, without regularization, we would repeatedly update the parameters theta J as follows for J equals 0, 1, 2, up through n.Let me take this and just write the case for theta 0 separately. So I'm just going to write the update for theta 0 separately than for the update for the parameters 1, 2, 3, and so on up to n. And so this is, I haven't changed anything yet, right. This is just writing the update for theta 0 separately from the updates for theta 1, theta 2, theta 3, up to theta n. And the reason I want to do this is you may remember that for our regularized linear regression, we penalize the parameters theta 1, theta 2, and so on up to theta n. But we don't penalize theta 0. So, when we modify this algorithm for regularized linear regression, we're going to end up treating theta zero slightly differently.Concretely, if we want to take this algorithm and modify it to use the regular rise objective, all we need to do is take this term at the bottom and modify it as follows. We'll take this term and add minus lambda over m times theta j. And if you implement this, then you have gradient descent for trying to minimize the regularized cost function, j of theta. And concretely, I'm not gonna do the calculus to prove it, but concretely if you look at this term, this term hat I've written in square brackets, if you know calculus it's possible to prove that that term is the partial derivative with respect to J of theta using the new definition of J of theta with the regularization term. And similarly, this term up on top which I'm drawing the cyan box, that's still the partial derivative respect of theta zero of J of theta. If you look at the update for theta j, it's possible to show something very interesting. Concretely, theta j gets updated as theta j minus alpha times, and then you have this other term here that depends on theta J. So if you group all the terms together that depend on theta j, you can show that this update can be written equivalently as follows. And all I did was add theta j here is, so theta j times 1. And this term is, right, lambda over m, there's also an alpha here, so you end up with alpha lambda over m multiplied into theta j.And this term here, 1 minus alpha times lambda m, is a pretty interesting term. It has a pretty interesting effect.Concretely this term, 1 minus alpha times lambda over m, is going to be a number that is usually a little bit less than one, because alpha times lambda over m is going to be positive, and usually if your learning rate is small and if m is large, this is usually pretty small. So this term here is gonna be a number that's usually a little bit less than 1, so think of it as a number like 0.99, let's say. And so the effect of our update to theta j is, we're going to say that theta j gets replaced by theta j times 0.99, right?So theta j times 0.99 has the effect of shrinking theta j a little bit towards zero. So this makes theta j a bit smaller. And more formally, this makes the square norm of theta j a little bit smaller. And then after that, the second term here, that's actually exactly the same as the original gradient descent update that we had, before we added all this regularization stuff. So, hopefully this gradient descent, hopefully this update makes sense. When we're using a regularized linear regression and what we're doing is on every iteration we're multiplying theta j by a number that's a little bit less then one, so its shrinking the parameter a little bit, and then we're performing a similar update as before. Of course that's just the intuition behind what this particular update is doing. Mathematically what it's doing is it's exactly gradient descent on the cost function J of theta that we defined on the previous slide that uses the regularization term. Gradient descent was just one of our two algorithms for fitting a linear regression model.
### Regularization Linear Regression Normal Equation
![Regularization Linear Regression Normal Equation](amWiki/images/001/03-Week3/2-Regularization/16-正则化线性回归正规方程表达式.jpg)  
The second algorithm was the one based on the normal equation, where what we did was we created the design matrix X where each row corresponded to a separate training example. And we created a vector y, so this is a vector, that's an m dimensional vector. And that contained the labels from my training set. So whereas X is an m by (n+1) dimensional matrix, y is an m dimensional vector. And in order to minimize the cost function J, we found that one way to do so is to set theta to be equal to this. Right, you have X transpose X, inverse, X transpose Y. I'm leaving room here to fill in stuff of course. And what this value for theta does is this minimizes the cost function J of theta, when we were not using regularization.Now that we are using regularization, if you were to derive what the minimum is, and just to give you a sense of how to derive the minimum, the way you derive it is you take partial derivatives with respect to each parameter. Set this to zero, and then do a bunch of math and you can then show that it's a formula like this that minimizes the cost function. And concretely, if you are using regularization, then this formula changes as follows. Inside this parenthesis, you end up with a matrix like this. 0, 1, 1, 1, and so on, 1, until the bottom. So this thing over here is a matrix whose upper left-most entry is 0. There are ones on the diagonals, and then zeros everywhere else in this matrix. Because I'm drawing this rather sloppily. But as a example, if n = 2, then this matrix is going to be a three by three matrix. More generally, this matrix is an (n+1) by (n+1) dimensional matrix. So if n = 2, then that matrix becomes something that looks like this. It would be 0, and then 1s on the diagonals, and then 0s on the rest of the diagonals. And once again, I'm not going to show this derivation, which is frankly somewhat long and involved, but it is possible to prove that if you are using the new definition of J of theta, with the regularization objective, then this new formula for theta is the one that we give you, the global minimum of J of theta.
### Regularization solve the problem for Linear Regression of Cost Function Derivative term  Non-invertible
![Regularization solve the problem for Linear Regression of Cost Function Derivative term  Non-invertible](amWiki/images/001/03-Week3/2-Regularization/17-正则化可解决线性回归代价函数导数项【X的转置X】不可逆问题【奇异矩阵退化矩阵】.jpg)  
So finally I want to just quickly describe the issue of non-invertibility. This is relatively advanced material, so you should consider this as optional. And feel free to skip it, or if you listen to it and positive it doesn't really make sense, don't worry about it either. But earlier when I talked about the normal equation method, we also had an optional video on the non-invertibility issue. So this is another optional part to this, sort of an add-on to that earlier optional video on non-invertibility. Now, consider a setting where m, the number of examples, is less than or equal to n, the number of features. If you have fewer examples than features, than this matrix, X transpose X will be non-invertible, or singular. Or the other term for this is the matrix will be degenerate. And if you implement this in Octave anyway and you use the pinv function to take the pseudo inverse, it will kind of do the right thing, but it's not clear that it would give you a very good hypothesis, even though numerically the Octave pinv function will give you a result that kinda makes sense.But if you were doing this in a different language, and if you were taking just the regular inverse, which in Octave denoted with the function inv, we're trying to take the regular inverse of X transpose X. Then in this setting, you find that X transpose X is singular, is non-invertible, and if you're doing this in different program language and using some linear algebra library to try to take the inverse of this matrix, it just might not work because that matrix is non-invertible or singular. Fortunately, regularization also takes care of this for us. And concretely, so long as the regularization parameter lambda is strictly greater than 0, it is actually possible to prove that this matrix, X transpose X plus lambda times this funny matrix here, it is possible to prove that this matrix will not be singular and that this matrix will be invertible. So using regularization also takes care of any non-invertibility issues of the X transpose X matrix as well. So you now know how to implement regularized linear regression. Using this you'll be able to avoid overfitting even if you have lots of features in a relatively small training set. And this should let you get linear regression to work much better for many problems. In the next video we'll take this regularization idea and apply it to logistic regression. So that you'd be able to get logistic regression to avoid overfitting and perform much better as well.
### Exam_in the video
![Exam_Regularization Linear Regression](amWiki/images/001/03-Week3/2-Regularization/18-内置习题_线性回归正则化.jpg)
