# 正则化线性回归
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/03-Week3/2-Regularization/3-Regularized Linear Regression.mp4" type="video/mp4">
</video>
## 中文
### 正则化逻辑回归代价函数表达式
![正则化逻辑回归代价函数表达式](amWiki/images/001/03-Week3/2-Regularization/20-正则化逻辑回归代价函数表达式.jpg)  
对于逻辑回归，我们之前讨论过两种类型的优化算法。我们讨论了如何利用梯度下降法来优化成本函数Jθ。我们还讨论了高级优化方法。那些要求你提供一种方法来计算你的成本函数Jθ的方法你提供了一种计算导数的方法。在这个视频中，我们将展示如何使这两种技术，梯度下降和更高级的优化技术，使它们为正规化的逻辑回归工作。这是这个想法。我们之前看到过，如果你用一个非常，高阶的多项式特征来适应，逻辑回归也很容易被过度拟合。G是sigmoid函数，特别是你最后得到的是一个假设，你知道，它的决定一定是一个过于复杂和非常扭曲的函数，对于这个训练集来说，这并不是一个很好的假设，如果你有很多特征的逻辑回归。不一定是多项式，但只要有很多特性，你就可以用过度拟合。这是逻辑回归的成本函数。如果我们想要修改它来使用正规化，我们需要做的就是加上下面的项，加上londer超过2M，从J = 1求和，和J = 1的一般求和。而不是J = 0的和，θJ的平方。因此，这就需要对参数θ1θ2进行惩罚，从而使θ(N)变得太大。如果你这样做，那么它就会有这样的效果即使你用很多参数拟合一个非常高的阶多项式。只要你应用正规化并保持参数小你就更有可能得到一个决策边界。你知道，也许看起来更像这样。它看起来更适合分开正面和反面的数据。因此,当使用正则化即使有很多特征量，正则化也可以帮助解决过度拟合的问题。
### 正则化逻辑回归梯度下降算法
![正则化逻辑回归梯度下降算法](amWiki/images/001/03-Week3/2-Regularization/21-正则化逻辑回归梯度下降算法.jpg)  
我们如何实现这个呢?对于原来的梯度下降算法，这是我们的更新。我们将重复执行下面的更新到θj。这张幻灯片看起来很像之前的线性回归。但是我要做的是分别写出θ0的更新。第一行是关于θ0的更新，第二行现在是θ1到θN的更新。因为我将分别处理θ0。为了修改这个算法，可以使用一个正规化的cos函数，我所需要做的和我们对线性回归做的类似，实际上只是修改第二个更新规则如下。再一次，这个，你知道，表面上看起来和线性回归是一样的。当然，这和我们的算法不一样，因为现在的假设是用这个来定义的。这与正规化线性回归不同。因为假设是不同的。即使我写下的更新。它实际上看起来和我们之前的一样。我们正在研究正规化线性回归的梯度下降。当然，为了结束这个讨论，这一项在方括号里，这一项，这一项，这一项，当然是，关于θJ的新偏导数J(J)θ。这里的J(θ)是我们在之前的幻灯片中定义的使用正则化的成本函数。这是正则线性回归的梯度下降。
### 正则化逻辑回归高级优化算法
![正则化逻辑回归高级优化算法](amWiki/images/001/03-Week3/2-Regularization/22-正则化逻辑回归高级优化算法.jpg)  
让我们来讨论如何使用更高级的优化方法来实现正规化线性回归。为了提醒你们这些方法我们需要做的是定义这个函数，叫做成本函数，它让我们输入参数向量θ我们在这里写下的方程中，我们用了0个索引向量。所以我们有θ0到θn，但是因为Octave指数从1开始。θ0的书写形式为θ1。θ1以八度的形式写成θ2，以此类推到θN+1。我们需要做的是提供一个函数。让我们提供一个叫做成本函数的函数，然后把它传递给我们所拥有的，我们之前看到的。我们将使用fminunc，然后在成本函数中，等等,对吧。但F(min,u,c)是F min，这和fminunc有关这将会得到成本函数并将其最小化。因此，返回的成本函数所需的两个主要因素是j - val。为此，我们需要编写代码来计算θ的函数J。现在，当我们使用正规化的逻辑回归时，当然，成本函数jθ变化，特别是，现在，一个成本函数还需要包括这个额外的正则化项。所以，当你计算j的θ时，一定要在最后加上这个项。另外，这个成本函数需要用梯度来推导。因此，梯度向量需要被设置成Jθ关于θ零的偏导数，梯度2需要被设置为这个，以此类推。再一次，这个指数是一个。对，因为有一个八度的用户索引。看看这些术语。这个词在这里。我们在之前的幻灯片中实际算过这个。它不会改变。因为θ零的导数不变。与没有正规化的版本相比。换句话说就是变化。特别是对θ1的导数。我们在之前的幻灯片中也算过。等于，你知道，原来的项，然后减去londer M乘以θ1。我们要确保正确地传递信息。我们可以在这里加个括号。对，求和没有延伸。类似地，这里的另一个术语看起来是这样的，这个额外的术语我们在之前的幻灯片中，对应于它们的正规化目标的梯度。因此，如果你执行这个成本函数并将其传递到fminunc或其中一个高级优化技术，那将最小化新的正规化成本函数J(θ)。而你得到的的参数将是与正则化的逻辑回归相对应的参数。现在你知道如何实现正规化的逻辑回归了。当我在硅谷漫步时，我住在硅谷，有很多工程师都是坦率的，他们用机器学习算法赚了很多钱。我知道我们只是在研究这些东西。但是如果你理解了线性的回归，高级的的优化算法和正规化，现在，坦白地说，你可能知道的机器学习比很多人多，当然，现在你可能知道更多的机器学习，但是，很多硅谷的工程师在那里有非常成功的职业生涯。你知道，为公司赚大钱。或者使用机器学习算法构建产品。所以,恭喜你。实际上，你已经走了很长一段路。实际上，你知道如何应用这些东西并解决很多问题。那么恭喜你。当然，还有很多我们想要教你的，在接下来的视频中，我们会开始讨论非线性分类器的一个非常强大的原因。而线性回归，逻辑回归，你知道，你可以形成多项式的术语，但结果是有更强大的非线性量词可以进行多项式回归。在接下来的一集视频中，我会告诉你们。这样你就有了比现在更强大的学习算法来解决不同的问题。
### 内置习题
![内置习题_正则化逻辑回归](amWiki/images/001/03-Week3/2-Regularization/23-内置习题_正则化逻辑回归.jpg)
## English
### Regularization Logistic Regression of Cost Function
![Regularization Logistic Regression of Cost Function](amWiki/images/001/03-Week3/2-Regularization/20-正则化逻辑回归代价函数表达式.jpg)  
For logistic regression, we previously talked about two types of optimization algorithms. We talked about how to use gradient descent to optimize as cost function J of theta. And we also talked about advanced optimization methods. Ones that require that you provide a way to compute your cost function J of theta and that you provide a way to compute the derivatives.In this video, we'll show how you can adapt both of those techniques, both gradient descent and the more advanced optimization techniques in order to have them work for regularized logistic regression.So, here's the idea. We saw earlier that Logistic Regression can also be prone to overfitting if you fit it with a very, sort of, high order polynomial features like this. Where G is the sigmoid function and in particular you end up with a hypothesis, you know, whose decision bound to be just sort of an overly complex and extremely contortive function that really isn't such a great hypothesis for this training set, and more generally if you have logistic regression with a lot of features. Not necessarily polynomial ones, but just with a lot of features you can end up with overfitting.This was our cost function for logistic regression. And if we want to modify it to use regularization, all we need to do is add to it the following term plus londer over 2M, sum from J equals 1, and as usual sum from J equals 1. Rather than the sum from J equals 0, of theta J squared. And this has to effect therefore, of penalizing the parameters theta 1 theta 2 and so on up to theta N from being too large.And if you do this,then it will the have the effect that even though you're fitting a very high order polynomial with a lot of parameters. So long as you apply regularization and keep the parameters small you're more likely to get a decision boundary.You know, that maybe looks more like this. It looks more reasonable for separating the positive and the negative examples.So, when using regularization even when you have a lot of features, the regularization can help take care of the overfitting problem.
### Regularization Logistic Regression of Gradient Descent
![Regularization Logistic Regression of Gradient Descent](amWiki/images/001/03-Week3/2-Regularization/21-正则化逻辑回归梯度下降算法.jpg)  
How do we actually implement this? Well, for the original gradient descent algorithm, this was the update we had. We will repeatedly perform the following update to theta J. This slide looks a lot like the previous one for linear regression. But what I'm going to do is write the update for theta 0 separately. So, the first line is for update for theta 0 and a second line is now my update for theta 1 up to theta N. Because I'm going to treat theta 0 separately. And in order to modify this algorithm, to use a regularized cos function, all I need to do is pretty similar to what we did for linear regression is actually to just modify this second update rule as follows.And, once again, this, you know, cosmetically looks identical what we had for linear regression. But of course is not the same algorithm as we had, because now the hypothesis is defined using this. So this is not the same algorithm as regularized linear regression. Because the hypothesis is different. Even though this update that I wrote down. It actually looks cosmetically the same as what we had earlier. We're working out gradient descent for regularized linear regression.And of course, just to wrap up this discussion, this term here in the square brackets, so this term here, this term is, of course, the new partial derivative for respect of theta J of the new cost function J of theta. Where J of theta here is the cost function we defined on a previous slide that does use regularization.So, that's gradient descent for regularized linear regression.
### Regularization Advanced Optimization of Cost Function
![Regularization Advanced Optimization of Cost Function](amWiki/images/001/03-Week3/2-Regularization/22-正则化逻辑回归高级优化算法.jpg)  
Let's talk about how to get regularized linear regression to work using the more advanced optimization methods.And just to remind you for those methods what we needed to do was to define the function that's called the cost function, that takes us input the parameter vector theta and once again in the equations we've been writing here we used 0 index vectors. So we had theta 0 up to theta N. But because Octave indexes the vectors starting from 1. Theta 0 is written in Octave as theta 1. Theta 1 is written in Octave as theta 2, and so on down to theta N plus 1. And what we needed to do was provide a function. Let's provide a function called cost function that we would then pass in to what we have, what we saw earlier. We will use the fminunc and then you know at cost function,and so on, right. But the F min, u and c was the F min unconstrained and this will work with fminunc was what will take the cost function and minimize it for us.So the two main things that the cost function needed to return were first J-val. And for that, we need to write code to compute the cost function J of theta.Now, when we're using regularized logistic regression, of course the cost function j of theta changes and, in particular,now a cost function needs to include this additional regularization term at the end as well. So, when you compute j of theta be sure to include that term at the end.And then, the other thing that this cost function thing needs to derive with a gradient. So gradient one needs to be set to the partial derivative of J of theta with respect to theta zero, gradient two needs to be set to that, and so on. Once again, the index is off by one. Right, because of the indexing from one Octave users.And looking at these terms.This term over here. We actually worked this out on a previous slide is actually equal to this. It doesn't change. Because the derivative for theta zero doesn't change. Compared to the version without regularization.And the other terms do change. And in particular the derivative respect to theta one. We worked this out on the previous slide as well. Is equal to, you know, the original term and then minus londer M times theta 1. Just so we make sure we pass this correctly. And we can add parentheses here. Right, so the summation doesn't extend. And similarly, you know, this other term here looks like this, with this additional term that we had on the previous slide, that corresponds to the gradient from their regularization objective. So if you implement this cost function and pass this into fminunc or to one of those advanced optimization techniques, that will minimize the new regularized cost function J of theta.And the parameters you get out will be the ones that correspond to logistic regression with regularization.So, now you know how to implement regularized logistic regression.When I walk around Silicon Valley, I live here in Silicon Valley, there are a lot of engineers that are frankly, making a ton of money for their companies using machine learning algorithms.And I know we've only been, you know, studying this stuff for a little while. But if you understand linear regression, the advanced optimization algorithms and regularization, by now, frankly, you probably know quite a lot more machine learning than many, certainly now, but you probably know quite a lot more machine learning right now than frankly, many of the Silicon Valley engineers out there having very successful careers. You know, making tons of money for the companies. Or building products using machine learning algorithms.So, congratulations.You've actually come a long ways. And you can actually, you actually know enough to apply this stuff and get to work for many problems.So congratulations for that. But of course, there's still a lot more that we want to teach you, and in the next set of videos after this, we'll start to talk about a very powerful cause of non-linear classifier. So whereas linear regression, logistic regression, you know, you can form polynomial terms, but it turns out that there are much more powerful nonlinear quantifiers that can then sort of polynomial regression. And in the next set of videos after this one, I'll start telling you about them. So that you have even more powerful learning algorithms than you have now to apply to different problems.
### Exam_in the video
![Exam_Regularization Logistic Regression](amWiki/images/001/03-Week3/2-Regularization/23-内置习题_正则化逻辑回归.jpg)
