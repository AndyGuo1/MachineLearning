# SVM支持向量机-优化目标【代价函数】
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/07-Week7/1-Support Vector Machines/1-Optimization Objective.mp4" type="video/mp4">
</video>
## 中文
### 回顾逻辑回归假设函数
![回顾逻辑回归假设函数](amWiki/images/001/07-Week7/1-Support Vector Machines/1-回顾逻辑回归假设函数.jpg)
到目前为止 你已经见过一系列不同的学习算法 在监督学习中 许多学习算法的性能 都非常类似 因此 重要的不是 你该选择使用 学习算法A还是学习算法B 而更重要的是 应用这些算法时 所创建的 大量数据 在应用这些算法时 表现情况通常依赖于你的水平 比如 你为学习算法 所设计的 特征量的选择 以及如何选择正则化参数 诸如此类的事 还有一个 更加强大的算法 广泛的应用于 工业界和学术界 它被称为支持向量机(Support Vector Machine) 与逻辑回归和神经网络相比 支持向量机 或者简称SVM 在学习复杂的非线性方程时 提供了一种更为清晰 更加强大的方式 因此 在接下来的视频中 我会探讨 这一算法 在稍后的课程中 我也会对监督学习算法 进行简要的总结 当然 仅仅是作简要描述 但对于支持向量机 鉴于该算法的强大和受欢迎度 在本课中 我会花许多时间来讲解它 它也是我们所介绍的 最后一个监督学习算法 正如我们之前开发的学习算法 我们从 优化目标开始 那么 我们开始学习 这个算法 为了描述支持向量机 事实上 我将会 从逻辑回归开始 展示我们如何 一点一点修改 来得到本质上的支持向量机 那么 在逻辑回归中 我们已经熟悉了 这里的假设函数形式 和右边的S型激励函数 然而 为了解释 一些数学知识 我将用 z 表示 θ 转置乘以 x 现在 让一起考虑下 我们想要逻辑回归做什么 如果有一个 y=1 的样本 我的意思是 不管是在训练集中 或是在测试集中 又或者在交叉验证集中 总之是 y=1 现在 我们希望 h(x) 趋近1 因为 我们想要 正确地将此样本分类 这就意味着 当 h(x) 趋近于1时 θ 转置乘以 x 应当 远大于0 这里的 大于大于号 >> 意思是 远远大于0 这是因为 由于 z 表示 θ 转置乘以 x 当 z 远大于 0时 即到了 该图的右边 你不难发现 此时逻辑回归的输出将趋近于1 相反地 如果我们 有另一个样本 即 y=0 我们希望假设函数 的输出值 将趋近于0 这对应于 θ 转置乘以 x 或者就是 z 会 远小于0 因为对应的 假设函数的输出值趋近0
### 二值逻辑回归代价函数
![二值逻辑回归代价函数](amWiki/images/001/07-Week7/1-Support Vector Machines/2-二值逻辑回归代价函数.jpg)  
如果你进一步 观察逻辑回归的代价函数 你会发现 每个样本 (x, y) 都会为总代价函数 增加这里的一项 因此 对于总代价函数 通常会有对所有的训练样本求和 并且这里还有一个1/m项 但是 在逻辑回归中 这里的这一项 就是表示一个训练样本 所对应的表达式 现在 如果我将完整定义的 假设函数 代入这里 那么 我们就会得到 每一个训练样本都影响这一项 现在 先忽略1/m这一项 但是这一项 是影响整个总代价函数 中的这一项的 现在 一起来考虑两种情况 一种是y等于1的情况 一种是y等于0的情况 在第一种情况中 假设 y 等于1 此时 在目标函数中 只需有第一项起作用 因为y等于1时 (1-y) 项将等于0 因此 当在y等于 1的样本中时 即在 (x, y) 中 y等于1 我们得到 -log(1/(1+e^z) ) 这样一项 这里同上一张幻灯片一致 我用 z 表示 θ 转置乘以 x 当然 在代价函数中 y 前面有负号 我们只是这样表示 如果y等于1 代价函数中 这一项也等于1 这样做 是为了简化 此处的表达式 如果画出 关于 z 的函数 你会看到 左下角的 这条曲线 我们同样可以看到 当 z 增大时 也就是相当于 θ 转置乘以x 增大时 z 对应的值 会变的非常小 对整个代价函数而言 影响也非常小 这也就解释了 为什么 逻辑回归在观察到 正样本 y=1 时 试图将 θ^T*x 设置的非常大 因为 在代价函数中的 这一项会变的非常小 现在 开始建立支持向量机 我们从这里开始 我们会从这个代价函数开始 也就是 -log(1/(1+e^z)) 一点一点修改 让我取这里的 z=1 点 我先画出将要用的代价函数 新的代价函数将会 水平的从这里到右边 (图外) 然后我再画一条 同逻辑回归 非常相似的直线 但是 在这里 是一条直线 也就是 我用紫红色画的曲线 就是这条紫红色的曲线 那么 到了这里 已经非常接近 逻辑回归中 使用的代价函数了 只是这里是由两条线段组成 即位于右边的水平部分 和位于左边的 直线部分 先别过多的考虑左边直线部分的斜率 这并不是很重要 但是 这里 我们将使用的新的代价函数 是在 y=1 的前提下的 你也许能想到 这应该能做同逻辑回归中类似的事情 但事实上 在之后的的优化问题中 这会变得更坚定 并且为支持向量机 带来计算上的优势 例如 更容易计算股票交易的问题 等等 目前 我们只是讨论了 y=1 的情况 另外 一种情况是当 y=0 时 此时 如果你仔细观察代价函数 只留下了第二项 因为第一项 被消除了 如果当 y=0 时 那么 这一项也就是0了 所以上述表达式 只留下了第二项 因此 这个样本的代价 或是代价函数的贡献 将会由 这一项表示 并且 如果你将 这一项作为 z 的函数 那么 这里就会得到横轴z 现在 你完成了 支持向量机中的部分内容 同样地 再来一次 我们要替代这一条蓝色的线 用相似的方法 如果我们用 一个新的代价函数来代替 即这条从0点开始的水平直线 然后是一条斜线 像这样 那么 现在让我给 这两个方程命名 左边的函数 我称之为 cost1(z) 同时 在右边函数 我称它为 cost0(z) 这里的下标是指 在代价函数中 对应的 y=1 和 y=0 的情况
### SVM支持向量机优化目标【代价函数】
![SVM支持向量机优化目标【代价函数】](amWiki/images/001/07-Week7/1-Support Vector Machines/3-SVM支持向量机优化目标【代价函数】.jpg)   
拥有了这些定义后 现在 我们就开始构建支持向量机 这是我们在逻辑回归中使用 代价函数 J(θ) 也许这个方程 看起来不是非常熟悉 这是因为 之前 有个负号在方程外面 但是 这里我所做的是 将负号移到了 表达式的里面 这样做使得方程 看起来有些不同 对于支持向量机而言 实质上 我们要将这一 替换为 cost1(z) 也就是cost1(θ^T*x) 同样地 我也将 这一项替换为cost0(z) 也就是代价 cost0(θ^T*x) 这里的代价函数 cost1 就是之前所提到的那条线 看起来是这样的 此外 代价函数 cost0 也是上面所介绍过的那条线 看起来是这样 因此 对于支持向量机 我们得到了这里的最小化问题 即 1/m 乘以 从1加到第 m 个 训练样本 y(i) 再乘以 cost1(θ^T*x(i)) 加上1减去 y(i) 乘以 cost0(θ^T*x(i)) 然后 再加上正则化参数 像这样 现在 按照支持向量机的惯例 事实上 我们的书写 会稍微有些不同 代价函数的参数表示也会稍微有些不同 首先 我们要 除去 1/m 这一项 当然 这仅仅是 仅仅是由于 人们使用支持向量机时 对比于逻辑回归而言 不同的习惯所致 但这里我所说的意思是 你知道 我将要做的是 仅仅除去 1/m 这一项 但是 这也会得出 同样的θ最优值 好的 因为 1/m 仅是个常量 因此 你知道 在这个最小化问题中 无论前面是否有 1/m 这一项 最终我所得到的 最优值θ都是一样的 这里我的意思是 先给你举一个实例 假定有一最小化问题 即要求当 (u-5)^2+1 取得最小值时的 u 值 好的 这时最小值为 当 u=5 时取得最小值 现在 如果我们想要 将这个目标函数 乘上常数10 这里我的最小化问题就变成了 求使得 10×(u-5)^2+10 最小的值u 然而 这里的u值 使得这里最小的u值仍为5 因此 将一些常数 乘以你的最小化项 例如 这里的常数10 这并不会改变 最小化该方程时得到u值 因此 这里我所做的 是删去常量m 也是相同的 现在 我将目标函数 乘上一个常量 m 并不会改变 取得最小值时的 θ 值 第二点概念上的变化 我们只是指在使用 支持向量机时 一些如下的标准惯例 而不是逻辑回归 因此 对于逻辑回归 在目标函数中 我们有两项 第一个是这一项 是来自于 训练样本的代价 第二个是这一项 是我们的正则化项 我们不得不去 用这一项来平衡 这就相当于 我们想要最小化 A 加上 正则化参数 λ 然后乘以 其他项 B 对吧？ 这里的 A 表示 这里的第一项 同时 我用 B 表示 第二项 但不包括 λ 我们不是 优化这里的 A+λ×B 我们所做的 是通过设置 不同正则参数 λ 达到优化目的 这样 我们就能够权衡 对应的项 是使得训练样本拟合的更好 即最小化 A 还是保证正则参数足够小 也即是 对于B项而言 但对于支持向量机 按照惯例 我们将使用一个不同的参数 为了替换这里使用的 λ 来权衡这两项 你知道 就是第一项和第二项 我们 依照惯例使用 一个不同的参数 称为C 同时改为优化目标 C×A+B 因此 在逻辑回归中 如果给定 λ 一个非常大的值 意味着给予B更大的权重 而这里 就对应于将C 设定为非常小的值 那么 相应的将会给 B 比给 A 更大的权重 因此 这只是 一种不同的方式来控制这种权衡 或者一种不同的方法 即用参数来决定 是更关心第一项的优化 还是更关心第二项的优化 当然你也可以 把这里的参数C 考虑成 1/λ 同 1/λ 所扮演的 角色相同 并且这两个方程 或这两个表达式并不相同 因为 C 等于 1/λ 但是也并不全是这样 如果当C等于 1/λ 时 这两个 优化目标应当 得到相同的值 相同的最优值θ 因此  就用它们来代替 那么 我现在删掉这里的 λ 并且用常数 C 来代替这里 因此 这就得到了 在支持向量机中 我们的整个优化目标函数 然后最小化 这个目标函数 得到 SVM 学习到的 参数C
### SVM支持向量机假设函数
![SVM支持向量机假设函数](amWiki/images/001/07-Week7/1-Support Vector Machines/4-SVM支持向量机假设函数.jpg)   
最后 有别于逻辑回归 有别于逻辑回归 输出的概率 在这里 我们的代价函数 当最小化代价函数 获得参数θ时 支持向量机所做的是 它来直接预测 y的值等于1 还是等于0 因此 这个假设函数 会预测1 当 θ^T*x 大于 或者等于0时 或者等于0时 所以学习 参数 θ 就是支持向量机假设函数的形式 那么 这就是 支持向量机 数学上的定义 再接下来的视频中 让我们再回去 从直观的角度看看优化目标 实际上是在做什么 以及 SVM 的假设函数 将会学习什么 同时 也会谈谈 如何 做些许修改 学习更加复杂、非线性的函数
### 内置习题
![内置习题_区别逻辑回归和SVM代价函数形式](amWiki/images/001/07-Week7/1-Support Vector Machines/5-内置习题_区别逻辑回归和SVM代价函数形式.jpg)  
## English
### Review the logistic regression hypothesis function
![Review the logistic regression hypothesis function](amWiki/images/001/07-Week7/1-Support Vector Machines/1-回顾逻辑回归假设函数.jpg)
By now, you've seen a range of difference learning algorithms. With supervised learning, the performance of many supervised learning algorithms will be pretty similar, and what matters less often will be whether you use learning algorithm a or learning algorithm b, but what matters more will often be things like the amount of data you create these algorithms on, as well as your skill in applying these algorithms. Things like your choice of the features you design to give to the learning algorithms, and how you choose the colorization parameter, and things like that. But, there's one more algorithm that is very powerful and is very widely used both within industry and academia, and that's called the support vector machine. And compared to both logistic regression and neural networks, the Support Vector Machine, or SVM sometimes gives a cleaner, and sometimes more powerful way of learning complex non-linear functions. And so let's take the next videos to talk about that. Later in this course, I will do a quick survey of a range of different supervisory algorithms just as a very briefly describe them. But the support vector machine, given its popularity and how powerful it is, this will be the last of the supervisory algorithms that I'll spend a significant amount of time on in this course as with our development other learning algorithms, we're gonna start by talking about the optimization objective. So, let's get started on this algorithm.In order to describe the support vector machine, I'm actually going to start with logistic regression, and show how we can modify it a bit, and get what is essentially the support vector machine. So in logistic regression, we have our familiar form of the hypothesis there and the sigmoid activation function shown on the right.And in order to explain some of the math, I'm going to use z to denote theta transpose axiom.Now let's think about what we would like logistic regression to do. If we have an example with y equals one and by this I mean an example in either the training set or the test set or the cross-validation set, but when y is equal to one then we're sort of hoping that h of x will be close to one. Right, we're hoping to correctly classify that example. And what having x subscript 1, what that means is that theta transpose x must be must larger than 0. So there's greater than, greater than sign that means much, much greater than 0. And that's because it is z, the theta of transpose x is when z is much bigger than 0 is far to the right of the sphere. That the outputs of logistic progression becomes close to one.Conversely, if we have an example where y is equal to zero, then what we're hoping for is that the hypothesis will output a value close to zero. And that corresponds to theta transpose x of z being much less than zero because that corresponds to a hypothesis of putting a value close to zero.
### Binary logistic regression cost function
![Binary logistic regression cost function](amWiki/images/001/07-Week7/1-Support Vector Machines/2-二值逻辑回归代价函数.jpg)  
If you look at the cost function of logistic regression, what you'll find is that each example (x,y) contributes a term like this to the overall cost function, right?So for the overall cost function, we will also have a sum over all the chain examples and the 1 over m term, that this expression here, that's the term that a single training example contributes to the overall objective function so we can just rush them.
Now if I take the definition for the fall of my hypothesis and plug it in over here, then what I get is that each training example contributes this term, ignoring the one over M but it contributes that term to my overall cost function for logistic regression.Now let's consider two cases of when y is equal to one and when y is equal to zero. In the first case, let's suppose that y is equal to 1. In that case, only this first term in the objective matters, because this one minus y term would be equal to zero if y is equal to one.So when y is equal to one, when in our example x comma y, when y is equal to 1 what we get is this term.. Minus log one over one, plus E to the negative Z where as similar to the last line I'm using Z to denote data transposed X and of course in a cost I should have this minus line that we just had if Y is equal to one so that's equal to one I just simplify in a way in the expression that I have written down here.And if we plot this function as a function of z, what you find is that you get this curve shown on the lower left of the slide. And thus, we also see that when z is equal to large, that is, when theta transpose x is large, that corresponds to a value of z that gives us a fairly small value, a very, very small contribution to the consumption. And this kinda explains why, when logistic regression sees a positive example, with y=1, it tries to set theta transport x to be very large because that corresponds to this term, in the cross function, being small. Now, to fill the support vec machine, here's what we're going to do. We're gonna take this cross function, this minus log 1 over 1 plus e to negative z, and modify it a little bit. Let me take this point 1 over here, and let me draw the cross functions you're going to use. The new pass functions can be flat from here on out, and then we draw something that grows as a straight line, similar to logistic regression. But this is going to be a straight line at this portion. So the curve that I just drew in magenta, and the curve I just drew purple and magenta, so if it's pretty close approximation to the cross function used by logistic regression. Except it is now made up of two line segments, there's this flat portion on the right, and then there's this straight line portion on the left. And don't worry too much about the slope of the straight line portion. It doesn't matter that much. But that's the new cost function we're going to use for when y is equal to one, and you can imagine it should do something pretty similar to logistic regression. But turns out, that this will give the support vector machine computational advantages and give us, later on, an easier optimization problem that would be easier for software to solve. We just talked about the case of y equals one. The other case is if y is equal to zero. In that case, if you look at the cost, then only the second term will apply because the first term goes away, right? If y is equal to zero, then you have a zero here, so you're left only with the second term of the expression above. And so the cost of an example, or the contribution of the cost function, is going to be given by this term over here. And if you plot that as a function of z, to have pure z on the horizontal axis, you end up with this one. And for the support vector machine, once again, we're going to replace this blue line with something similar and at the same time we replace it with a new cost, this flat out here, this 0 out here. And that then grows as a straight line, like so. So let me give these two functions names. This function on the left I'm going to call cost subscript 1 of z, and this function of the right I'm gonna call cost subscript 0 of z. And the subscript just refers to the cost corresponding to when y is equal to 1, versus when y Is equal to zero.
### Support vector machine SVM optimization objective 【cost function】
![Support vector machine SVM optimization objective 【cost function】](amWiki/images/001/07-Week7/1-Support Vector Machines/3-SVM支持向量机优化目标【代价函数】.jpg)   
Armed with these definitions, we're now ready to build a support vector machine. Here's the cost function, j of theta, that we have for logistic regression. In case this equation looks a bit unfamiliar, it's because previously we had a minus sign outside, but here what I did was I instead moved the minus signs inside these expressions, so it just makes it look a little different.For the support vector machine what we're going to do is essentially take this and replace this with cost1 of z, that is cost1 of theta transpose x. And we're going to take this and replace it with cost0 of z, that is cost0 of theta transpose x. Where the cost one function is what we had on the previous slide that looks like this. And the cost zero function, again what we had on the previous slide, and it looks like this. So what we have for the support vector machine is a minimization problem of one over M, the sum of Y I times cost one, theta transpose X I, plus one minus Y I times cause zero of theta transpose X I, and then plus my usual regularization parameter.Like so. Now, by convention, for the support of vector machine, we're actually write things slightly different. We re-parameterize this just very slightly differently.First, we're going to get rid of the 1 over m terms, and this just this happens to be a slightly different convention that people use for support vector machines compared to or just a progression. But here's what I mean. You're one way to do this, we're just gonna get rid of these one over m terms and this should give you me the same optimal value of beta right? Because one over m is just as constant so whether I solved this minimization problem with one over n in front or not. I should end up with the same optimal value for theta. Here's what I mean, to give you an example, suppose I had a minimization problem. Minimize over a long number U of U minus five squared plus one. Well, the minimum of this happens to be U equals five.
Now if I were to take this objective function and multiply it by 10. So here my minimization problem is min over U, 10 U minus five squared plus 10. Well the value of U that minimizes this is still U equals five right? So multiply something that you're minimizing over, by some constant, 10 in this case, it does not change the value of U that gives us, that minimizes this function. So the same way, what I've done is by crossing out the M is all I'm doing is multiplying my objective function by some constant M and it doesn't change the value of theta. That achieves the minimum. The second bit of notational change, which is just, again, the more standard convention when using SVMs instead of logistic regression, is the following. So for logistic regression, we add two terms to the objective function. The first is this term, which is the cost that comes from the training set and the second is this row, which is the regularization term.And what we had was we had a, we control the trade-off between these by saying, what we want is A plus, and then my regularization parameter lambda. And then times some other term B, where I guess I'm using your A to denote this first term, and I'm using B to denote the second term, maybe without the lambda.And instead of prioritizing this as A plus lambda B, and so what we did was by setting different values for this regularization parameter lambda, we could trade off the relative weight between how much we wanted the training set well, that is, minimizing A, versus how much we care about keeping the values of the parameter small, so that will be, the parameter is B for the support vector machine, just by convention, we're going to use a different parameter. So instead of using lambda here to control the relative waiting between the first and second terms. We're instead going to use a different parameter which by convention is called C and is set to minimize C times a + B. So for logistic regression, if we set a very large value of lambda, that means you will give B a very high weight. Here is that if we set C to be a very small value, then that responds to giving B a much larger rate than C, than A. So this is just a different way of controlling the trade off, it's just a different way of prioritizing how much we care about optimizing the first term, versus how much we care about optimizing the second term. And if you want you can think of this as the parameter C playing a role similar to 1 over lambda. And it's not that it's two equations or these two expressions will be equal. This equals 1 over lambda, that's not the case. It's rather that if C is equal to 1 over lambda, then these two optimization objectives should give you the same value the same optimal value for theta so we just filling that in I'm gonna cross out lambda here and write in the constant C there.So that gives us our overall optimization objective function for the support vector machine. And if you minimize that function, then what you have is the parameters learned by the SVM.
### Support vector machine SVM hypothesis function
![Support vector machine SVM hypothesis function](amWiki/images/001/07-Week7/1-Support Vector Machines/4-SVM支持向量机假设函数.jpg)  
Finally unlike logistic regression, the support vector machine doesn't output the probability is that what we have is we have this cost function, that we minimize to get the parameter's data, and what a support vector machine does is it just makes a prediction of y being equal to one or zero, directly. So the hypothesis will predict one if theta transpose x is greater or equal to zero, and it will predict zero otherwise and so having learned the parameters theta, this is the form of the hypothesis for the support vector machine. So that was a mathematical definition of what a support vector machine does. In the next few videos, let's try to get back to intuition about what this optimization objective leads to and whether the source of the hypotheses SVM will learn and we'll also talk about how to modify this just a little bit to the complex nonlinear functions.
### Exam_in the video
![Exam_The difference between logistic regression and SVM cost function form](amWiki/images/001/07-Week7/1-Support Vector Machines/5-内置习题_区别逻辑回归和SVM代价函数形式.jpg)    
