# 神经网络学习-代价函数
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/05-Week5/1-Neural Networks Learning/1-Cost Function.mp4" type="video/mp4">
</video>
## 中文
### 人工神经网络解决分类问题【二元分类和多类别分类】
![人工神经网络解决分类问题【二元分类和多类别分类】](amWiki/images/001/05-Week5/1-Neural Networks Learning/1-人工神经网络解决分类问题【二元分类和多类别分类】.jpg)  
神经网络是当今 最强大的学习算法之一 在本节课视频 和后面几次课程中 我将开始讲述一种 在给定训练集下 为神经网络拟合参数的学习算法 正如我们讨论大多数 学习算法一样 我们准备从拟合神经网络参数的 代价函数开始讲起 我准备重点讲解 神经网络在分类问题 中的应用 假设我们有一个如左边所示的 神经网络结构 然后假设我们有一个 像这样的训练集 m个训练样本x(i) y(i) 我用大写字母 L 来表示 这个神经网络结构的总层数 所以 对于左边的网络结构 我们得到 L等于4 然后我准备用 sl表示 第L层的单元的数量 也就是神经元的数量 这其中不包括L层的偏差单元 比如说 我们得到s1 也就是输入层 是等于3的单元 s2在这个例子里等于5个单位 然后输出层s4 也就是sl 因为L本身等于4 在左边这个例子中输出层 有4个单位 我们将会讨论两种分类问题 第一种是二元分类 在这里y只能等于0或1 在这个例子中 我们有一个输出单元 上面这个神经网络的有四个输出单元 但是如果我们 用二元分类的话 我们就只能有一个输出结果 也就是计算出来的h(x) 神经网络的输出结果 h(x)就会是 一个实数 在这类问题里 输出单元的个数 sl L同样代表最后一层的序号 因为这就是我们 在这个网络结构中的层数 所以我们在输出层的单元数目 就将是1 在这类问题里 为了简化记法 我会把K设为1 这样你可以把K看作 输出层的 单元数目 我们要考虑的第二类分类问题 就是多类别的分类问题 也就是会有K个不同的类 比如说 如果我们有四类的话 我们就用这样的表达形式来代表y 在这类问题里 我们就会有K个输出单元 我们的假设输出 就是一个K维向量 输出单元的个数 就等于K 通常这类问题里 我们都有K大于 或等于3 因为如果只有两个类别 我们就不需要 使用这种一对多的方法 我们只有在K大于 或者等于3个类的时候 才会使用这种 一对多的方法 因为如果只有两个类别 我们就只需要一个输出单元就可以了
### 定义人工神经网络代价函数
![定义人工神经网络代价函数](amWiki/images/001/05-Week5/1-Neural Networks Learning/2-定义人工神经网络代价函数.jpg)  
现在我们来为神经网络定义代价函数 我们在神经网络里 使用的代价函数 应该是逻辑回归里 使用的代价函数的一般化形式 对于逻辑回归而言 我们通常使代价函数 J(θ) 最小化 也就是-1/m 乘以后面这个代价函数 然后再加上这个额外正则化项 这里是一个 j从1到n的求和形式 因为我们 并没有把偏差项 0正则化 对于一个神经网络来说 我们的代价函数是这个式子的一般化形式 这里不再是仅有一个 逻辑回归输出单元 取而代之的是K个 所以这是我们的代价函数 神经网络现在输出了 在K维的向量 这里K可以取到1 也就是 原来的二元分类问题 我准备用这样一个记法 h(x)带下标i 来表示第i个输出 也就是h(x)是一个K维向量 下标 i 表示 选择了神经网络输出向量的 第i个元素 我的代价函数 J(θ) 将成为下面这样的形式 -1/m乘以 一个类似于我们在 逻辑回归里所用的 求和项 除了这里我们求的是 k从1到K的所有和 这个求和项主要是 K个输出单元的求和 所以如果我有四个输出单元 也就是我的神经网络最后一层 有四个输出单元 那么这个求和就是 这个求和项就是 求k等于从1到4的 每一个的逻辑回归算法的代价函数 然后按四次输出的顺序 依次把这些代价函数 加起来 所以你会特别注意到 这个求和符号应用于 yk和hk 因为 我们主要是讨论 K个输出单元 并且把它和yk的值相比 yk的值就是 这些向量里表示 它应当属于哪个类别的量 最后 这里的第二项 这就是类似于我们在逻辑回归里所用的 正则化项 这个求和项看起来 确实非常复杂 它所做的就是把这些项全部相加 也就是对所有i j和l 的θji的值都相加 正如我们在逻辑回归里一样 这里要除去那些对应于偏差值的项 那些项我们是不加进去的 那些项我们是不加进去的 具体地说 我们不把 那些对于i等于0的项 加入其中 这是因为 当我们计算神经元的激励值时 我们会有这些项 θi0 加上θi1 乘以x1 再加上 等等等等 这里我认为 我们可以加上2的上标 如果这是第一个隐含层的话 所以这些带0的项 所以这些带0的项 对应于乘进去了 x0 或者是a0什么的 这就是一个类似于 偏差单元的项 类比于我们在做 逻辑回归的时候 我们就不应该把这些项 加入到正规化项里去 因为我们并不想正规化这些项 并把这些项设定为0 但这只是一个合理的规定 即使我们真的把他们加进去了 也就是i从0加到sL 这依然成立 并且不会有大的差异 但是这个"不把偏差项正规化" 的规定可能只是会 更常见一些 好了 这就是我们准备 应用于神经网络的代价函数 在下一个视频中 我会开始讲解一个算法 来最优化这个代价函数
### 内置习题
![内置习题_神经网络代价函数](amWiki/images/001/05-Week5/1-Neural Networks Learning/3-内置习题_神经网络代价函数.jpg)
## English
### Classification of Artificial Neurons Networks【Binary Classification and Multi-class Classification】
![Classification of Artificial Neurons Networks【Binary Classification and Multi-class Classification】](amWiki/images/001/05-Week5/1-Neural Networks Learning/1-人工神经网络解决分类问题【二元分类和多类别分类】.jpg)   
Neural networks are one of the most powerful learning algorithms that we have today. In this and in the next few videos, I'd like to start talking about a learning algorithm for fitting the parameters of a neural network given a training set. As with the discussion of most of our learning algorithms, we're going to begin by talking about the cost function for fitting the parameters of the network.I'm going to focus on the application of neural networks to classification problems. So suppose we have a network like that shown on the left. And suppose we have a training set like this is x I, y I pairs of M training example.I'm going to use upper case L to denote the total number of layers in this network. So for the network shown on the left we would have capital L equals 4. I'm going to use S subscript L to denote the number of units, that is the number of neurons. Not counting the bias unit in their L of the network. So for example, we would have a S one, which is equal there, equals S three unit, S two in my example is five units. And the output layer S four, which is also equal to S L because capital L is equal to four. The output layer in my example under that has four units.We're going to consider two types of classification problems. The first is Binary classification, where the labels y are either 0 or 1. In this case, we will have 1 output unit, so this Neural Network unit on top has 4 output units, but if we had binary classification we would have only one output unit that computes h(x). And the output of the neural network would be h(x) is going to be a real number. And in this case the number of output units, S L, where L is again the index of the final layer. Cuz that's the number of layers we have in the network so the number of units we have in the output layer is going to be equal to 1. In this case to simplify notation later, I'm also going to set K=1 so you can think of K as also denoting the number of units in the output layer. The second type of classification problem we'll consider will be multi-class classification problem where we may have K distinct classes. So our early example had this representation for y if we have 4 classes, and in this case we will have capital K output units and our hypothesis or output vectors that are K dimensional. And the number of output units will be equal to K. And usually we would have K greater than or equal to 3 in this case, because if we had two causes, then we don't need to use the one verses all method. We use the one verses all method only if we have K greater than or equals V classes, so having only two classes we will need to use only one upper unit.
### Define the cost function for Artificial Neurons Networks
![Define the cost function for Artificial Neurons Networks](amWiki/images/001/05-Week5/1-Neural Networks Learning/2-定义人工神经网络代价函数.jpg)  
Now let's define the cost function for our neural network.The cost function we use for the neural network is going to be a generalization of the one that we use for logistic regression. For logistic regression we used to minimize the cost function J(theta) that was minus 1/m of this cost function and then plus this extra regularization term here, where this was a sum from J=1 through n, because we did not regularize the bias term theta0.For a neural network, our cost function is going to be a generalization of this. Where instead of having basically just one, which is the compression output unit, we may instead have K of them. So here's our cost function. Our new network now outputs vectors in R K where R might be equal to 1 if we have a binary classification problem. I'm going to use this notation h(x) subscript i to denote the ith output. That is, h(x) is a k-dimensional vector and so this subscript i just selects out the ith element of the vector that is output by my neural network.My cost function J(theta) is now going to be the following. Is - 1 over M of a sum of a similar term to what we have for logistic regression, except that we have the sum from K equals 1 through K. This summation is basically a sum over my K output. A unit. So if I have four output units, that is if the final layer of my neural network has four output units, then this is a sum from k equals one through four of basically the logistic regression algorithm's cost function but summing that cost function over each of my four output units in turn. And so you notice in particular that this applies to Yk Hk, because we're basically taking the K upper units, and comparing that to the value of Yk which is that one of those vectors saying what cost it should be. And finally, the second term here is the regularization term, similar to what we had for the logistic regression. This summation term looks really complicated, but all it's doing is it's summing over these terms theta j i l for all values of i j and l. Except that we don't sum over the terms corresponding to these bias values like we have for logistic progression. Completely, we don't sum over the terms responding to where i is equal to 0. So that is because when we're computing the activation of a neuron, we have terms like these. Theta i 0. Plus theta i1, x1 plus and so on. Where I guess put in a two there, this is the first hit in there. And so the values with a zero there, that corresponds to something that multiplies into an x0 or an a0. And so this is kinda like a bias unit and by analogy to what we were doing for logistic progression, we won't sum over those terms in our regularization term because we don't want to regularize them and string their values as zero. But this is just one possible convention, and even if you were to sum over i equals 0 up to Sl, it would work about the same and doesn't make a big difference. But maybe this convention of not regularizing the bias term is just slightly more common.So that's the cost function we're going to use for our neural network. In the next video we'll start to talk about an algorithm for trying to optimize the cost function.
### Exam_in the video
![Exam_Artificial Neurons Networks of Cost Function](amWiki/images/001/05-Week5/1-Neural Networks Learning/3-内置习题_神经网络代价函数.jpg)
