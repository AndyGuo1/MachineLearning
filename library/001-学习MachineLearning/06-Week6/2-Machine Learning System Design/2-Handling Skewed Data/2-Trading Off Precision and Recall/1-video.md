# 机器学习系统设计-权衡查准率和召回率
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/06-Week6/2-Machine Learning System Design/4-Trading Off Precision and Recall.mp4" type="video/mp4">
</video>
## 中文
### 权衡查准率和召回率
![权衡查准率和召回率](amWiki/images/001/06-Week6/2-Machine Learning System Design\15-权衡查准率和召回率.jpg)  
在之前的课程中 我们谈到 查准率和召回率 作为遇到偏斜类问题 的评估度量值 在很多应用中 我们希望能够保证 查准率和召回率的相对平衡 在这节课中 我将告诉你应该怎么做 同时也向你展示一些 查准率和召回率 作为算法评估度量值的 更有效的方式 回忆一下 这是查准率和召回率的定义 我们在上一节中讲到的 让我们继续用癌症分类的例子 如果病人患癌症 则y=1 反之则y=0 假设我们用 逻辑回归模型训练了数据 输出概率在0-1之间的值 因此 我们预测y=1 如果h(x) 大于或等于0.5 预测值为0 如果方程输出值 小于0.5 这个回归模型 能够计算查准率和召回率 但是现在 假如我们希望 在我们非常确信地情况下 才预测一个病人得了癌症 因为你知道 如果你告诉一个病人 告诉他们你得了癌症 他们会非常震惊 因为这是一个 非常坏的消息 而且他们会经历一段 非常痛苦的治疗过程 因此我们希望 只有在我们非常确信的情况下 才告诉这个人他得了癌症这样做的一种方法 是修改算法 我们不再将临界值 设为0.5 也许 我们只在 h(x)的值大于或等于0.7 的情况下 才预测y=1 因此 我们会告诉一个人 他得了癌症 在我们认为 他有大于等于70%得癌症的概率情况下 如果你这么做 那么你只在 非常确信地情况下 才预测癌症 那么你的回归模型会有较高的查准率 因为所有你准备 告诉他们 患有癌症的病人 所有这些人 有比较高的可能性 他们真的患有癌症 你预测患有癌症的病人中 有较大比率的人 他们确实患有癌症 因为这是我们 在非常确信的情况下做出的预测 与之相反 这个回归模型会有较低的召回率 因为 当我们做预测的时候 我们只给很小一部分的病人预测y=1 现在我们把这个情况夸大一下 我们不再把临界值 设在0.7 我们把它设为0.9 我们只在至少90%肯定 这个病人患有癌症的情况下 预测y=1 那么这些病人当中 有非常大的比率 真正患有癌症 因此这是一个高查准率的模型 但是召回率会变低 因为我们希望能够正确检测患有癌症的病人 现在考虑一个不同的例子 假设我们希望 避免遗漏掉患有癌症的人 即我们希望避免假阴性 具体地说 如果一个病人实际患有癌症 但是我们并没有告诉他患有癌症 那这可能造成严重后果 因为 如果我们告诉病人他们没有患癌症 那么 他们就不会接受治疗 但是如果 他们患有癌症 我们又没有告诉他们 那么他们就根本不会接受治疗 那么 这么可能造成严重后果 病人丧失生命 因为我们没有告诉他患有癌症 他没有接受治疗 但事实上他又患有癌症 这种i情况下 我们希望预测y=1 我们希望 预测病人患有癌症 这样 他们会做进一步的检测 然后接受治疗 以避免他们真的患有癌症 在这个例子中 我们不再设置高的临界值 我们会设置另一个值 将临界值 设得较低 比如0.3 这样做 我们认为 他们有大于30%的几率 患有癌症 我们以更加保守的方式 告诉他们患有癌症 因此他们能够接受治疗 在这种情况下 我们会有一个 较高召回率的模型 因为 确实患有癌症的病人 有很大一部分 被我们正确标记出来了 但是 我们会得到较低的查准率 因为 我们预测患有癌症的病人比例越大 那么就有较大比例的人其实没有患癌症 顺带一提 当我在给 别的学生讲这个的时候 令人惊讶的是 有的学生问 怎么可以从两面来看这个问题 为什么我总是 只想要高查准率或高召回率 但是这看起来可以使两边都提高 但是我希望 算法是正确的 更普遍的一个原则是 这取决于你想要什么 你想要高查准率 低召回率 还是高召回率 低查准率 你可以预测y=1 当h(x)大于某个临界值 因此 总的来说 对于大多数的回归模型 你得权衡查准率和召回率 当你改变 临界值的值时 我在这儿画了一个 临界值 你可以画出曲线 来权衡查准率 和召回率 这里的一个值 反应出一个较高的临界值 这个临界值可能等于0.99 我们假设 只在有大于99%的确信度的情况下 才预测y=1 至少 有99%的可能性 因此这个点反应高查准率 低召回率 然而这里的一个点 反映一个较低的临界值 比如说0.01 毫无疑问 在这里预测y=1 如果你这么做 你最后会得到 很低的查准率 但是较高的召回率 当你改变临界值 如果你愿意 你可以画出回归模型的所有曲线 来看看你能得到的查准率和召回率的范围 顺带一提 查准率-召回率曲线可以是各种不同的形状 有时它看起来是这样 有时是那样 查准率-召回率曲线的形状 有很多可能性 这取决于回归模型的具体算法 因此这又产生了 另一个有趣的问题 那就是 有没有办法自动选取临界值 或者 更广泛地说 如果我们有不同的算法 或者不同的想法 我们如何比较不同的查准率和召回率呢？具体来说 假设我们有三个 不同的学习算法 或者这三个不同的学习曲线 是同样的算法 但是临界值不同 我们怎样决定哪一个算法是最好的 我们之前讲到的 其中一件事就是 评估度量值的重要性 这个概念是 通过一个具体的数字 来反映你的回归模型到底如何 但是查准率和召回率的问题 我们却不能这样做 因为在这里我们有两个可以判断的数字 因此 我们经常会 不得不面对这样的情况 如果我们正在试图比较算法1 和算法2 我们最后问自己 到底是0.5的查准率与 0.4的召回率好 还是说 0.7的查准率与 0.1的召回率好 或者每一次 你设计一个新算法 你都要坐下来思考 到底0.5 0.4好 还是说 0.7 0.1好 我不知道 如果你最后这样坐下来思考 这回降低 你的决策速度 思考到底哪些改变是有用的 应该被融入到你的算法
### 查准率和召回率平衡公式F
![查准率和召回率平衡公式F](amWiki/images/001/06-Week6/2-Machine Learning System Design\16-查准率和召回率平衡公式F.jpg)  
与此相反的是 如果我们有一个评估度量值 一个数字 能够告诉我们到底是算法1好还是算法2好 这能够帮助我们 更快地决定 哪一个算法更好 同时也能够更快地帮助我们 评估不同的改动 哪些应该被融入进算法里面 那么 我们怎样才能 得到这个评估度量值呢？你可能会去尝试的 一件事情是 计算一下查准率和召回率的平均值 用 P 和 R 来表示查准率和召回率 你可以做的是 计算它们的平均值 看一看哪个模型有最高的均值 但是这可能 并不是一个很好的解决办法 因为 像我们之前的例子一样 如果我们的回归模型 总是预测 y=1 这么做你可能得到非常高的召回率 得到非常低的查准率 相反地 如果你的模型 总是预测y=0 就是说 如果很少预测y=1 对应的 设置了一个高临界值 最后 你会得到非常高的 查准率和非常低的召回率 这两个极端情况 一个有非常高的临界值 一个有非常低的临界值 它们中的任何一个都不是一个好的模型 我们可以通过 非常低的查准率 或者非常低的召回率 判断这不是一个好模型 如果你只是使用(P+R)/2 算法3的这个值 是最高的 即使你可以通过 使用总是预测y=1这样的方法 来得到这样的值 但这并不是一个好的模型 对吧 你总是预测y=1 这不是一个有用的模型 因为它只输出y=1 那么算法1和 算法2 比算法3更有用 但是在这个例子中 查准率和召回率的平均值 算法3是最高的 因此我们通常认为 查准率和召回率的平均值 不是评估算法的一个好的方法 相反地 有一种结合查准率和召回率的不同方式 叫做F值 公式是这样 在这个例子中 F值是这样的 我们可以通过 F值来判断 算法1 有最高的F值 算法2第二 算法3是最低的 因此 通过F值 我们会在这几个算法中选择算法1 F值 也叫做F1值 一般写作F1值 但是人们一般只说F值它的定义 会考虑一部分 查准率和召回率的平均值 但是它 会给查准率和召回率中较低的值 更高的权重 因此 你可以看到F值的分子 是查准率和召回率的乘积 因此如果查准率等于0 或者召回率等于0 F值也会 等于0 因此它结合了查准率和召回率 对于一个较大的F值 查准率 和召回率都必须较大 我必须说 有较多的公式 可以结合查准率和召回率 F值公式 只是 其中一个 但是出于历史原因 和习惯问题 人们在机器学习中使用F值 这个术语F值 没有什么特别的意义 所以不要担心 它到底为什么叫做F值或者F1值 但是它给了你 你需要的有效方法 因为无论是查准率等于0 还是召回率等于0 它都会得到一个很低的F值 因此 如果要得到一个很高的F值 你的算法的查准率和召回率都要接近于1 具体地说 如果P=0或者 R=0 你的F值也会等于0 对于一个最完美的F值 如果查准率等于1 同时召回率 也等于1 那你得到的F值 等于1乘以1 除以2再乘以2 那么F值 就等于1 如果你能得到最完美的查准率和召回率 在0和1中间的值 这经常是 回归模型最经常出现的分数 在这次的视频中 我们讲到了如何 权衡查准率和召回率 以及我们如何变动 临界值 来决定我们希望预测y=1 还是y=0 比如我们 需要一个 70%还是90%置信度的临界值 或者别的 来预测y=1 通过变动临界值 你可以控制权衡 查准率和召回率 之后我们讲到了F值 它权衡查准率和召回率 给了你一个 评估度量值 当然 如果你的目标是 自动选择临界值 来决定 你希望预测y=1 还是y=0 那么一个比较理想的办法是 试一试不同的 临界值 试一下 不同的临界值 然后评估这些不同的临界值 在交叉检验集上进行测试 然后选择哪一个临界值 能够在交叉检验集上 得到最高的F值 这是自动选择临界值的较好办法 较好办法
### 内置习题
![内置习题_理解查准率和召回率平衡公式](amWiki/images/001/06-Week6/2-Machine Learning System Design/17-内置习题_理解查准率和召回率平衡公式.jpg)  
## English
### Trade off the precision and recall
![Trade off the precision and recall](amWiki/images/001/06-Week6/2-Machine Learning System Design\15-权衡查准率和召回率.jpg)  
In the last video, we talked about precision and recall as an evaluation metric for classification problems with skewed constants. For many applications, we'll want to somehow control the trade-off between precision and recall. Let me tell you how to do that and also show you some even more effective ways to use precision and recall as an evaluation metric for learning algorithms.As a reminder, here are the definitions of precision and recall from the previous video.Let's continue our cancer classification example, where y equals 1 if the patient has cancer and y equals 0 otherwise. And let's say we're trained in logistic regression classifier which outputs probability between 0 and 1. So, as usual, we're going to predict 1, y equals 1, if h(x) is greater or equal to 0.5. And predict 0 if the hypothesis outputs a value less than 0.5. And this classifier may give us some value for precision and some value for recall.But now, suppose we want to predict that the patient has cancer only if we're very confident that they really do. Because if you go to a patient and you tell them that they have cancer, it's going to give them a huge shock. What we give is a seriously bad news, and they may end up going through a pretty painful treatment process and so on. And so maybe we want to tell someone that we think they have cancer only if they are very confident. One way to do this would be to modify the algorithm, so that instead of setting this threshold at 0.5, we might instead say that we will predict that y is equal to 1 only if h(x) is greater or equal to 0.7. So this is like saying, we'll tell someone they have cancer only if we think there's a greater than or equal to, 70% chance that they have cancer.And, if you do this, then you're predicting someone has cancer only when you're more confident and so you end up with a classifier that has higher precision. Because all of the patients that you're going to and saying, we think you have cancer, although those patients are now ones that you're pretty confident actually have cancer. And so a higher fraction of the patients that you predict have cancer will actually turn out to have cancer because making those predictions only if we're pretty confident.But in contrast this classifier will have lower recall because now we're going to make predictions, we're going to predict y = 1 on a smaller number of patients. Now, can even take this further. Instead of setting the threshold at 0.7, we can set this at 0.9. Now we'll predict y=1 only if we are more than 90% certain that the patient has cancer. And so, a large fraction of those patients will turn out to have cancer. And so this would be a higher precision classifier will have lower recall because we want to correctly detect that those patients have cancer. Now consider a different example. Suppose we want to avoid missing too many actual cases of cancer, so we want to avoid false negatives. In particular, if a patient actually has cancer, but we fail to tell them that they have cancer then that can be really bad. Because if we tell a patient that they don't have cancer, then they're not going to go for treatment. And if it turns out that they have cancer, but we fail to tell them they have cancer, well, they may not get treated at all. And so that would be a really bad outcome because they die because we told them that they don't have cancer. They fail to get treated, but it turns out they actually have cancer. So, suppose that, when in doubt, we want to predict that y=1. So, when in doubt, we want to predict that they have cancer so that at least they look further into it, and these can get treated in case they do turn out to have cancer.In this case, rather than setting higher probability threshold, we might instead take this value and instead set it to a lower value. So maybe 0.3 like so, right? And by doing so, we're saying that, you know what, if we think there's more than a 30% chance that they have cancer we better be more conservative and tell them that they may have cancer so that they can seek treatment if necessary.And in this case what we would have is going to be a higher recall classifier, because we're going to be correctly flagging a higher fraction of all of the patients that actually do have cancer. But we're going to end up with lower precision because a higher fraction of the patients that we said have cancer, a high fraction of them will turn out not to have cancer after all.And by the way, just as a sider, when I talk about this to other students, I've been told before, it's pretty amazing, some of my students say, is how I can tell the story both ways. Why we might want to have higher precision or higher recall and the story actually seems to work both ways. But I hope the details of the algorithm is true and the more general principle is depending on where you want, whether you want higher precision- lower recall, or higher recall- lower precision. You can end up predicting y=1 when h(x) is greater than some threshold. And so in general, for most classifiers there is going to be a trade off between precision and recall, and as you vary the value of this threshold that we join here, you can actually plot out some curve that trades off precision and recall. Where a value up here, this would correspond to a very high value of the threshold, maybe threshold equals 0.99. So that's saying, predict y=1 only if we're more than 99% confident, at least 99% probability this one. So that would be a high precision, relatively low recall. Where as the point down here, will correspond to a value of the threshold that's much lower, maybe equal 0.01, meaning, when in doubt at all, predict y=1, and if you do that, you end up with a much lower precision, higher recall classifier. And as you vary the threshold, if you want you can actually trace of a curve for your classifier to see the range of different values you can get for precision recall. And by the way, the precision-recall curve can look like many different shapes. Sometimes it will look like this, sometimes it will look like that. Now there are many different possible shapes for the precision-recall curve, depending on the details of the classifier. So, this raises another interesting question which is, is there a way to choose this threshold automatically? Or more generally, if we have a few different algorithms or a few different ideas for algorithms, how do we compare different precision recall numbers? Concretely, suppose we have three different learning algorithms. So actually, maybe these are three different learning algorithms, maybe these are the same algorithm but just with different values for the threshold. How do we decide which of these algorithms is best? One of the things we talked about earlier is the importance of a single real number evaluation metric. And that is the idea of having a number that just tells you how well is your classifier doing. But by switching to the precision recall metric we've actually lost that. We now have two real numbers. And so we often, we end up face the situations like if we trying to compare Algorithm 1 and Algorithm 2, we end up asking ourselves, is the precision of 0.5 and a recall of 0.4, was that better or worse than a precision of 0.7 and recall of 0.1? And, if every time you try out a new algorithm you end up having to sit around and think, well, maybe 0.5/0.4 is better than 0.7/0.1, or maybe not, I don't know. If you end up having to sit around and think and make these decisions, that really slows down your decision making process for what changes are useful to incorporate into your algorithm.
### The precision and recall balance formula F
![The precision and recall balance formula F](amWiki/images/001/06-Week6/2-Machine Learning System Design\16-查准率和召回率平衡公式F.jpg)  
Whereas in contrast, if we have a single real number evaluation metric like a number that just tells us is algorithm 1 or is algorithm 2 better, then that helps us to much more quickly decide which algorithm to go with. It helps us as well to much more quickly evaluate different changes that we may be contemplating for an algorithm. So how can we get a single real number evaluation metric?One natural thing that you might try is to look at the average precision and recall. So, using P and R to denote precision and recall, what you could do is just compute the average and look at what classifier has the highest average value.But this turns out not to be such a good solution, because similar to the example we had earlier it turns out that if we have a classifier that predicts y=1 all the time, then if you do that you can get a very high recall, but you end up with a very low value of precision. Conversely, if you have a classifier that predicts y equals zero, almost all the time, that is that it predicts y=1 very sparingly, this corresponds to setting a very high threshold using the notation of the previous y. Then you can actually end up with a very high precision with a very low recall. So, the two extremes of either a very high threshold or a very low threshold, neither of that will give a particularly good classifier. And the way we recognize that is by seeing that we end up with a very low precision or a very low recall. And if you just take the average of (P+R)/2 from this example, the average is actually highest for Algorithm 3, even though you can get that sort of performance by predicting y=1 all the time and that's just not a very good classifier, right? You predict y=1 all the time, just normal useful classifier, but all it does is prints out y=1. And so Algorithm 1 or Algorithm 2 would be more useful than Algorithm 3. But in this example, Algorithm 3 has a higher average value of precision recall than Algorithms 1 and 2. So we usually think of this average of precision and recall as not a particularly good way to evaluate our learning algorithm.In contrast, there's a different way for combining precision and recall. This is called the F Score and it uses that formula. And so in this example, here are the F Scores. And so we would tell from these F Scores, it looks like Algorithm 1 has the highest F Score, Algorithm 2 has the second highest, and Algorithm 3 has the lowest. And so, if we go by the F Score we would pick probably Algorithm 1 over the others.The F Score, which is also called the F1 Score, is usually written F1 Score that I have here, but often people will just say F Score, either term is used. Is a little bit like taking the average of precision and recall, but it gives the lower value of precision and recall, whichever it is, it gives it a higher weight. And so, you see in the numerator here that the F Score takes a product of precision and recall. And so if either precision is 0 or recall is equal to 0, the F Score will be equal to 0. So in that sense, it kind of combines precision and recall, but for the F Score to be large, both precision and recall have to be pretty large. I should say that there are many different possible formulas for combing precision and recall. This F Score formula is really maybe a, just one out of a much larger number of possibilities, but historically or traditionally this is what people in Machine Learning seem to use. And the term F Score, it doesn't really mean anything, so don't worry about why it's called F Score or F1 Score.But this usually gives you the effect that you want because if either a precision is zero or recall is zero, this gives you a very low F Score, and so to have a high F Score, you kind of need a precision or recall to be one. And concretely, if P=0 or R=0, then this gives you that the F Score = 0. Whereas a perfect F Score, so if precision equals one and recall equals 1, that will give you an F Score,that's equal to 1 times 1 over 2 times 2, so the F Score will be equal to 1, if you have perfect precision and perfect recall. And intermediate values between 0 and 1, this usually gives a reasonable rank ordering of different classifiers.So in this video, we talked about the notion of trading off between precision and recall, and how we can vary the threshold that we use to decide whether to predict y=1 or y=0. So it's the threshold that says, do we need to be at least 70% confident or 90% confident, or whatever before we predict y=1. And by varying the threshold, you can control a trade off between precision and recall. We also talked about the F Score, which takes precision and recall, and again, gives you a single real number evaluation metric. And of course, if your goal is to automatically set that threshold to decide what's really y=1 and y=0, one pretty reasonable way to do that would also be to try a range of different values of thresholds. So you try a range of values of thresholds and evaluate these different thresholds on, say, your cross-validation set and then to pick whatever value of threshold gives you the highest F Score on your crossvalidation [INAUDIBLE]. And that be a pretty reasonable way to automatically choose the threshold for your classifier as well.
### Exam_in the video
![Exam_Understand the precision and recall](amWiki/images/001/06-Week6/2-Machine Learning System Design/17-内置习题_理解查准率和召回率平衡公式.jpg)  
