# 机器学习系统设计-偏斜类的误差度量
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/06-Week6/2-Machine Learning System Design/3-Error Metrics for Skewed Classes.mp4" type="video/mp4">
</video>
## 中文
### 偏斜类问题举例_癌症预测
![偏斜类问题举例_癌症预测](amWiki/images/001/06-Week6/2-Machine Learning System Design\11-偏斜类问题举例_癌症预测.jpg)  
在前面的课程中 我提到了误差分析 以及设定误差度量值的重要性 那就是 设定某个实数来评估你的学习算法 并衡量它的表现 有了算法的评估和误差度量值 有一件重要的事情要注意 就是使用一个合适的误差度量值 这有时会对于你的学习算法 造成非常微妙的影响 这件重要的事情就是 偏斜类（skewed classes）的问题 让我告诉你这是什么意思 想一想之前的癌症分类问题 我们拥有 内科病人的特征变量 我们希望知道他们是否患有癌症 因此这就像恶性 与良性肿瘤的分类问题 我们之前讲过这个 我们假设 y=1 表示患者患有癌症 假设 y=0 表示他们没有得癌症 我们训练逻辑回归模型 假设我们用测试集 检验了这个分类模型 并且发现它只有1%的错误 因此我们99%会做出正确诊断 看起来是非常不错的结果 我们99%的情况都是正确的 但是 假如我们发现 在测试集中 只有0.5%的患者 真正得了癌症 因此 在我们的筛选程序里 只有0.5%的患者患了癌症 因此在这个例子中 1%的错误率就不再显得那么好了 举个具体的例子 这里有一行代码 不是机器学习代码 它忽略了输入值X 它让y总是等于0 因此它总是预测 没有人得癌症 那么这个算法实际上只有 0.5%的错误率 因此这甚至比 我们之前得到的1%的错误率更好 这是一个 非机器学习算法 因为它只是预测y总是等于0 这种情况发生在 正例和负例的比率 非常接近于 一个极端 在这个例子中 正样本的数量 与负样本的数量相比 非常非常少 因为y=1非常少 我们把这种情况叫做 偏斜类 一个类中的样本数 与另一个类的数据相比 多很多 通过总是预测y=0 或者 总是预测y=1 算法可能表现非常好 因此使用分类误差 或者分类精确度 来作为评估度量可能会产生如下问题 假如说你有一个算法 它的精确度是99.2% 因此它只有0.8%的误差 假设 你对你的算法做出了一点改动 现在你得到了 99.5%的精确度 只有0.5%的误差 这到底是不是算法的一个提升呢 用某个实数来 作为评估度量值 的一个好处就是 它可以帮助我们迅速决定 我们是否需要对算法做出一些改进 将精确度从99.2%提高到99.5% 但是我们的改进到底是有用的 还是说 我们只是把代码替换成了 例如总是预测y=0 这样的东西 因此如果你有一个偏斜类 用分类精确度 并不能很好地衡量算法 因为你可能会获得一个很高的精确度 非常低的错误率 但是我们并不知道 我们是否真的提升了 分类模型的质量 因为总是预测y=0 并不是一个 好的分类模型 但是总是预测y=0 会将你的误差降低至 比如 降低至0.5%
### 采用查准率和召回率对偏斜类数据集评估机器学习算法精确度
![采用查准率和召回率对偏斜类数据集评估机器学习算法精确度](amWiki/images/001/06-Week6/2-Machine Learning System Design\12-采用查准率和召回率对偏斜类数据集评估机器学习算法精确度.jpg)  
当我们遇到 这样一个偏斜类时 我们希望有一个 不同的误差度量值 或者不同的评估度量值 其中一种评估度量值 叫做查准率（precision）和召回率（recall）让我来解释一下 假设我们正在用测试集来评估一个分类模型 对于 测试集中的样本 每个测试集中的样本 都会等于 0或者1 假设这是一个二分问题 我们的学习算法 要做的是 做出值的预测 并且学习算法 会为每一个 测试集中的实例 做出预测 预测值也是等于0或1 让我画一个 2x2的表格 基于所有这些值 基于 实际的类与预测的类 如果 有一个样本它实际所属的类是1 预测的类也是1 那么 我们把这个样本叫做真阳性（true positive） 意思是说我们的学习算法 预测这个值为阳性 实际上这个样本也确实是阳性 如果我们的学习算法 预测某个值是阴性 等于0 实际的类也确实属于0 那么我们把这个叫做真阴性（true negative） 我们预测为0的值实际上也等于0 还剩另外的两个单元格 如果我们的学习算法 预测某个值等于1 但是实际上它等于0 这个叫做假阳性（false positive）比如我们的算法 预测某些病人患有癌症 但是事实上他们并没有得癌症 最后 这个单元格是 1和0 这个叫做假阴性（false negative） 因为我们的算法预测值为0 但是实际值是1 这样 我们有了一个2x2的表格 基于 实际类与预测类 这样我们有了一个 另一种方式来 评估算法的表现 我们要计算两个数字 第一个叫做查准率 这个意思是 对于所有我们预测 他们患有癌症的病人 有多大比率的病人是真正患有癌症的 让我把这个写下来 一个分类模型的查准率 等于 真阳性除以所有我们预测为阳性的数量 对于那些病人 我们告诉他们 "你们患有癌症" 对于这些病人而言 有多大比率是真正患有癌症的 这个就叫做查准率 另一个写法是 分子是真阳性 分母是 所有预测阳性的数量 那么这个等于 表格第一行的值 的和 也就是真阳性除以真阳性... 这里我把阳性简写为 POS 加上假阳性 这里我还是把阳性简写为POS 这个就叫做查准率 查准率越高就越好 这是说 对于那些病人 我们告诉他们 "非常抱歉 我们认为你得了癌症" 高查准率说明 对于这类病人 我们对预测他们得了癌症 有很高的准确率 另一个数字我们要计算的 叫做召回率 召回率是 如果所有的病人 假设测试集中的病人 或者交叉验证集中的 如果所有这些在数据集中的病人 确实得了癌症 有多大比率 我们正确预测他们得了癌症 如果所有的病人 都患了癌症 有多少人我们能够 正确告诉他们 你需要治疗 把这个写下来 召回率被定义为 真阳性 的数量 意思是我们正确预测 患有癌症的人 的数量 我们用这个来 除以 实际阳性 这个值是 所有患有癌症的人的数量 有多大比率 我们能正确发现癌症 并给予治疗 把这个以另一种形式 写下来 分母是 实际阳性的数量 表格第一列值的和将这个以不同的形式写下来 那就是 真阳性除以真阳性 加上假阴性 同样地 召回率越高越好 通过计算查准率 和召回率 我们能更好的知道 分类模型到底好不好 具体地说 如果我们有一个算法 总是预测y=0 它总是预测 没有人患癌症 那么这个分类模型 召回率等于0 因为它不会有 真阳性 因此我们能会快发现 这个分类模型 总是预测y=0 它不是一个好的模型 总的来说 即使我们有一个 非常偏斜的类 算法也不能够 "欺骗"我们 仅仅通过预测 y总是等于0 或者y总是等于1 它没有办法得到 高的查准率 和高的召回率 因此我们 能够更肯定 拥有高查准率或者高召回率的模型 是一个好的分类模型 这给予了我们一个 更好的评估值 给予我们一种更直接的方法 来评估模型的好与坏 最后一件需要记住的事 在查准率和召回率的定义中 我们定义 查准率和召回率 我们总是习惯性地用y=1 如果这个类出现得非常少 因此如果我们试图检测 某种很稀少的情况 比如癌症 我希望它是个很稀少的情况 查准率和召回率 会被定义为 y=1 而不是y=0 作为某种我们希望检测的 出现较少的类 通过使用查准率和召回率 我们发现 即使我们拥有 非常偏斜的类 算法不能够 通过总是预测y=1  来"欺骗"我们 或者总是预测y=0 因为它不能够获得高查准率和召回率 具体地说 如果一个分类模型 拥有高查准率和召回率 那么 我们可以确信地说 这个算法表现很好 即便我们拥有很偏斜的类 因此对于偏斜类的问题 查准率和召回率 给予了我们更好的方法 来检测学习算法表现如何 这是一种 更好地评估学习算法的标准 当出现偏斜类时 比仅仅只用分类误差或者分类精度好
### 内置习题
![内置习题_理解查准率和召回率](amWiki/images/001/06-Week6/2-Machine Learning System Design/13-内置习题_理解查准率和召回率1.jpg)  
![内置习题_理解查准率和召回率](amWiki/images/001/06-Week6/2-Machine Learning System Design/14-内置习题_理解查准率和召回率2.jpg)  
## English
### The problem of skewed classes_cancer prediction
![The problem of skewed classes_cancer prediction](amWiki/images/001/06-Week6/2-Machine Learning System Design\11-偏斜类问题举例_癌症预测.jpg)  
In the previous video, I talked about error analysis and the importance of having error metrics, that is of having a single real number evaluation metric for your learning algorithm to tell how well it's doing.In the context of evaluation and of error metrics, there is one important case, where it's particularly tricky to come up with an appropriate error metric, or evaluation metric, for your learning algorithm.That case is the case of what's called skewed classes.Let me tell you what that means.Consider the problem of cancer classification, where we have features of medical patients and we want to decide whether or not they have cancer. So this is like the malignant versus benign tumor classification example that we had earlier.So let's say y equals 1 if the patient has cancer and y equals 0 if they do not. We have trained the progression classifier and let's say we test our classifier on a test set and find that we get 1 percent error. So, we're making 99% correct diagnosis. Seems like a really impressive result, right. We're correct 99% percent of the time.But now, let's say we find out that only 0.5 percent of patients in our training test sets actually have cancer. So only half a percent of the patients that come through our screening process have cancer.In this case, the 1% error no longer looks so impressive.And in particular, here's a piece of code, here's actually a piece of non learning code that takes this input of features x and it ignores it. It just sets y equals 0 and always predicts, you know, nobody has cancer and this algorithm would actually get 0.5 percent error. So this is even better than the 1% error that we were getting just now and this is a non learning algorithm that you know, it is just predicting y equals 0 all the time.So this setting of when the ratio of positive to negative examples is very close to one of two extremes, where, in this case, the number of positive examples is much, much smaller than the number of negative examples because y equals one so rarely, this is what we call the case of skewed classes.We just have a lot more of examples from one class than from the other class. And by just predicting y equals 0 all the time, or maybe our predicting y equals 1 all the time, an algorithm can do pretty well. So the problem with using classification error or classification accuracy as our evaluation metric is the following.Let's say you have one joining algorithm that's getting 99.2% accuracy.So, that's a 0.8% error. Let's say you make a change to your algorithm and you now are getting 99.5% accuracy.That is 0.5% error.
So, is this an improvement to the algorithm or not? One of the nice things about having a single real number evaluation metric is this helps us to quickly decide if we just need a good change to the algorithm or not. By going from 99.2% accuracy to 99.5% accuracy.You know, did we just do something useful or did we just replace our code with something that just predicts y equals zero more often? So, if you have very skewed classes it becomes much harder to use just classification accuracy, because you can get very high classification accuracies or very low errors, and it's not always clear if doing so is really improving the quality of your classifier because predicting y equals 0 all the time doesn't seem like a particularly good classifier.But just predicting y equals 0 more often can bring your error down to, you know, maybe as low as 0.5%.
### Using the precision and recall evaluate skewed classes of machine learning algorithms
![Using the precision and recall evaluate skewed classes of machine learning algorithms](amWiki/images/001/06-Week6/2-Machine Learning System Design\12-采用查准率和召回率对偏斜类数据集评估机器学习算法精确度.jpg)  
When we're faced with such a skewed classes therefore we would want to come up with a different error metric or a different evaluation metric. One such evaluation metric are what's called precision recall.Let me explain what that is.Let's say we are evaluating a classifier on the test set. For the examples in the test set the actual class of that example in the test set is going to be either one or zero, right, if there is a binary classification problem.And what our learning algorithm will do is it will, you know, predict some value for the class and our learning algorithm will predict the value for each example in my test set and the predicted value will also be either one or zero.So let me draw a two by two table as follows, depending on a full of these entries depending on what was the actual class and what was the predicted class. If we have an example where the actual class is one and the predicted class is one then that's called an example that's a true positive, meaning our algorithm predicted that it's positive and in reality the example is positive. If our learning algorithm predicted that something is negative, class zero, and the actual class is also class zero then that's what's called a true negative. We predicted zero and it actually is zero.To find the other two boxes, if our learning algorithm predicts that the class is one but the actual class is zero, then that's called a false positive.So that means our algorithm for the patient is cancelled out in reality if the patient does not.And finally, the last box is a zero, one. That's called a false negative because our algorithm predicted zero, but the actual class was one.And so, we have this little sort of two by two table based on what was the actual class and what was the predicted class.So here's a different way of evaluating the performance of our algorithm. We're going to compute two numbers. The first is called precision - and what that says is,of all the patients where we've predicted that they have cancer,what fraction of them actually have cancer?So let me write this down, the precision of a classifier is the number of true positives divided by the number that we predicted as positive, right?So of all the patients that we went to those patients and we told them, "We think you have cancer." Of all those patients, what fraction of them actually have cancer? So that's called precision. And another way to write this would be true positives and then in the denominator is the number of predicted positives, and so that would be the sum of the, you know, entries in this first row of the table. So it would be true positives divided by true positives. I'm going to abbreviate positive as POS and then plus false positives, again abbreviating positive using POS.So that's called precision, and as you can tell high precision would be good. That means that all the patients that we went to and we said, "You know, we're very sorry. We think you have cancer," high precision means that of that group of patients most of them we had actually made accurate predictions on them and they do have cancer.The second number we're going to compute is called recall, and what recall say is, if all the patients in, let's say, in the test set or the cross-validation set, but if all the patients in the data set that actually have cancer,what fraction of them that we correctly detect as having cancer. So if all the patients have cancer, how many of them did we actually go to them and you know, correctly told them that we think they need treatment.So, writing this down, recall is defined as the number of positives, the number of true positives, meaning the number of people that have cancer and that we correctly predicted have cancer and we take that and divide that by, divide that by the number of actual positives,so this is the right number of actual positives of all the people that do have cancer. What fraction do we directly flag and you know, send the treatment.So, to rewrite this in a different form, the denominator would be the number of actual positives as you know, is the sum of the entries in this first column over here.And so writing things out differently, this is therefore, the number of true positives, divided by the number of true positives plus the number of false negatives.And so once again, having a high recall would be a good thing.So by computing precision and recall this will usually give us a better sense of how well our classifier is doing.And in particular if we have a learning algorithm that predicts y equals zero all the time, if it predicts no one has cancer, then this classifier will have a recall equal to zero, because there won't be any true positives and so that's a quick way for us to recognize that, you know, a classifier that predicts y equals 0 all the time, just isn't a very good classifier. And more generally, even for settings where we have very skewed classes, it's not possible for an algorithm to sort of "cheat" and somehow get a very high precision and a very high recall by doing some simple thing like predicting y equals 0 all the time or predicting y equals 1 all the time. And so we're much more sure that a classifier of a high precision or high recall actually is a good classifier, and this gives us a more useful evaluation metric that is a more direct way to actually understand whether, you know, our algorithm may be doing well.So one final note in the definition of precision and recall, that we would define precision and recall, usually we use the convention that y is equal to 1, in the presence of the more rare class. So if we are trying to detect. rare conditions such as cancer, hopefully that's a rare condition, precision and recall are defined setting y equals 1, rather than y equals 0, to be sort of that the presence of that rare class that we're trying to detect. And by using precision and recall, we find, what happens is that even if we have very skewed classes, it's not possible for an algorithm to you know, "cheat" and predict y equals 1 all the time, or predict y equals 0 all the time, and get high precision and recall. And in particular, if a classifier is getting high precision and high recall, then we are actually confident that the algorithm has to be doing well, even if we have very skewed classes.So for the problem of skewed classes precision recall gives us more direct insight into how the learning algorithm is doing and this is often a much better way to evaluate our learning algorithms, than looking at classification error or classification accuracy, when the classes are very skewed.
### Exam_in the video
![Exam_Understand the precision and recall](amWiki/images/001/06-Week6/2-Machine Learning System Design/13-内置习题_理解查准率和召回率1.jpg)
![Exam_Understand the precision and recall](amWiki/images/001/06-Week6/2-Machine Learning System Design/14-内置习题_理解查准率和召回率2.jpg)  
