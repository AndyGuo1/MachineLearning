# 机器学习系统设计-误差分析
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/06-Week6/2-Machine Learning System Design/2-Error Analysis.mp4" type="video/mp4">
</video>
## 中文
### 构造机器学习系统的建议方法
![构造机器学习系统的建议方法](amWiki/images/001/06-Week6/2-Machine Learning System Design\6-构造机器学习系统的建议方法.jpg)  
在上一节课中 我讲到了应当怎样面对 机器学习问题 有很多提高算法表现的方法 在本次课程中 我们将会讲到 误差分析（error analysis）的概念 这会帮助你 更系统地做出决定 如果你准备 研究机器学习的东西 或者构造机器学习应用程序 最好的实践方法 不是建立一个 非常复杂的系统 拥有多么复杂的变量 而是 构建一个简单的算法 这样你可以很快地实现它 每当我研究 机器学习的问题时 我最多只会花一天的时间 就是字面意义上的24小时 来试图很快的把结果搞出来 即便效果不好 坦白的说 就是根本没有用复杂的系统 但是只是很快的得到的结果 即便运行得不完美 但是也把它运行一遍 最后通过交叉验证来检验数据 一旦做完 你可以画出学习曲线 这个我们在前面的课程中已经讲过了 通过画出学习曲线 以及检验误差 来找出 你的算法是否有 高偏差和高方差的问题 或者别的问题 在这样分析之后 再来决定用更多的数据训练 或者加入更多的特征变量是否有用 这么做的原因是 这在你刚接触机器学习问题时 是一个很好的方法 你并不能 提前知道 你是否需要复杂的特征变量 或者你是否需要 更多的数据 还是别的什么 提前知道你应该做什么 是非常难的 因为你缺少证据 缺少学习曲线 因此 你很难知道 你应该把时间花在什么地方来提高算法的表现 但是当你实践一个 非常简单即便不完美的 方法时 你可以通过画出学习曲线来做出进一步的选择 你可以 用这种方式 来避免一种 电脑编程里的过早优化问题 这种理念是 我们必须 用证据来领导我们的决策 怎样分配自己的时间来优化算法 而不是仅仅凭直觉 凭直觉得出的东西一般总是错误的
### 误差分析
![误差分析](amWiki/images/001/06-Week6/2-Machine Learning System Design\7-误差分析.jpg)  
除了画出学习曲线之外 一件非常有用的事是 误差分析 我的意思是说 当我们在构造 比如构造垃圾邮件分类器时 我会看一看 我的交叉验证数据集 然后亲自看一看 哪些邮件被算法错误地分类 因此 通过这些 被算法错误分类的垃圾邮件 与非垃圾邮件 你可以发现某些系统性的规律 什么类型的邮件总是被错误分类 经常地 这样做之后 这个过程能启发你构造新的特征变量 或者告诉你 现在 这个系统的短处 然后启发你 如何去提高它 具体地说 这里有一个例子 假设你正在构造一个 垃圾邮件分类器 你拥有500个实例 在交叉验证集中 假设在这个例子中 该算法有非常高的误差率 它错误分类了 一百个交叉验证实例 所以我要做的是 人工检查这100个错误 然后手工为它们分类 基于例如 这些是什么类型的邮件 哪些变量 能帮助这个算法来正确分类它们 明确地说 通过鉴定这是哪种类型的邮件 通过检查 这一百封错误分类的邮件 我可能会发现 最容易被误分类的邮件 可能是 有关药物的邮件 基本上这些邮件都是卖药的 或者 卖仿品的 比如卖假表 或者一些骗子邮件 又叫做钓鱼邮件 等等 所以 在检查哪些邮件被错误分类的时候 我会看一看每封邮件 数一数 比如 在这100封错误归类的邮件中 我发现有12封 错误归类的邮件是和卖药有关的邮件 4封 是推销仿品的 推销假表或者别的东西 然后我发现 有53封邮件 是钓鱼邮件 诱骗你 告诉他们你的密码 剩下的31封别的类型的邮件 通过算出 每个类别中 不同的邮件数 你可能会发现 比如 该算法在区分钓鱼邮件的时候 总是表现得很差 这说明 你应该花更多的时间 来研究这种类型的邮件 然后 看一看你是否能通过构造更好的特征变量 来正确区分这种类型的邮件 同时 我要做的是 看一看哪些特征变量 可能会帮助算法正确地分类邮件 我们假设 能帮助我们提高 邮件分类表现 的方法是 检查有意的拼写错误 不寻常的邮件路由来源以及垃圾邮件特有的标点符号方式 比如很多感叹号 与之前一样 我会手动地浏览这些邮件 假设有5封这种类型的邮件 16封这种类型的 32封这种类型的 以及一些别的类型的 如果 这就是你从交叉验证中得到的结果 那么 这可能说明 有意地拼写错误出现频率较少 这可能并不值得 你花费时间 去编写算法来检测这种类型的邮件 但是如果你发现 很多的垃圾邮件 都有不一般的标点符号规律 那么这是一个很强的特征 说明你应该 花费你的时间 去构造基于标点符号的 更加复杂的特征变量 因此 这种类型的误差分析 是一种手动检测的过程 检测算法可能会犯的错误 这经常能够帮助你 找到更为有效的手段 这也解释了为什么 我总是推荐先实践一种 快速即便不完美的算法 我们真正想要的是 找出什么类型的邮件 是这种算法最难分类出来的 对于不同的算法 不同的机器学习算法 它们 所遇到的问题一般总是相同的 通过实践一些快速 即便不完美的算法 你能够更快地 找到错误的所在 并且快速找出算法难以处理的例子 这样你就能集中精力在这些真正的问题上
### 数值计算的方式评估机器学习算法的重要性
![数值计算的方式评估机器学习算法的重要性](amWiki/images/001/06-Week6/2-Machine Learning System Design\8-数值计算的方式评估机器学习算法的重要性.jpg)  
最后 在构造机器学习算法时另一个有用的小窍门是 保证你自己 保证你能有一种 数值计算的方式来评估你的机器学习算法 我这么说的意思是 如果你在构造一个学习算法 如果你能有一种 评估你算法的方法 这是非常有用的 一种用数字说话的评估方法 你的算法可能精确 可能有错 但是它能准确的告诉你你的算法到底表现有多好 在接下来的课程中 我会更详细的讲述这个概念 但是先看看这个例子 假设我们试图 决定是否应该 把像"discount""discounts""discounter""discountring" 这样的单词都视为等同 一种方法 是检查这些单词的 开头几个字母 比如 当你在检查这些单词开头几个字母的时候 你发现 这几个单词 大概可能有着相同的意思 在自然语言处理中 这种方法 是通过一种叫做词干提取的软件实现的 如果你想自己来试试 你可以 在网上搜索一下 "Porter Stemmer(波特词干提取法)" 这是在词干提取方面 一个比较不错的软件 这个软件会 将单词"discount""discounts"以及等等 都视为同一个单词 但是这种词干提取软件 只会检查 单词的头几个字母 这有用 但是也可能会造成一些问题 因为 举个例子 因为这个软件会把单词"universe(宇宙)" 和"university(大学)" 也视为同一个单词 因为 这两个单词开头的字母是一样的 因此 当你在决定 是否应该使用词干提取软件用来分类 这总是很难说清楚 特别地 误差分析 也并不能帮助你决定 词干提取是不是一个好的方法 与之相对地 最好的方法 来发现词干提取软件 对你的分类器 到底有没有用 是迅速地着手试一试 来看看它表现到底怎么样 为了这么做 通过数值来评估你的算法 是非常有用的 具体地说 自然而然地 你应该通过交叉验证 来验证不用词干提取与用词干提取的算法的错误率 因此 如果你不在你的算法中使用词干提取 然后你得到 比如 5%的分类错误率 然后你再使用词干提取来运行你的算法 你得到 比如 3%的分类错误 那么这很大的减少了错误发生 于是你决定 词干提取是一个好的办法 就这个特定的问题而言 这里有一个数量的评估数字 即交差验证错误率 我们以后会发现 这个例子中的评估数字 还需要一些处理 但是 我们可以在今后的课程中看到 这么做还是会让你 能更快地做出决定 比如 是否使用词干提取 再说一个例子 假设 你在想是否应该 区分单词的大小写 比如 单词"mom" 大写的"M" 和小写的"m" 它们应该被视作 同一个单词还是不同的单词 它们应该被视作相同的特征变量还是不同的 再说一次 因为我们有一种 能够评估我们算法的方法 如果你在这里试一试 如果我不区分 大小写 最后得到3.2%的错误率 然后我发现 这个表现的较差些 如果 如果我只用了词干提取 这之后我再思考 是否要区分 大小写 因此当你在 构造学习算法的时候 你总是会去尝试 很多新的想法 实现出很多版本的学习算法 如果每一次 你实践新想法的时候 你都手动地检测 这些例子 去看看是表现差还是表现好 那么这很难让你 做出决定 到底是否使用词干提取 是否区分大小写 但是通过一个 量化的数值评估 你可以看看这个数字 误差是变大还是变小了 你可以通过它 更快地实践 你的新想法 它基本上非常直观地告诉你 你的想法是提高了算法表现 还是让它变得更坏 这会大大提高 你实践算法时的速度 所以我强烈推荐 在交叉验证集上来实施误差分析 而不是在测试集上 但是 还是有一些人 会在测试集上来做误差分析 即使这从数学上讲 是不合适的 所以我还是推荐你 在交叉验证向量上 来做误差分析 总结一下 当你在研究一个新的机器学习问题时 我总是推荐你 实现一个较为简单快速 即便不是那么完美的算法 我几乎从未见过 人们这样做 大家经常干的事情是 花费大量的时间 在构造算法上 构造他们以为的简单的方法 因此 不要担心你的算法太简单 或者太不完美 而是尽可能快地 实现你的算法 当你有了初始的实现之后 它会变成一个非常有力的工具 来帮助你决定 下一步的做法 因为我们可以先看看算法造成的错误 通过误差分析 来看看他犯了什么错 然后来决定优化的方式 另一件事是 假设你有了一个快速而不完美的算法实现 又有一个数值的评估数据 这会帮助你 尝试新的想法 快速地发现 你尝试的这些想法 是否能够提高算法的表现 从而 你会更快地 做出决定 在算法中放弃什么 吸收什么
### 内置习题
![内置习题_理解用验证集进行误差分析的原因](amWiki/images/001/06-Week6/2-Machine Learning System Design/9-内置习题_理解用验证集进行误差分析的原因.jpg)  
## English
### Recommend approach for design machine learning system
![Recommend approach for design machine learning system](amWiki/images/001/06-Week6/2-Machine Learning System Design\6-构造机器学习系统的建议方法.jpg)  
In the last video I talked about how, when faced with a machine learning problem, there are often lots of different ideas for how to improve the algorithm. In this video, let's talk about the concept of error analysis. Which will hopefully give you a way to more systematically make some of these decisions.If you're starting work on a machine learning problem, or building a machine learning application. It's often considered very good practice to start, not by building a very complicated system with lots of complex features and so on. But to instead start by building a very simple algorithm that you can implement quickly. And when I start with a learning problem what I usually do is spend at most one day, like literally at most 24 hours, To try to get something really quick and dirty. Frankly not at all sophisticated system but get something really quick and dirty running, and implement it and then test it on my cross-validation data. Once you've done that you can then plot learning curves, this is what we talked about in the previous set of videos. But plot learning curves of the training and test errors to try to figure out if you're learning algorithm maybe suffering from high bias or high variance, or something else. And use that to try to decide if having more data, more features, and so on are likely to help. And the reason that this is a good approach is often, when you're just starting out on a learning problem, there's really no way to tell in advance. Whether you need more complex features, or whether you need more data, or something else. And it's just very hard to tell in advance, that is, in the absence of evidence, in the absence of seeing a learning curve. It's just incredibly difficult to figure out where you should be spending your time. And it's often by implementing even a very, very quick and dirty implementation. And by plotting learning curves, that helps you make these decisions. So if you like you can to think of this as a way of avoiding whats sometimes called premature optimization in computer programming. And this idea that says we should let evidence guide our decisions on where to spend our time rather than use gut feeling, which is often wrong.
### Error analysis
![Error analysis](amWiki/images/001/06-Week6/2-Machine Learning System Design\7-误差分析.jpg)  
In addition to plotting learning curves, one other thing that's often very useful to do is what's called error analysis. And what I mean by that is that when building say a spam classifier. I will often look at my cross validation set and manually look at the emails that my algorithm is making errors on. So look at the spam e-mails and non-spam e-mails that the algorithm is misclassifying and see if you can spot any systematic patterns in what type of examples it is misclassifying. And often, by doing that, this is the process that will inspire you to design new features. Or they'll tell you what are the current things or current shortcomings of the system. And give you the inspiration you need to come up with improvements to it. Concretely, here's a specific example. Let's say you've built a spam classifier and you have 500 examples in your cross validation set. And let's say in this example that the algorithm has a very high error rate. And this classifies 100 of these cross validation examples.So what I do is manually examine these 100 errors and manually categorize them. Based on things like what type of email it is, what cues or what features you think might have helped the algorithm classify them correctly. So, specifically, by what type of email it is, if I look through these 100 errors, I might find that maybe the most common types of spam emails in these classifies are maybe emails on pharma or pharmacies, trying to sell drugs. Maybe emails that are trying to sell replicas such as fake watches, fake random things, maybe some emails trying to steal passwords,. These are also called phishing emails, that's another big category of emails, and maybe other categories. So in terms of classify what type of email it is, I would actually go through and count up my hundred emails. Maybe I find that 12 of them is label emails, or pharma emails, and maybe 4 of them are emails trying to sell replicas, that sell fake watches or something. And maybe I find that 53 of them are these what's called phishing emails, basically emails trying to persuade you to give them your password. And 31 emails are other types of emails. And it's by counting up the number of emails in these different categories that you might discover, for example. That the algorithm is doing really, particularly poorly on emails trying to steal passwords. And that may suggest that it might be worth your effort to look more carefully at that type of email and see if you can come up with better features to categorize them correctly. And, also what I might do is look at what cues or what additional features might have helped the algorithm classify the emails. So let's say that some of our hypotheses about things or features that might help us classify emails better are. Trying to detect deliberate misspellings versus unusual email routing versus unusual spamming punctuation. Such as if people use a lot of exclamation marks. And once again I would manually go through and let's say I find five cases of this and 16 of this and 32 of this and a bunch of other types of emails as well. And if this is what you get on your cross validation set, then it really tells you that maybe deliberate spellings is a sufficiently rare phenomenon that maybe it's not worth all the time trying to write algorithms that detect that. But if you find that a lot of spammers are using, you know, unusual punctuation, then maybe that's a strong sign that it might actually be worth your while to spend the time to develop more sophisticated features based on the punctuation. So this sort of error analysis, which is really the process of manually examining the mistakes that the algorithm makes, can often help guide you to the most fruitful avenues to pursue. And this also explains why I often recommend implementing a quick and dirty implementation of an algorithm. What we really want to do is figure out what are the most difficult examples for an algorithm to classify. And very often for different algorithms, for different learning algorithms they'll often find similar categories of examples difficult. And by having a quick and dirty implementation, that's often a quick way to let you identify some errors and quickly identify what are the hard examples. So that you can focus your effort on those.
### The importance of  numerical calculation of evaluating machine learning algorithms
![The importance of  numerical calculation of evaluating machine learning algorithmss](amWiki/images/001/06-Week6/2-Machine Learning System Design\8-数值计算的方式评估机器学习算法的重要性.jpg)  
Lastly, when developing learning algorithms, one other useful tip is to make sure that you have a numerical evaluation of your learning algorithm.And what I mean by that is you if you're developing a learning algorithm, it's often incredibly helpful. If you have a way of evaluating your learning algorithm that just gives you back a single real number, maybe accuracy, maybe error. But the single real number that tells you how well your learning algorithm is doing. I'll talk more about this specific concept in later videos, but here's a specific example. Let's say we're trying to decide whether or not we should treat words like discount, discounts, discounted, discounting as the same word. So you know maybe one way to do that is to just look at the first few characters in the word like, you know. If you just look at the first few characters of a word, then you figure out that maybe all of these words roughly have similar meanings.In natural language processing, the way that this is done is actually using a type of software called stemming software. And if you ever want to do this yourself, search on a web-search engine for the porter stemmer, and that would be one reasonable piece of software for doing this sort of stemming, which will let you treat all these words, discount, discounts, and so on, as the same word.But using a stemming software that basically looks at the first few alphabets of a word, more of less, it can help, but it can hurt. And it can hurt because for example, the software may mistake the words universe and university as being the same thing. Because, you know, these two words start off with the same alphabets.So if you're trying to decide whether or not to use stemming software for a spam cross classifier, it's not always easy to tell. And in particular, error analysis may not actually be helpful for deciding if this sort of stemming idea is a good idea. Instead, the best way to figure out if using stemming software is good to help your classifier is if you have a way to very quickly just try it and see if it works.And in order to do this, having a way to numerically evaluate your algorithm is going to be very helpful. Concretely, maybe the most natural thing to do is to look at the cross validation error of the algorithm's performance with and without stemming. So, if you run your algorithm without stemming and end up with 5 percent classification error. And you rerun it and you end up with 3 percent classification error, then this decrease in error very quickly allows you to decide that it looks like using stemming is a good idea. For this particular problem, there's a very natural, single, real number evaluation metric, namely the cross validation error. We'll see later examples where coming up with this sort of single, real number evaluation metric will need a little bit more work. But as we'll see in a later video, doing so would also then let you make these decisions much more quickly of say, whether or not to use stemming.And, just as one more quick example, let's say that you're also trying to decide whether or not to distinguish between upper versus lower case. So, you know, as the word, mom, were upper case, and versus lower case m, should that be treated as the same word or as different words? Should this be treated as the same feature, or as different features?And so, once again, because we have a way to evaluate our algorithm. If you try this down here, if I stopped distinguishing upper and lower case, maybe I end up with 3.2 percent error. And I find that therefore, this does worse than if I use only stemming. So, this let's me very quickly decide to go ahead and to distinguish or to not distinguish between upper and lowercase. So when you're developing a learning algorithm, very often you'll be trying out lots of new ideas and lots of new versions of your learning algorithm. If every time you try out a new idea, if you end up manually examining a bunch of examples again to see if it got better or worse, that's gonna make it really hard to make decisions on. Do you use stemming or not? Do you distinguish upper and lower case or not? But by having a single real number evaluation metric, you can then just look and see, oh, did the arrow go up or did it go down? And you can use that to much more rapidly try out new ideas and almost right away tell if your new idea has improved or worsened the performance of the learning algorithm. And this will let you often make much faster progress. So the recommended, strongly recommended the way to do error analysis is on the cross validations there rather than the test set. But, you know, there are people that will do this on the test set, even though that's definitely a less mathematic appropriate, certainly a less recommended way to, thing to do than to do error analysis on your cross validation set. Set to wrap up this video, when starting on a new machine learning problem, what I almost always recommend is to implement a quick and dirty implementation of your learning out of them. And I've almost never seen anyone spend too little time on this quick and dirty implementation. I've pretty much only ever seen people spend much too much time building their first, supposedly, quick and dirty implementation. So really, don't worry about it being too quick, or don't worry about it being too dirty. But really, implement something as quickly as you can. And once you have the initial implementation, this is then a powerful tool for deciding where to spend your time next. Because first you can look at the errors it makes, and do this sort of error analysis to see what other mistakes it makes, and use that to inspire further development. And second, assuming your quick and dirty implementation incorporated a single real number evaluation metric. This can then be a vehicle for you to try out different ideas and quickly see if the different ideas you're trying out are improving the performance of your algorithm. And therefore let you, maybe much more quickly make decisions about what things to fold in and what things to incorporate into your learning algorithm.
### Exam_in the video
![Exam_Understand the reason of the error analysis in validation set](amWiki/images/001/06-Week6/2-Machine Learning System Design/9-内置习题_理解用验证集进行误差分析的原因.jpg)  
