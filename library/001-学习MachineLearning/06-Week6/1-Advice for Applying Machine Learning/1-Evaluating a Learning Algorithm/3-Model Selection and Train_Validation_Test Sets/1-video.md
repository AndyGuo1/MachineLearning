# 应用机器学习算法建议-模型选择_训练集/交叉验证集/测试集
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/06-Week6/1-Advice for Applying Machine Learning/3-Model Selection and Train_Validation_Test Sets.mp4" type="video/mp4">
</video>
## 中文
### 过拟合现象
![过拟合现象](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/10-过拟合现象.jpg)  
假如你想要确定对于某组数据 最合适的多项式次数是几次 怎样选用正确的特征来构造学习算法 或者假如你需要正确选择 学习算法中的正则化参数λ 你应该怎样做呢？ 这些问题我们称之为模型选择问题 在我们对于这一问题的讨论中 我们还将提到 如何将数据分为三组 也就是训练集、验证集和测试集 而不仅仅是前面提到的两组数据 在这节视频中 我们将会介绍这些内容的含义 以及如何使用它们进行模型选择 我们已经多次接触到过拟合现象 在过拟合的情况中  学习算法在适用于训练集时表现非常完美  但这并不代表此时的假设也很完美 更一般地说 这也是为什么训练集误差 通常不能正确预测出该假设是否能很好地拟合新样本的原因 具体来讲 如果你把这些参数集 比如θ0 θ1 θ2等等 调整到非常拟合你的训练集 那么结果就是 你的假设会在训练集上表现地很好  但这并不能确定 当你的假设推广到训练集之外的新的样本上时  预测的结果是怎样的 而更为普遍的规律是 只要你的参数非常拟合某个数据组 比如说 非常拟合训练集 当然也可以是其他数据集 那么你的假设对于相同数据组的预测误差 比如说训练误差 是不能够用来推广到一般情况的 或者说 是不能作为实际的泛化误差的 也就是说 不能说明你的假设对于新样本的效果
### 关于模型选择
![关于模型选择](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/11-关于模型选择.jpg)  
下面我们来考虑模型选择问题 假如说你现在要选择能最好地拟合你数据的多项式次数 换句话说 你应该选择一次函数 二次函数 还是三次函数呢 等等一直到十次函数 所以似乎应该有这样一个参数 这里我用 d 来表示 d表示的就是你应该选择的多项式次数 所以 似乎除了你要确定的参数θ之外 你还要考虑确定一个参数 你同样需要用你的数据组来确定这个多项式的次数d 第一个选择是 d=1 也就表示线性(一次)方程 我们也可以选择d=2或者3 等等一直到d=10 因此 我们想确定这个多出来的参数d最适当的取值 具体地说 比如你想要选择一个模型  那就从这10个模型中 选择一个最适当的多项式次数 并且用这个模型进行估计 预测你的假设能否很好地推广到新的样本上 那么你可以这样做 你可以先选择第一个模型 然后求训练误差的最小值 这样你就会得到 一个参数向量θ 然后你再选择第二个模型 二次函数模型  进行同样的过程 这样你会得到另一个参数向量 θ 为了区别这些不同的 参数向量θ 我想用上标(1) 上标(2)来表示 这里的上标(1)表示的是 在调整第一个模型 使其拟合训练数据时得到的参数θ 同样地 θ上标(2)表示的是 二次函数在和训练数据拟合的过程中得到的参数 以此类推 在拟合三次函数模型时我又得到一个参数θ(3) 等等 直到θ(10)  接下来我们要做的是 对所有这些模型 求出测试集误差 因此 我可以算出 Jtest(θ(1))  Jtest(θ(2)) Jtest(θ(3)) 以此类推 也就是对于每一个模型对应的假设 都计算出其作用于测试集的表现如何 接下来为了确定选择哪一个模型最好 我要做的是看看这些模型中 哪一个对应的测试集误差最小 那么对于这一个例子 我们假设最终选择了五次多项式模型 目前看来还比较合理 那么现在 我确定了我的模型 我得到了我的假设 也就是这个五次函数模型 现在我想知道 这个模型能不能很好地推广到新样本 我们可以观察这个五次多项式假设模型  对测试集的拟合情况 但这里有一个问题是 这样做仍然不能公平地说明 我的假设推广到一般时的效果 其原因在于 我们刚才是使用的测试集 跟假设拟合 来得到的 多项式次数d 这个参数 这也就是说 我们选择了一个 能够最好地拟合测试集的 参数d的值 因此 我们的参数向量θ(5) 在拟合测试集时的结果  很可能导致一个比实际泛化误差更完美的预测结果 对吧？ 因为我是找了一个最能拟合测试集的参数d 因此我再用测试集 来评价我的假设就显得不公平了 因为我已经选了一个能够最拟合测试集的参数 我选择的多项式次数d 本身就是按照最拟合测试集来选择的 因此我的假设 很可能很好地拟合测试集 而且这种拟合的效果很可能会比对那些没见过的新样本拟合得更好 而我们其实是更关心对新样本的拟合效果的 所以 再回过头来说 在前面的幻灯片中 我们看到 如果我们 用训练集来拟合参数 θ0 θ1 等等参数时 那么 拟合后的模型 在作用于训练集上的效果 是不能预测出 我们将这个假设推广到新样本上时效果如何的 这是因为这些参数能够很好地拟合训练集 因此它们很有可能 在对训练集的预测中表现地很好 但对其他的新样本来说 就不一定那么好了
### 采用交叉验证集评估假设
![采用交叉验证集评估假设](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/12-采用交叉验证集评估假设.jpg)  
而在刚才这一页幻灯片上 我讲到的步骤 也是在做同样的事 具体来讲 我们做的实际上是用测试集来拟合参数d 通过用测试集来拟合这个参数 同样也意味着 这并不能较为公平地预测出 假设函数的在遇到新样本时的表现 为了解决这一问题 在模型选择中 如果我们想要评价某个假设 我们通常采用以下的方法 给定某个数据集 和刚才将数据分为 训练和测试集不同的是 我们要将其分为三段 第一部分还是叫训练集 所以 我们还是称这部分为训练集 第二部分我把它叫做交叉验证集（cross validation set）Cross validation我用CV来简写“交叉验证” 有时候也直接叫验证集 不叫交叉验证集 最后一部分依然和以前一样是测试集 同时 一种典型的分割比例是 将60%的数据 分给训练集 大约20%的数据给交叉验证集 最后20%给测试集 这个比例可以稍微调整 但这种分法是最典型的 所以现在我们的训练集就只占总数据的60%了 然后交叉验证集 或者说验证集 将拥有一部分样本 我把它的数量用m下标CV来表示 这是交叉验证集样本的数量 按照之前我们的符号表示习惯 我将用(x(i)CV, y(i)CV) 来表示第i个交叉验证样本 最后 我们还是有这样一些测试集样本 用m下标test来表示测试样本的总数 好的 现在我们就定义了训练集、交叉验证集 以及测试集
### 定义训练集、交叉验证集、测试集误差
![定义训练集、交叉验证集、测试集误差](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/13-定义训练集、交叉验证集、测试集误差.jpg)  
我们随之也可以定义训练误差 交叉验证误差 和测试误差 因此这便是我定义的训练误差 我用J下标train来表示 这跟我们之前定义的 J(θ)没有任何区别 也就是对训练集数据进行预测得到的误差 然后J下标CV定义为交叉验证集误差 这也不难想象 跟训练误差类似的定义 只不过是在交叉验证集上预测得到的误差 然后这是测试集 跟前面一样 好的
### 采用交叉验证后改进的模型选择
![采用交叉验证后改进的模型选择](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/14-采用交叉验证后改进的模型选择.jpg)  
那么我们的模型选择问题是这样的 和之前使用测试集来选择模型不同 我们现在要使用验证集 或者说交叉验证集来选择模型 具体来讲 首先我们用第一个假设函数 也就是第一个模型 然后求代价函数的最小值 然后我们会得到这个线性模型对应的参数向量θ 和之前一样 我们还是用上表(1)来表示 这个参数是对应于线性模型的 对二次函数 我们也做同样的事情 这样可以得到θ(2) 然后是θ(3) 等等以此类推 一直到10次多项式 然后我要做的是 跟之前用测试集来预测这些假设不同 我要在交叉验证集中测试这些假设的表现 我要测出Jcv来看看 这些假设在交叉验证集中表现如何 然后我要选择的是交叉验证集误差最小的那个假设 因此 对于这个例子 假如是 四次函数的模型有最小的交叉验证误差 因此 我们就选择这个四次多项式模型 最后 这样做的意义是 参数d 别忘了参数d 是多项式的次数 d=2 d=3 一直到d=10 我们刚才做的是拟合出最好的系数d等于4 并且我们是通过交叉验证集来完成的 因此 这样一来这个参数d 这个多项式的次数 就没有跟测试集进行拟合 这样我们就回避了测试集的嫌疑 我们可以光明正大地使用测试集 来估计所选模型的泛化误差了 好的 这就是模型选择了 以及你应该怎样 将数据分成训练集、验证集和测试集 以及使用你的交叉验证集数据来选择模型 最后用测试集来评价模型的表现 最后我还想提醒的一点是 在如今的机器学习应用中 确实也有很多人是像我之前介绍的那样做的 我说过这并不是一个好的方法 也就是用测试集来选择模型 然后用同样的测试集来 评价模型的表现 报告测试误差 看起来好像还能得到比较不错的泛化误差 这的确是一种做法 但不幸的是 现在还有很多人这样做 如果你有很多很多测试集的话 这也许还能行得通 但大多数的机器学习开发人员 还是不会选择这样做 因为最佳做法还是把数据分成训练集、验证集、测试集 但我还是告诉你 在现实中确实有很大一部分人 有时会使用同样一组数据 既作为验证集 也作为测试集 也就是只有训练集和测试集 你的确可能会看到很多人选择这种方法 但如果可能的话 我反对采用这种方式
### 内置习题
![内置习题_理解交叉验证集](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/15-内置习题_理解交叉验证.jpg)
## English
### Overfitting phenomenon
![Overfitting phenomenon](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/10-过拟合现象.jpg)  
Suppose you're left to decide what degree of polynomial to fit to a data set. So that what features to include that gives you a learning algorithm. Or suppose you'd like to choose the regularization parameter longer for learning algorithm. How do you do that? This account model selection process. Browsers, and in our discussion of how to do this, we'll talk about not just how to split your data into the train and test sets, but how to switch data into what we discover is called the train, validation, and test sets. We'll see in this video just what these things are, and how to use them to do model selection. We've already seen a lot of times the problem of overfitting, in which just because a learning algorithm fits a training set well, that doesn't mean it's a good hypothesis. More generally, this is why the training set's error is not a good predictor for how well the hypothesis will do on new example. Concretely, if you fit some set of parameters. Theta0, theta1, theta2, and so on, to your training set. Then the fact that your hypothesis does well on the training set. Well, this doesn't mean much in terms of predicting how well your hypothesis will generalize to new examples not seen in the training set. And a more general principle is that once your parameter is what fit to some set of data. Maybe the training set, maybe something else. Then the error of your hypothesis as measured on that same data set, such as the training error, that's unlikely to be a good estimate of your actual generalization error. That is how well the hypothesis will generalize to new examples.
### About model selection
![About model selection](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/11-关于模型选择.jpg)  
Now let's consider the model selection problem. Let's say you're trying to choose what degree polynomial to fit to data. So, should you choose a linear function, a quadratic function, a cubic function? All the way up to a 10th-order polynomial.So it's as if there's one extra parameter in this algorithm, which I'm going to denote d, which is, what degree of polynomial. Do you want to pick. So it's as if, in addition to the theta parameters, it's as if there's one more parameter, d, that you're trying to determine using your data set. So, the first option is d equals one, if you fit a linear function. We can choose d equals two, d equals three, all the way up to d equals 10. So, we'd like to fit this extra sort of parameter which I'm denoting by d. And concretely let's say that you want to choose a model, that is choose a degree of polynomial, choose one of these 10 models. And fit that model and also get some estimate of how well your fitted hypothesis was generalize to new examples. Here's one thing you could do. What you could, first take your first model and minimize the training error. And this would give you some parameter vector theta. And you could then take your second model, the quadratic function, and fit that to your training set and this will give you some other. Parameter vector theta. In order to distinguish between these different parameter vectors, I'm going to use a superscript one superscript two there where theta superscript one just means the parameters I get by fitting this model to my training data. And theta superscript two just means the parameters I get by fitting this quadratic function to my training data and so on. By fitting a cubic model I get parenthesis three up to, well, say theta 10. And one thing we ccould do is that take these parameters and look at test error. So I can compute on my test set J test of one, J test of theta two, and so on.J test of theta three, and so on.So I'm going to take each of my hypotheses with the corresponding parameters and just measure the performance of on the test set. Now, one thing I could do then is, in order to select one of these models, I could then see which model has the lowest test set error. And let's just say for this example that I ended up choosing the fifth order polynomial. So, this seems reasonable so far. But now let's say I want to take my fifth hypothesis, this, this, fifth order model, and let's say I want to ask, how well does this model generalize?One thing I could do is look at how well my fifth order polynomial hypothesis had done on my test set. But the problem is this will not be a fair estimate of how well my hypothesis generalizes. And the reason is what we've done is we've fit this extra parameter d, that is this degree of polynomial. And what fits that parameter d, using the test set, namely, we chose the value of d that gave us the best possible performance on the test set. And so, the performance of my parameter vector theta5, on the test set, that's likely to be an overly optimistic estimate of generalization error. Right, so, that because I had fit this parameter d to my test set is no longer fair to evaluate my hypothesis on this test set, because I fit my parameters to this test set, I've chose the degree d of polynomial using the test set. And so my hypothesis is likely to do better on this test set than it would on new examples that it hasn't seen before, and that's which is, which is what I really care about. So just to reiterate, on the previous slide, we saw that if we fit some set of parameters, you know, say theta0, theta1, and so on, to some training set, then the performance of the fitted model on the training set is not predictive of how well the hypothesis will generalize to new examples. Is because these parameters were fit to the training set, so they're likely to do well on the training set, even if the parameters don't do well on other examples.
### Using cross validation set evaluate Hypothesis
![Using cross validation set evaluate Hypothesis](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/12-采用交叉验证集评估假设.jpg)  
And, in the procedure I just described on this line, we just did the same thing. And specifically, what we did was, we fit this parameter d to the test set. And by having fit the parameter to the test set, this means that the performance of the hypothesis on that test set may not be a fair estimate of how well the hypothesis is, is likely to do on examples we haven't seen before. To address this problem, in a model selection setting, if we want to evaluate a hypothesis, this is what we usually do instead. Given the data set, instead of just splitting into a training test set, what we're going to do is then split it into three pieces. And the first piece is going to be called the training set as usual.So let me call this first part the training set.And the second piece of this data, I'm going to call the cross validation set. [SOUND] Cross validation. And the cross validation, as V-D. Sometimes it's also called the validation set instead of cross validation set. And then the loss can be to call the usual test set. And the pretty, pretty typical ratio at which to split these things will be to send 60% of your data's, your training set, maybe 20% to your cross validation set, and 20% to your test set. And these numbers can vary a little bit but this integration be pretty typical. And so our training sets will now be only maybe 60% of the data, and our cross-validation set, or our validation set, will have some number of examples. I'm going to denote that m subscript cv. So that's the number of cross-validation examples.Following our early notational convention I'm going to use xi cv comma y i cv, to denote the i cross validation example. And finally we also have a test set over here with our m subscript test being the number of test examples. So, now that we've defined the training validation or cross validation and test sets.
###  The definition of training error, cross validation error, and test error
![The definition of training error, cross validation error, and test error](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/13-定义训练集、交叉验证集、测试集误差.jpg)  
We can also define the training error, cross validation error, and test error. So here's my training error, and I'm just writing this as J subscript train of theta. This is pretty much the same things. These are the same thing as the J of theta that I've been writing so far, this is just a training set error you know, as measuring a training set and then J subscript cv my cross validation error, this is pretty much what you'd expect, just like the training error you've set measure it on a cross validation data set, and here's my test set error same as before.
### Improved after using cross validation of models selection
![Improved after using cross validation of models selection](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/14-采用交叉验证后改进的模型选择.jpg)  
So when faced with a model selection problem like this, what we're going to do is, instead of using the test set to select the model, we're instead going to use the validation set, or the cross validation set, to select the model. Concretely, we're going to first take our first hypothesis, take this first model, and say, minimize the cross function, and this would give me some parameter vector theta for the new model. And, as before, I'm going to put a superscript 1, just to denote that this is the parameter for the new model. We do the same thing for the quadratic model. Get some parameter vector theta two. Get some para, parameter vector theta three, and so on, down to theta ten for the polynomial. And what I'm going to do is, instead of testing these hypotheses on the test set, I'm instead going to test them on the cross validation set. And measure J subscript cv, to see how well each of these hypotheses do on my cross validation set.And then I'm going to pick the hypothesis with the lowest cross validation error. So for this example, let's say for the sake of argument, that it was my 4th order polynomial, that had the lowest cross validation error. So in that case I'm going to pick this fourth order polynomial model. And finally, what this means is that that parameter d, remember d was the degree of polynomial, right? So d equals two, d equals three, all the way up to d equals 10. What we've done is we'll fit that parameter d and we'll say d equals four. And we did so using the cross-validation set. And so this degree of polynomial, so the parameter, is no longer fit to the test set, and so we've not saved away the test set, and we can use the test set to measure, or to estimate the generalization error of the model that was selected. By the of them. So, that was model selection and how you can take your data, split it into a training, validation, and test set. And use your cross validation data to select the model and evaluate it on the test set.One final note, I should say that in. The machine learning, as of this practice today, there aren't many people that will do that early thing that I talked about, and said that, you know, it isn't such a good idea, of selecting your model using this test set. And then using the same test set to report the error as though selecting your degree of polynomial on the test set, and then reporting the error on the test set as though that were a good estimate of generalization error. That sort of practice is unfortunately many, many people do do it. If you have a massive, massive test that is maybe not a terrible thing to do, but many practitioners, most practitioners that machine learnimg tend to advise against that. And it's considered better practice to have separate train validation and test sets. I just warned you to sometimes people to do, you know, use the same data for the purpose of the validation set, and for the purpose of the test set. You need a training set and a test set, and that's good, that's practice, though you will see some people do it. But, if possible, I would recommend against doing that yourself.
### Exam_in the video
![Exam_Understand cross validation](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/15-内置习题_理解交叉验证.jpg)
