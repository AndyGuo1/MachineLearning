# 应用机器学习算法建议-评估假设函数
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/06-Week6/1-Advice for Applying Machine Learning/2-Evaluating a Hypothesis.mp4" type="video/mp4">
</video>
## 中文
### 引出问题_通过绘图方式评估假设函数是否过拟合不太现实
![引出问题_通过绘图方式评估假设函数是否过拟合不太现实](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/4-引出问题_通过绘图方式评估假设函数是否过拟合不太现实.jpg)  
在本节视频中我想介绍一下 怎样用你学过的算法来评估假设函数 在之后的课程中 我们将以此为基础来 讨论如何避免 过拟合和欠拟合的问题 当我们确定学习算法的参数的时候 我们考虑的是选择参量来使训练误差最小化 有人认为 得到一个非常小的训练误差 一定是一件好事 但我们已经知道 仅仅是因为这个假设具有很小的训练误差 并不能说明它就一定是一个好的假设函数 而且我们也学习了过拟合假设函数的例子 所以这推广到新的训练集上是不适用的 那么 你该如何判断一个假设函数是过拟合的呢 对于这个简单的例子 我们可以 对假设函数 h(x) 进行画图 然后观察图形趋势 但对于特征变量不止一个的这种一般情况 还有像有很多特征变量的问题 想要通过画出假设函数来进行观察 就会变得很难甚至是不可能实现
### 评估假设函数是否过拟合的标准方法
![评估假设函数是否过拟合的标准方法](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/5-评估假设函数是否过拟合的标准方法.jpg)  
因此 我们需要另一种方法来评估我们的假设函数 如下给出了一种评估假设函数的标准方法 假设我们有这样一组数据组 在这里我只展示出10组训练样本 当然我们通常可以有 成百上千组训练样本 为了确保我们可以评估我们的假设函数 我们要做的是 将这些数据分成两部分 第一部分将成为我们的常用训练集 而第二部分将成为我们的测试集 将所有数据分成训练集和测试集 其中一种典型的分割方法是 按照7:3的比例 将70%的数据作为训练集 30%的数据作为测试集 因此 现在如果我们有了一些数据 我们只用其中的70% 作为我们的训练集 这里的m依然表示训练样本的总数 而剩下的那部分数据 将被用作测试集 在这里 我使用m下标test 来表示测试样本的总数 因此 这里的下标test将表示 这些样本是来自测试集 因此x(1)test y(1)test将成为我的 第一组测试样本 我想应该是这里的这一组样本 最后再提醒一点 在这里我是选择了前70%的数据作为训练集 后30%的数据作为测试集 但如果这组数据有某种规律或顺序的话 那么最好是 随机选择70%作为训练集 剩下的30%作为测试集 当然如果你的数据已经随机分布了 那你可以选择前70%和后30% 但如果你的数据不是随机排列的 最好还是打乱顺序 或者使用一种随机的顺序来构建你的数据 然后再取出前70%作为训练集 后30%作为测试集 接下来 这里展示了一种典型的方法 你可以按照这些步骤训练和测试你的学习算
### 举例_线性回归中评估假设函数是否过拟合
![举例_线性回归中评估假设函数是否过拟合](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/6-举例_线性回归中评估假设函数是否过拟合.jpg)  
比如线性回归算法 首先 你需要对训练集进行学习得到参数θ 具体来讲就是最小化训练误差J(θ) 这里的J(θ)是使用那70%数据 来定义得到的 也就是仅仅是训练数据 接下来 你要计算出测试误差 我将用J下标test来表示测试误差 那么你要做的就是 取出你之前从训练集中学习得到的参数θ放在这里 来计算你的测试误差 可以写成如下的形式 这实际上是测试集 平方误差的 平均值 这也不难想象 因此 我们使用包含参数θ的假设函数对每一个测试样本进行测试 然后通过假设函数和测试样本 计算出mtest个平方误差 当然 这是当我们使用线性回归 和平方误差标准时 测试误差的定义
### 举例_逻辑回归中评估假设函数是否过拟合
![举例_逻辑回归中评估假设函数是否过拟合](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/7-举例_逻辑回归中评估假设函数是否过拟合.jpg)  
那么如果是考虑分类问题 比如说使用逻辑回归的时候呢 训练和测试逻辑回归的步骤 与之前所说的非常类似 首先我们要从训练数据 也就是所有数据的70%中 学习得到参数θ 然后用如下的方式计算测试误差 目标函数和我们平常 做逻辑回归的一样 唯一的区别是 现在我们使用的是mtest个测试样本 这里的测试误差Jtest(θ) 其实不难理解 有时这是另一种形式的测试集 更易于理解 这里的误差其实叫误分类率 也被称为0/1错分率 0/1表示了 你预测到的正确或错误样本的情况 比如说 可以这样定义一次预测的误差 关于假设h(x) 和标签y的误差 那么这个误差等于1 当你的假设函数h(x)的值大于等于0.5 并且y的值等于0 或者当h(x)小于0.5 并且y的值等于1 因此 这两种情况都表明 你的假设对样本进行了误判 这里定义阈值为0.5 那么也就是说 假设结果更趋向于1 但实际是0 或者说假设更趋向于0 但实际的标签却是1 否则 我们将误差值定义为0 此时你的假设值能够正确对样本y进行分类 然后 我们就能应用错分率误差 来定义测试误差 也就是1/mtest 乘以 h(i)(xtest)和y(i)的错分率误差 从i=1到mtest 的求和 这样我就写出了我的定义方式 这实际上就是我的假设函数误标记的 那部分测试集中的样本 这也就是使用 0/1错分率或误分类率 的准则来定义的测试误差 以上我们介绍了一套标准技术 来评价一个已经学习过的假设 在下一段视频中我们要应用这些方法 来帮助我们进行诸如特征选择一类的问题 比如多项式次数的选择 或者正则化参数的选择
### 内置习题
![内置习题_理解评估假设函数是否过拟合](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/8-内置习题_理解评估假设函数是否过拟合的方法.jpg)
## English
### Raises a question_Plot cannot evaluate whether hypothesis function overfitting
![Raises a question_Plot cannot evaluate whether hypothesis function overfitting](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/4-引出问题_通过绘图方式评估假设函数是否过拟合不太现实.jpg)  
In this video, I would like to talk about how to evaluate a hypothesis that has been learned by your algorithm. In later videos, we will build on this to talk about how to prevent in the problems of overfitting and underfitting as well. When we fit the parameters of our learning algorithm we think about choosing the parameters to minimize the training error. One might think that getting a really low value of training error might be a good thing, but we have already seen that just because a hypothesis has low training error, that doesn't mean it is necessarily a good hypothesis. And we've already seen the example of how a hypothesis can overfit. And therefore fail to generalize the new examples not in the training set. So how do you tell if the hypothesis might be overfitting. In this simple example we could plot the hypothesis h of x and just see what was going on. But in general for problems with more features than just one feature, for problems with a large number of features like these it becomes hard or may be impossible to plot what the hypothesis looks like and so we need some other way to evaluate our hypothesis.
### The standard method of evaluate whether hypothesis function overfitting
![The standard method of Evaluate whether hypothesis function overfitting](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/5-评估假设函数是否过拟合的标准方法.jpg)  
we need some other way to evaluate our hypothesis. The standard way to evaluate a learned hypothesis is as follows. Suppose we have a data set like this. Here I have just shown 10 training examples, but of course usually we may have dozens or hundreds or maybe thousands of training examples. In order to make sure we can evaluate our hypothesis, what we are going to do is split the data we have into two portions. The first portion is going to be our usual training set and the second portion is going to be our test set, and a pretty typical split of this all the data we have into a training set and test set might be around say a 70%, 30% split. Worth more today to grade the training set and relatively less to the test set. And so now, if we have some data set, we run a sine of say 70% of the data to be our training set where here "m" is as usual our number of training examples and the remainder of our data might then be assigned to become our test set. And here, I'm going to use the notation m subscript test to denote the number of test examples. And so in general, this subscript test is going to denote examples that come from a test set so that x1 subscript test, y1 subscript test is my first test example which I guess in this example might be this example over here. Finally, one last detail whereas here I've drawn this as though the first 70% goes to the training set and the last 30% to the test set. If there is any sort of ordinary to the data. That should be better to send a random 70% of your data to the training set and a random 30% of your data to the test set. So if your data were already randomly sorted, you could just take the first 70% and last 30% that if your data were not randomly ordered, it would be better to randomly shuffle or to randomly reorder the examples in your training set. Before you know sending the first 70% in the training set and the last 30% of the test set.
### For example_linear regression to evaluate whether hypothesis function overfitting
![For example_linear regression to evaluate whether hypothesis function overfitting](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/6-举例_线性回归中评估假设函数是否过拟合.jpg)  
Here then is a fairly typical procedure for how you would train and test the learning algorithm and the learning regression. First, you learn the parameters theta from the training set so you minimize the usual training error objective j of theta, where j of theta here was defined using that 70% of all the data you have. There is only the training data. And then you would compute the test error. And I am going to denote the test error as j subscript test. And so what you do is take your parameter theta that you have learned from the training set, and plug it in here and compute your test set error. Which I am going to write as follows. So this is basically the average squared error as measured on your test set. It's pretty much what you'd expect. So if we run every test example through your hypothesis with parameter theta and just measure the squared error that your hypothesis has on your m subscript test, test examples. And of course, this is the definition of the test set error if we are using linear regression and using the squared error metric.
### For example_logistic regression to evaluate whether hypothesis function overfitting
![For example_logistic regression to evaluate whether hypothesis function overfitting](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/7-举例_逻辑回归中评估假设函数是否过拟合.jpg)  
How about if we were doing a classification problem and say using logistic regression instead. In that case, the procedure for training and testing say logistic regression is pretty similar first we will do the parameters from the training data, that first 70% of the data. And it will compute the test error as follows. It's the same objective function as we always use but we just logistic regression, except that now is define using our m subscript test, test examples. While this definition of the test set error j subscript test is perfectly reasonable. Sometimes there is an alternative test sets metric that might be easier to interpret, and that's the misclassification error. It's also called the zero one misclassification error, with zero one denoting that you either get an example right or you get an example wrong. Here's what I mean. Let me define the error of a prediction. That is h of x. And given the label y as equal to one if my hypothesis outputs the value greater than equal to five and Y is equal to zero or if my hypothesis outputs a value of less than 0.5 and y is equal to one, right, so both of these cases basic respond to if your hypothesis mislabeled the example assuming your threshold at an 0.5. So either thought it was more likely to be 1, but it was actually 0, or your hypothesis stored was more likely to be 0, but the label was actually 1. And otherwise, we define this error function to be zero. If your hypothesis basically classified the example y correctly. We could then define the test error, using the misclassification error metric to be one of the m tests of sum from i equals one to m subscript test of the error of h of x(i) test comma y(i). And so that's just my way of writing out that this is exactly the fraction of the examples in my test set that my hypothesis has mislabeled. And so that's the definition of the test set error using the misclassification error of the 0 1 misclassification metric. So that's the standard technique for evaluating how good a learned hypothesis is. In the next video, we will adapt these ideas to helping us do things like choose what features like the degree polynomial to use with the learning algorithm or choose the regularization parameter for learning algorithm.
### Exam_in the video
![Exam__Understand the method to evaluate whether hypothesis function overfitting](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/8-内置习题_理解评估假设函数是否过拟合的方法.jpg)
