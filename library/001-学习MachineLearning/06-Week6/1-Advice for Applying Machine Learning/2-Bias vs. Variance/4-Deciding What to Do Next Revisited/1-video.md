# 应用机器学习算法建议-回顾改进学习算法的几种方式并理解其使用场景
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/06-Week6/1-Advice for Applying Machine Learning/7-Deciding What to Do Next Revisited.mp4" type="video/mp4">
</video>
## 中文
### 回顾改进学习算法的几种方式深入理解其使用场景
![回顾改进学习算法的几种方式深入理解其使用场景](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/34-回顾改进学习算法的几种方式深入理解其使用场景.jpg)  
我们已经介绍了怎样评价一个学习算法 我们讨论了模型选择问题 偏差和方差的问题 那么这些诊断法则怎样帮助我们判断 哪些方法可能有助于 改进学习算法的效果 而哪些可能是徒劳的呢 让我们再次回到最开始的例子 在那里寻找答案 这就是我们之前的例子 我们使用正则化的线性回归拟合模型 却发现该算法没有达到预期效果 我们提到我们有如下这些选择 那么如何判断 哪些方法更可能是有效的呢 第一种可供选择的方法 是使用更多的训练集数据 这种方法对于高方差的情况 是有帮助的 也就是说 如果你的模型不处于高方差问题 而是处于高偏差的时候 那么通过前面的视频 我们已经知道 获取更多的训练集数据并不会有太明显的帮助 所以 要选择第一种方法 你应该先画出 学习曲线 然后看出你的模型 应该至少有那么一点方差问题 也就是说你的交叉验证集误差 应该比训练集误差大一点 第二种方法情况又如何呢 第二种方法是 少选几种特征 这同样是对高方差时有效 换句话说 如果你通过绘制学习曲线 或者别的什么方法 看出你的模型处于高偏差问题 那么切记 千万不要浪费时间 试图从已有的特征中 挑出一小部分来使用 因为你已经发现高偏差的问题了 使用更少的特征不会有任何帮助 反过来 如果你发现 从你的学习曲线 或者别的某种诊断图中 你看出了高方差的问题 那么恭喜你 花点时间挑选出一小部分合适的特征吧 这是把时间用在了刀刃上 方法三 选用更多的特征又如何呢 通常来讲 尽管不是所有时候都适用 但增加特征数 一般可以帮助解决高偏差问题 所以如果你需要增加 更多的特征时 一般是由于你现有的 假设函数太简单 因此我们才决定增加一些 别的特征来让假设函数 更好地拟合训练集 类似的 增加更多的多项式特征 这实际上也是属于增加特征 因此也是用于 修正高偏差问题 具体来说 如果你画出的学习曲线告诉你 你还是处于高方差问题 那么采取这种方法 就是浪费时间 最后 增大和减小λ 这种方法尝试起来很方便 我想 尝试这个方法 不至于花费你几个月时间 但我们已经知道 减小λ可以修正高偏差 如果我说的你还不清楚的话 我建议你暂停视频 仔细回忆一下 想明白 减小λ的值 为何有助于修正高偏差 而增大λ的值为何解决高方差 如果你确实不明白 其中的原因 那就暂停一下好好想想 直到真的弄清楚这个道理 或者看看 上一节视频最后 我们绘制的学习曲线 试着理解清楚 为什么是那样的
### 神经网络和过拟合
![神经网络和过拟合](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/35-神经网络和过拟合.jpg)  
最后 我们回顾一下 这几节课介绍的这些内容 并且看看它们和神经网络的联系 我想介绍一些 很实用的经验或建议 这些也是我平时为神经网络模型 选择结构或者连接形式的一些技巧 当你在进行神经网络拟合的时候 如果你要进行神经网络的拟合 比如说 一个相对比较简单的神经网络模型 相对来讲 它的隐藏单元比较少 甚至只有一个隐藏单元 如果你要进行神经网络的拟合 其中一个选择是 选用一个相对简单的网络结构 比如说只有一个 隐藏层 或者可能相对来讲 比较少的隐藏单元 因此像这样的一个简单的神经网络 参数就不会很多 很容易出现欠拟合 这种比较小型的神经网络 其最大优势在于计算量较小 与之相对的另一种情况 是相对较大型的神经网络结构 要么隐藏层单元比较多 比如这一层中的隐藏单元数就很多 要么隐藏层比较多 因此这种比较复杂的神经网络 参数一般较多 也更容易出现过拟合 这种结构的一大劣势 也许不是主要的 但还是需要考虑 那就是当网络中的 神经元数量很多的时候 这种结构会显得 计算量较大 虽然有这个情况 但通常来讲这不是大问题 这种大型网络结构最主要的问题 还是它更容易出现过拟合现象 事实上 如果你经常应用神经网络 特别是大型神经网络的话 你就会发现越大型的网络性能越好 但如果发生了过拟合 你可以使用正则化的方法 来修正过拟合 一般来说 使用一个大型的神经网络 并使用正则化来修正过拟合问题 通常比使用一个小型的神经网络 效果更好 但主要可能出现的问题 是计算量相对较大 最后 你还需要选择 隐藏层的层数 你是应该用一个 隐藏层呢 还是应该用三个呢 就像我们这里画的 或者还是用两个隐藏层呢 通常来说 正如我在前面的视频中讲过的 默认的情况是 使用一个隐藏层 但是如果你确实想要选择 多个隐藏层 你也可以试试 把数据分割为训练集 验证集 和测试集 然后使用交叉验证的方法 比较一个隐藏层的神经网络 然后试试两个 三个隐藏层 以此类推 然后看看哪个神经网络 在交叉验证集上表现得最理想 也就是说 你得到了三个神经网络模型 分别有一个 两个 三个隐藏层 然后你对每一个模型 都用交叉验证集数据进行测试 算出三种情况下的 交叉验证集误差Jcv 然后选出你认为最好的 神经网络结构 好了 这就是 偏差和方差问题 以及诊断该问题的学习曲线方法 在改进学习算法的表现时 你可以充分运用 以上这些内容来判断 哪些途径可能是有帮助的 而哪些方法可能是无意义的 如果你理解了以上几节视频中 介绍的内容 并且懂得如何运用 那么你已经可以使用机器学习方法有效的解决实际问题了 你也能像硅谷的 大部分机器学习从业者一样 他们每天的工作就是 使用这些学习算法 来解决众多实际问题 我希望这几节中 提到的一些技巧 关于方差 偏差 以及学习曲线为代表的诊断法 能够真正帮助你更有效率地 应用机器学习 让它们高效地工作
### 内置习题
![内置习题_理解高偏差和高方差下的学习曲线](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/36-内置习题_理解神经网络和过拟合.jpg)  
## English
### Review several ways of improve learning algorithm and understanding the usage scenario
![Review several ways of improve learning algorithm and understanding the usage scenario](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/34-回顾改进学习算法的几种方式深入理解其使用场景.jpg)  
We've talked about how to evaluate learning algorithms, talked about model selection, talked a lot about bias and variance. So how does this help us figure out what are potentially fruitful, potentially not fruitful things to try to do to improve the performance of a learning algorithm.Let's go back to our original motivating example and go for the result.So here is our earlier example of maybe having fit regularized linear regression and finding that it doesn't work as well as we're hoping. We said that we had this menu of options. So is there some way to figure out which of these might be fruitful options? The first thing all of this was getting more training examples. What this is good for, is this helps to fix high variance.And concretely, if you instead have a high bias problem and don't have any variance problem, then we saw in the previous video that getting more training examples,while maybe just isn't going to help much at all. So the first option is useful only if you, say, plot the learning curves and figure out that you have at least a bit of a variance, meaning that the cross-validation error is, you know, quite a bit bigger than your training set error. How about trying a smaller set of features? Well, trying a smaller set of features, that's again something that fixes high variance.And in other words, if you figure out, by looking at learning curves or something else that you used, that have a high bias problem; then for goodness sakes, don't waste your time trying to carefully select out a smaller set of features to use. Because if you have a high bias problem, using fewer features is not going to help. Whereas in contrast, if you look at the learning curves or something else you figure out that you have a high variance problem, then, indeed trying to select out a smaller set of features, that might indeed be a very good use of your time. How about trying to get additional features, adding features, usually, not always, but usually we think of this as a solution for fixing high bias problems. So if you are adding extra features it's usually because your current hypothesis is too simple, and so we want to try to get additional features to make our hypothesis better able to fit the training set. And similarly, adding polynomial features; this is another way of adding features and so there is another way to try to fix the high bias problem.And, if concretely if your learning curves show you that you still have a high variance problem, then, you know, again this is maybe a less good use of your time.
And finally, decreasing and increasing lambda. This are quick and easy to try, I guess these are less likely to be a waste of, you know, many months of your life. But decreasing lambda, you already know fixes high bias.In case this isn't clear to you, you know, I do encourage you to pause the video and think through this that convince yourself that decreasing lambda helps fix high bias, whereas increasing lambda fixes high variance.And if you aren't sure why this is the case, do pause the video and make sure you can convince yourself that this is the case. Or take a look at the curves that we were plotting at the end of the previous video and try to make sure you understand why these are the case.
### Neural networks and overfitting
![Neural networks and overfitting](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/35-神经网络和过拟合.jpg)  
Finally, let us take everything we have learned and relate it back to neural networks and so, here is some practical advice for how I usually choose the architecture or the connectivity pattern of the neural networks I use.So, if you are fitting a neural network, one option would be to fit, say, a pretty small neural network with you know, relatively few hidden units, maybe just one hidden unit. If you're fitting a neural network, one option would be to fit a relatively small neural network with, say,relatively few, maybe only one hidden layer and maybe only a relatively few number of hidden units. So, a network like this might have relatively few parameters and be more prone to underfitting.The main advantage of these small neural networks is that the computation will be cheaper.An alternative would be to fit a, maybe relatively large neural network with either more hidden units--there's a lot of hidden in one there--or with more hidden layers.And so these neural networks tend to have more parameters and therefore be more prone to overfitting.One disadvantage, often not a major one but something to think about, is that if you have a large number of neurons in your network, then it can be more computationally expensive.Although within reason, this is often hopefully not a huge problem.The main potential problem of these much larger neural networks is that it could be more prone to overfitting and it turns out if you're applying neural network very often using a large neural network often it's actually the larger, the better but if it's overfitting, you can then use regularization to address overfitting, usually using a larger neural network by using regularization to address is overfitting that's often more effective than using a smaller neural network. And the main possible disadvantage is that it can be more computationally expensive.And finally, one of the other decisions is, say, the number of hidden layers you want to have, right? So, do you want one hidden layer or do you want three hidden layers, as we've shown here, or do you want two hidden layers?And usually, as I think I said in the previous video, using a single hidden layer is a reasonable default, but if you want to choose the number of hidden layers, one other thing you can try is find yourself a training cross-validation, and test set split and try training neural networks with one hidden layer or two hidden layers or three hidden layers and see which of those neural networks performs best on the cross-validation sets. You take your three neural networks with one, two and three hidden layers, and compute the cross validation error at Jcv and all of them and use that to select which of these is you think the best neural network.So, that's it for bias and variance and ways like learning curves, who tried to diagnose these problems. As far as what you think is implied, for one might be truthful or not truthful things to try to improve the performance of a learning algorithm.If you understood the contents of the last few videos and if you apply them you actually be much more effective already and getting learning algorithms to work on problems and even a large fraction, maybe the majority of practitioners of machine learning here in Silicon Valley today doing these things as their full-time jobs.So I hope that these pieces of advice on by experience in diagnostics will help you to much effectively and powerfully apply learning and get them to work very well.
### Exam_in the video
![Exam_Understand Neural networks and overfitting](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/36-内置习题_理解神经网络和过拟合.jpg)  
