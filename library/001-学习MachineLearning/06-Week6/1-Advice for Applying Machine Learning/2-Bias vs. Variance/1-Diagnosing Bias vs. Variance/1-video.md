# 应用机器学习算法建议-高偏差【欠拟合】vs.高方差【过拟合】
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/06-Week6/1-Advice for Applying Machine Learning/4-Diagnosing Bias vs. Variance.mp4" type="video/mp4">
</video>
## 中文
### 高偏差和高方差下的假设函数示意图
![高偏差和高方差下的假设函数示意图](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/17-高偏差和高方差下的假设函数示意图.jpg)  
当你运行一个学习算法时 如果这个算法的表现不理想 那么多半是出现 两种情况 要么是偏差比较大 要么是方差比较大 换句话说 出现的情况要么是欠拟合 要么是过拟合问题 那么这两种情况 哪个和偏差有关 哪个和方差有关 或者是不是和两个都有关 搞清楚这一点非常重要 因为能判断出现的情况 是这两种情况中的哪一种 其实是一个很有效的指示器 指引着可以改进算法的 最有效的方法和途径 在这段视频中 我想更深入地探讨一下 有关偏差和方差的问题 希望你能对它们有一个更深入的理解 并且也能弄清楚怎样评价一个学习算法 能够判断一个算法是偏差还是方差有问题 因为这个问题对于弄清 如何改进学习算法的效果非常重要 好的 这几幅图 你已经见过很多次了 如果你用两个很简单的假设来拟合数据 比如说用一条直线 那么不足以拟合这组数据(欠拟合) 而如果你用两个很复杂的假设来拟合时 那么对训练集来说 则会拟合得很好 但又过于完美(过拟合) 而像这样的 中等复杂度的假设 比如某种二次多项式的假设 次数既不高也不低 这种假设对数据拟合得刚刚好 此时对应的的泛化误差 也是三种情况中最小的
### 高偏差和高方差下的训练集和验证集误差示意图
![高偏差和高方差下的训练集和验证集误差示意图](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/18-高偏差和高方差下的训练集和验证集误差示意图.jpg)  
现在我们已经掌握了 训练集 验证集和测试集的概念 我们就能更好地理解 偏差和方差的问题 具体来说 我们沿用之前所使用的 训练集误差和验证集 误差的定义 也就是平方误差 即对训练集数据进行预测 或对验证集数据进行预测 所产生的平均平方误差 下面我们来画出如下这个示意图 横坐标上表示的是 多项式的次数 因此横坐标越往右的位置表示多项式的次数越大 那么我们来画这幅图对应的情况 d可能等于1的情况 是用很简单的函数 来进行拟合 而在右边的这个图中 水平横坐标表示 有更多更大的d值 表示更高次数的多项式 因此这些位置对应着使用 更复杂的函数来拟合你的训练集时所需要的d值 让我们来把训练集误差 和交叉验证集误差画在这个坐标中 我们先来画训练集误差 随着我们增大多项式的次数 我们将对训练集拟合得越来越好 所以如果d等于1时 对应着一个比较大的训练误差 而如果我们的多项式次数很高时 我们的训练误差就会很小 甚至可能等于0 因为可能非常拟合训练集 所以 当我们增大多项式次数时 我们不难发现 训练误差明显下降 这里我写上J下标train 来表示训练集误差 因为随着我们对数据拟合 所需多项式次数的增大 训练误差是趋于下降的 接下来我们再看交叉验证误差 事实上如果我们观察测试集误差的话 我们会得到一个和交叉验证误差 非常接近的结果 所以 我们知道 如果d等于1的话 意味着用一个很简单的函数来拟合数据 此时我们不能很好地拟合训练集(欠拟合) 也就是说 我们会得到 一个较大的交叉验证误差 而如果我们用一个中等大小的 多项式次数来拟合时 在前一张幻灯片中 我们用的d等于2 那么我们会得到一个 更小的交叉验证误差 因为我们找了一个能够更好拟合数据的次数 同样地 反过来 如果次数d太大 比如说d的值取为4 那么我们又过拟合了 我们又会得到一个较大的交叉验证误差 因此 如果你平稳地过渡这几个点 你可以绘制出一条平滑的曲线 就像这样 我用Jcv(θ)来表示 同样地 如果你画出Jtest(θ) 你也将得到一条类似的曲线
### 高偏差和高方差下的训练集和验证集误差图示分析
![高偏差和高方差下的训练集和验证集误差图示分析](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/19-高偏差和高方差下的训练集和验证集误差图示分析.jpg)  
这样一幅图同时也帮助我们 更好地理解偏差和方差的概念 具体来说 假设你得出了一个学习算法 而这个算法并没有表现地 如你期望那么好 所以你的交叉验证误差或者测试集误差都很大 我们应该如何判断 此时的学习算法 正处于高偏差的问题还是高方差的问题 交叉验证误差比较大的情况 对应着曲线中的这一端 或者这一端 那么左边的这一端 对应的就是高偏差的问题 也就是你使用了一个 过于小的多项式次数 比如d等于1 但实际上我们需要一个较高的多项式次数来拟合数据 相反地 右边这一端对应的是高方差问题 也就是说 多项式次数d 对于我们的数据来讲太大了 这幅图也提示了我们 怎样区分这两种情况 具体地说 对于高偏差的情况 也就是对应欠拟合的情况 我们发现交叉验证误差和训练误差 都会很大 因此 如果你的算法有偏差问题的话 那么训练集误差 将会比较大 同时你可能会发现 交叉验证集误差也很大 两个误差可能很接近 或者可能验证误差稍大一点 所以如果你看到这样的组合情况 那就表示你的算法 正处于高偏差的问题 反过来 如果你的算法处于高方差的问题 那么如果你观察这里 我们会发现 Jtrain就是训练误差 会很小 也就意味着 你对训练集数据拟合得非常好 而你的交叉验证误差 假设此时我们最小化的 是平方误差 而反过来 你的交叉验证集误差或者说你的交叉验证集对应的代价函数的值 将会远远大于训练集误差 这里的双大于符号 是一个数学符号 表示远远大于 用两个大于符号表示 因此如果你看见这种组合的情况 这就预示着你的学习算法可能正处于 高方差和过拟合的情况 同时 区分这两种不同情形 的关键依据是 如果你的算法处于高偏差的情况 那么你的训练集误差会很大 因为你的假设不能很好地拟合训练集数据 而当你处于高方差的问题时 你的训练误差 通常都会很小 并且远远小于交叉验证误差 好的 但愿这节课能让你 更清楚地理解 偏差和方差这两种问题 在之后几段视频中 我还将对偏差和误差做更多的解释 但我们之后要关注的 是诊断一个学习算法 是处于高偏差还是高方差的情况 在后面几段视频中我还将向你展示更多细节我们将会看到 通过分清一个学习算法是处于高偏差 还是高方差 还是两种情况的结合 这能够更好地指引我们 应该采取什么样的措施来提高学习算法的性能表现
### 内置习题
![内置习题_理解高偏差【欠拟合】和高方差【过拟合】](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/20-内置习题_理解高偏差【欠拟合】和高方差【过拟合】.jpg)
## English
### Under the high bias and variance of hypothesis function diagram
![Under the high bias and variance of hypothesis function diagram](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/17-高偏差和高方差下的假设函数示意图.jpg)  
If you run the learning algorithm and it doesn't do as well as you are hoping, almost all the time it will be because you have either a high bias problem or a high variance problem. In other words they're either an underfitting problem or an overfitting problem.And in this case it's very important to figure out which of these two problems is bias or variance or a bit of both that you actually have. Because knowing which of these two things is happening would give a very strong indicator for whether the useful and promising ways to try to improve your algorithm.In this video, I would like to delve more deeply into this bias and various issue and understand them better as well as figure out how to look at and evaluate knows whether or not we might have a bias problem or a variance problem. Since this would be critical to figuring out how to improve the performance of learning algorithm that you implement. So you've already seen this figure a few times, where if you fit two simple hypothesis, like a straight line that that underfits the data.If you fit a two complex hypothesis, then that might fit the training set perfectly but overfit the data and this may be hypothesis of some intermediate level of complexity, of some, maybe degree two polynomials are not too low and not too high degree. That's just right. And gives you the best generalization error out of these options.
### High bias and variance of the training set and validation set error
![High bias and variance of the training set and validation set error](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/18-高偏差和高方差下的训练集和验证集误差示意图.jpg)  
Now that we're armed with the notion of training and validation in test sets, we can understand the concepts of bias and variance a little bit better. Concretely, let our training error and cross validation error be defined as in the previous videos, just say, the squared error, the average squared error as measured on the 20 sets or as measured on the cross validation set.Now let's plot the following figure. On the horizontal axis I am going to plot the degree of polynomial, so as I go the right I'm going to be fitting higher and higher order polynomials.So, we'll do that for this figure, where maybe d equals 1, were going to be fitting very simple functions where as we are the right of this this may be d equals 4 or relatively may be even larger numbers. I'm going to be fitting very complex high order polynomials that might fit the training set with much more complex functions whereas we're here on the right of the horizontal axis, I have much larger values of these of a much higher degree polynomial, and so here that is going to correspond to fitting much more complex functions to your training set. Let's look at the training error and cause-validation error and plot them on this figure. Let's start with the training error. As we increase the degree of the polynomial, we're going to fit our training set better and better and so, if d equals 1 that ever rose to the high training error. If we have a very high degree of polynomial, our training error is going to be really low. Maybe even zero, because it will fit the training set really well. And so as we increase of the greater polynomial we find typically that the training error decreases, so I'm going to write j subscript train of theta there, because our training error tends to decrease with the degree of the polynomial that we fit to the data. Next, let's look at the cross validation error. Often that matter, if we look at the test set error we'll get a pretty similar result as if we were to plot the cross validation error. So, we know that if d equals 1, we're fitting a very simple function, and so we may be underfitting the training set, and so we're going to go very high cross-validation error. If we fit, you know, an intermediate degree polynomial; we have a d equals 2 in our example in the previous slide, we are going to have a much lower cross-validation error, because we are just fitting, finding a much better fit to the data.
### High bias and variance of the training set and validation set error analysis
![High bias and variance of the training set and validation set error analysis](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/19-高偏差和高方差下的训练集和验证集误差图示分析.jpg)  
And conversely if d were too high, so if d took on say a value of four, then we're again overfitting and so we end up with a high value for cross-validation error.So if you were to vary this smoothly and plot a curve you might end up with a curve like that, where that's Jcv of theta, and again if you plot j test of theta you get something very similar.And so this sort of plot also helps us to better understand the notions of bias and variance. Concretely, if you have a learning algorithm that's not performing as well as you wanted it to, how can you figure out if your learning algorithm is suffering.Concretly, suppose you have applied a learning algorithm and it is not performing as well as your are hoping, so your cross-validation set error or your test set error is high.How can we figure out if the learning algorithm is suffering from high bias or if it is suffering from high variance.So the setting of a cross-validation error being high corresponds to either this regime or this regime.So this regime on the left corresponds to a high bias problem, that is, if you are fitting an overly low order polynomial such as a plus one, when we really needed a higher order polynomial to fit the data. Whereas in contrast, this regime corresponds to a high variance problem. That is, if d--the degree of polynomial--was too large for the data set that we have. And this figure gives us a clue for how to distinguish between these two cases.Concretely, for the high bias case, that is, the case of under fitting, what we find is that both the cross validation error and the training error are going to be high. So, if your algorithm is suffering from a bias problem,the training set error would be high and you may find that the cross validation error will also be high. It might be close, maybe just slightly higher then a training error. And so, if you see this combination,that's a sign that your algorithm may be suffering from high bias.In contrast; if your algorithm is suffering from high variance; then, if you look here, we'll notice that, J train, that is the training error, is going to be low.That is, you're fitting the training set very well.Whereas, your cross validation error, assuming that this say the squared error which we're trying to minimize. Whereas in contrast; your error on a cross validation set or your cross function like cross validation set, will be much bigger than your training set error.This double greater than sign, here, it means much bigger than, all right. So, it's much greater than to multiply great to great.So this is a double greater than sign, that is the map symbol for much greater than denoted by two greater than signs.And so if you see this combination, then what you find. And so if you see this combination of values, then that is a clue that your learning algorithm may be suffering from high variance and might be overfitting.And the key that distinguishes these two cases is if you have a high bias problem your training set error will also be high as your hypothesis just not fitting the training set well.And if you have a high variance problem, your training set error will usually be low, that is much lower than the cross validation error.So, hopefully that gives you a somewhat better understanding of the two problems of bias and variance. I still have a lot more to say about bias and variance in the next few videos. But what we will see later; is that by diagnosing, whether a learning algorithm may be suffering from high bias or a high variance. I'll show you even more details on how to do that in later videos. We'll see that by figuring out whether a learning algorithm may be suffering from high bias or a combination of both that that would give us much better guidance for what might be promising things to try in order to improve the performance of the learning algorithm.
### Exam_in the video
![Exam_Understandthe high bias【underfitting】 and variance 【overfitting 】](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/20-内置习题_理解高偏差【欠拟合】和高方差【过拟合】.jpg)  
