# 应用机器学习算法建议-学习曲线
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/06-Week6/1-Advice for Applying Machine Learning/6-Learning Curves.mp4" type="video/mp4">
</video>
## 中文
### 学习曲线示意图
![学习曲线示意图](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/29-学习曲线示意图.jpg)  
本节课我们介绍学习曲线 绘制学习曲线非常有用 也许你想检查你的学习算法 运行是否一切正常 或者你希望改进算法的表现或效果 那么学习曲线 就是一种很好的工具 我经常使用学习曲线 来判断某一个学习算法 是否处于偏差 方差问题 或是二者皆有 下面我们就来介绍学习曲线 为了绘制一条学习曲线 我通常先绘制出Jtrain 也就是训练集数据的平均误差平方和 或者Jcv 也即交叉验证集数据的 平均误差平方和 我要将其绘制成一个 关于参数m的函数 也就是一个关于训练集 样本总数的函数 所以m一般都是一个常数 比如m等于100 表示100组训练样本 但我要自己取一些m的值 也就是说我要自行对m的取值 做一点限制 比如说我取10 20或者 30 40组训练集 然后绘出训练集误差 以及交叉验证集误差 好的 那么我们来看看 这条曲线绘制出来是什么样子 假设我只有一组训练样本 也即m=1 正如第一幅图中所示 并且假设使用二次函数来拟合模型 那么由于我只有一个训练样本 拟合的结果很明显会很好 是吧 用二次函数来拟合 对这一个训练样本拟合 其误差一定为0 如果有两组训练样本 二次函数也能很好地拟合 即使是使用正则化 拟合的结果也会很好 而如果不使用正则化的话 那么拟合效果绝对棒极了 如果我用三组训练样本的话 好吧 看起来依然能很好地 用二次函数拟合 也就是说 当m等于1 m=2 或m=3时 对训练集数据进行预测 得到的训练集误差 都将等于0 这里假设我不使用正则化 当然如果使用正则化 那么误差就稍大于0 顺便提醒一下 如果我的训练集样本很大 而我要人为地限制训练集 样本的容量 比如说这里 我将m值设为3 然后我仅用这三组样本进行训练 然后对应到这个图中 我只看对这三组训练样本 进行预测得到的训练误差 也是和我模型拟合的三组样本 所以即使我有100组训练样本 而我还是想绘制 当m等于3时的训练误差 那么我要关注的仍然是 对这三组训练样本进行预测的误差 同样 这三组样本也是我们用来拟合模型的三组样本 所有其他的样本 我都在训练过程中选择性忽略了 好的 总结一下 我们现在已经看到 当训练样本容量m很小的时候 训练误差也会很小 因为很显然 如果我们训练集很小 那么很容易就能把 训练集拟合到很好 甚至拟合得天衣无缝 现在我们来看 当m等于4的时候 好吧 二次函数似乎也能 对数据拟合得很好 那我们再看 当m等于5的情况 这时候再用二次函数来拟合 好像效果有下降但还是差强人意 而当我的训练集越来越大的时候 你不难发现 要保证使用二次函数 的拟合效果依然很好 就显得越来越困难了 因此 事实上随着训练集容量的增大 我们不难发现 我们的平均训练误差 是逐渐增大的 因此如果你画出这条曲线 你就会发现 训练集误差 也就是 对假设进行预测的误差平均值 随着m的增大而增大 再重复一遍对这一问题的理解 当训练样本很少的时候 对每一个训练样本 都能很容易地拟合到很好 所以训练误差将会很小 而反过来 当m的值逐渐增大 那么想对每一个训练样本都拟合到很好 就显得愈发的困难了 因此训练集误差就会越来越大 那么交叉验证集误差的情况如何呢 好的 交叉验证集误差 是对完全陌生的交叉验证集数据 进行预测得到的误差 那么我们知道 当训练集很小的时候 泛化程度不会很好 意思是不能很好地适应新样本 因此这个假设 就不是一个理想的假设 只有当我使用 一个更大的训练集时 我才有可能 得到一个能够更好拟合数据的 可能的假设 因此 你的验证集误差和 测试集误差 都会随着训练集样本容量m的增加 而减小 因为你使用的数据越多 你越能获得更好地泛化表现 或者说对新样本的适应能力更强 因此 数据越多 越能拟合出合适的假设 所以 如果你把Jtrain和Jcv绘制出来 就应该得到这样的曲线
### 高偏差【欠拟合】下的学习曲线分析
![高偏差【欠拟合】下的学习曲线分析](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/30-高偏差【欠拟合】下的学习曲线分析.jpg)  
现在我们来看看 当处于高偏差或者高方差的情况时 这些学习曲线 又会变成什么样子 假如你的假设处于高偏差问题 为了更清楚地解释这个问题 我要用一个简单的例子来说明 也就是用一条直线 来拟合数据的例子 很显然一条直线不能很好地拟合数据 所以最后得到的假设很有可能是这样的 现在我们来想一想 如果我们增大训练集样本容量 会发生什么情况呢 所以现在不像画出的这样 只有这五组样本了 我们有了更多的训练样本 那么如果你用一条直线来拟合 不难发现 还是会得到类似的一条直线假设 我的意思是 刚才的情况用一条直线不能很好地拟合 而现在把样本容量扩大了 这条直线也基本不会变化太大 因为这条直线是对这组数据 最可能也是最接近的拟合 但一条直线再怎么接近 也不可能对这组数据进行很好的拟合 所以 如果你绘出交叉验证集误差 应该是这样子的 最左端表示训练集样本容量很小 比如说只有一组样本 那么表现当然很不好 而随着你增大训练集样本数 当达到某一个容量值的时候 你就会找到那条最有可能 拟合数据的那条直线 并且此时即便 你继续增大训练集的 样本容量 即使你不断增大m的值 你基本上还是会得到的一条差不多的直线 因此 交叉验证集误差 我把它标在这里 或者测试集误差 将会很快变为水平而不再变化 只要训练集样本容量值达到 或超过了那个特定的数值 交叉验证集误差和测试集误差就趋于不变 这样你会得到最能拟合数据的那条直线 那么训练误差又如何呢 同样 训练误差一开始也是很小的 而在高偏差的情形中 你会发现训练集误差 会逐渐增大 一直趋于接近 交叉验证集误差 这是因为你的参数很少 但当m很大的时候 数据太多 此时训练集和交叉验证集的 预测效果将会非常接近 这就是当你的学习算法处于 高偏差情形时 学习曲线的大致走向 最后补充一点 高偏差的情形 反映出的问题是 交叉验证集和训练集 误差都很大 也就是说 你最终会得到一个 值比较大Jcv 和Jtrain 这也得出一个很有意思的结论 那就是 如果一个学习算法 有很大的偏差 那么当我们选用更多的训练样本时 也就是在这幅图中 随着我们增大横坐标 我们发现交叉验证集误差的值 不会表现出明显的下降 实际上是变为水平了 所以如果学习算法 正处于高偏差的情形 那么选用更多的训练集数据 对于改善算法表现无益 正如我们右边的 这两幅图所体现的 这里我们只有五组训练样本 然后我们找到这条直线来拟合 然后我们增加了更多的训练样本 但我们仍然得到几乎一样的 一条直线 因此如果学习算法 处于高偏差时 给我再多的训练数据也于事无补 交叉验证集误差或测试集误差 也不会降低多少 所以 能够看清你的算法正处于 高偏差的情形 是一件很有意义的事情 因为这样可以让你避免 把时间浪费在 想收集更多的训练样本 因为再多的数据也是无意义的
### 高方差【过拟合】下学习曲线分析
![高方差【过拟合】下学习曲线分析](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/31-高方差【过拟合】下学习曲线分析.jpg)  
接下来我们再来看看 当学习算法正处于高方差的时候 学习曲线应该是什么样子的 首先我们来看 训练集误差 如果你的训练集样本容量很小 比如像图中所示情形 只有五组训练样本 如果我们用很高阶次的 多项式来拟合 比如这里我用了100次的多项式函数 当然不会有人这么用的 这里只是演示 并且假设我们使用 一个很小的lambda值 可能不等于0 但足够小的lambda 那么很显然 我们会对这组数据 拟合得非常非常好 因此这个假设函数对数据过拟合 所以 如果训练集 样本容量很小时 训练集误差Jtrain 将会很小 随着训练集样本容量的增加 可能这个假设函数仍然会 对数据或多或少 有一点过拟合 但很明显此时要对数据很好地拟合 显得更加困难和吃力了 所以 随着训练集样本容量的增大 我们会发现Jtrain的值 会随之增大 因为当训练样本越多的时候 我们就越难跟训练集数据拟合得很好 但总的来说训练集误差还是很小 交叉验证集误差又如何呢 好的 在高方差的情形中 假设函数对数据过拟合 因此交叉验证集误差 将会一直都很大 即便我们选择一个 比较合适恰当的 训练集样本数 因此交叉验证集误差 画出来差不多是这样的 所以算法处于高方差情形 最明显的一个特点是 在训练集误差 和交叉验证集误差之间 有一段很大的差距 而这个曲线图也反映出 如果我们要考虑增大训练集的样本数 也就是在这幅图中 向右延伸曲线 我们大致可以看出 这两条学习曲线 蓝色和红色的两条曲线 正在相互靠近 因此 如果我们将曲线 向右延伸出去 那么似乎 训练集误差很可能会 逐渐增大 而交叉验证集误差 则会持续下降 当然我们最关心的还是交叉验证集误差 或者测试集误差 对吧 所以从这幅图中 我们基本可以预测 如果继续增大训练样本的数量 将曲线向右延伸 交叉验证集误差将会 逐渐下降 所以 在高方差的情形中 使用更多的训练集数据 对改进算法的表现 事实上是有效果的 这同样也体现出 知道你的算法正处于 高方差的情形 也是非常有意义的 因为它能告诉你 是否有必要花时间 来增加更多的训练集数据 好的 在前一页和这一页幻灯片中 我画出的学习曲线 都是相当理想化的曲线 针对一个实际的学习算法 如果你画出学习曲线的话 你会看到基本类似的结果 就像我在这里画的一样 虽然如此 有时候你也会看到 带有一点噪声或干扰的曲线 但总的来说 像这样画出学习曲线 确实能帮助你 看清你的学习算法 是否处于高偏差 高方差 或二者皆有的情形 所以当我打算 改进一个学习算法 的表现时 我通常会进行的一项工作 就是画出这些学习曲线 一般来讲 这项工作会让你 更轻松地看出偏差或方差的问题 在下一节视频中 我们将介绍如何判断 是否应采取具体的某个行为 来改进学习算法的表现
### 内置习题
![内置习题_理解高偏差和高方差下的学习曲线](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/32-内置习题_理解高偏差和高方差下的学习曲线.jpg)  
## English
### Learning curves diagram
![Learning curves diagram](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/29-学习曲线示意图.jpg)  
In this video, I'd like to tell you about learning curves.Learning curves is often a very useful thing to plot. If either you wanted to sanity check that your algorithm is working correctly, or if you want to improve the performance of the algorithm.And learning curves is a tool that I actually use very often to try to diagnose if a physical learning algorithm may be suffering from bias, sort of variance problem or a bit of both.Here's what a learning curve is. To plot a learning curve, what I usually do is plot j train which is, say,average squared error on my training set or Jcv which is the average squared error on my cross validation set. And I'm going to plot that as a function of m, that is as a function of the number of training examples I have. And so m is usually a constant like maybe I just have, you know, a 100 training examples but what I'm going to do is artificially with use my training set exercise. So, I deliberately limit myself to using only, say, 10 or 20 or 30 or 40 training examples and plot what the training error is and what the cross validation is for this smallest training set exercises. So let's see what these plots may look like. Suppose I have only one training example like that shown in this this first example here and let's say I'm fitting a quadratic function. Well, I have only one training example. I'm going to be able to fit it perfectly right? You know, just fit the quadratic function. I'm going to have 0 error on the one training example. If I have two training examples. Well the quadratic function can also fit that very well. So,even if I am using regularization, I can probably fit this quite well. And if I am using no neural regularization, I'm going to fit this perfectly and if I have three training examples again. Yeah, I can fit a quadratic function perfectly so if m equals 1 or m equals 2 or m equals 3,my training error on my training set is going to be 0 assuming I'm not using regularization or it may slightly large in 0 if I'm using regularization and by the way if I have a large training set and I'm artificially restricting the size of my training set in order to J train. Here if I set M equals 3, say, and I train on only three examples, then, for this figure I am going to measure my training error only on the three examples that actually fit my data too and so even I have to say a 100 training examples but if I want to plot what my training error is the m equals 3. What I'm going to do is to measure the training error on the three examples that I've actually fit to my hypothesis 2.And not all the other examples that I have deliberately omitted from the training process. So just to summarize what we've seen is that if the training set size is small then the training error is going to be small as well. Because you know, we have a small training set is going to be very easy to fit your training set very well may be even perfectly now say we have m equals 4 for example. Well then a quadratic function can be a longer fit this data set perfectly and if I have m equals 5 then you know, maybe quadratic function will fit to stay there so so, then as my training set gets larger.It becomes harder and harder to ensure that I can find the quadratic function that process through all my examples perfectly. So in fact as the training set size grows what you find is that my average training error actually increases and so if you plot this figure what you find is that the training set error that is the average error on your hypothesis grows as m grows and just to repeat when the intuition is that when m is small when you have very few training examples. It's pretty easy to fit every single one of your training examples perfectly and so your error is going to be small whereas when m is larger then gets harder all the training examples perfectly and so your training set error becomes more larger now, how about the cross validation error. Well, the cross validation is my error on this cross validation set that I haven't seen and so, you know, when I have a very small training set, I'm not going to generalize well, just not going to do well on that. So, right, this hypothesis here doesn't look like a good one, and it's only when I get a larger training set that, you know, I'm starting to get hypotheses that maybe fit the data somewhat better. So your cross validation error and your test set error will tend to decrease as your training set size increases because the more data you have, the better you do at generalizing to new examples. So, just the more data you have, the better the hypothesis you fit. So if you plot j train, and Jcv this is the sort of thing that you get.
### High bias 【underfitting】 of the learning curve analysis
![High bias 【underfitting】 of the learning curve analysis](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/30-高偏差【欠拟合】下的学习曲线分析.jpg)  
Now let's look at what the learning curves may look like if we have either high bias or high variance problems. Suppose your hypothesis has high bias and to explain this I'm going to use a, set an example, of fitting a straight line to data that, you know, can't really be fit well by a straight line.So we end up with a hypotheses that maybe looks like that.Now let's think what would happen if we were to increase the training set size. So if instead of five examples like what I've drawn there, imagine that we have a lot more training examples.Well what happens, if you fit a straight line to this. What you find is that, you end up with you know, pretty much the same straight line. I mean a straight line that just cannot fit this data and getting a ton more data, well the straight line isn't going to change that much. This is the best possible straight-line fit to this data, but the straight line just can't fit this data set that well. So, if you plot across validation error,this is what it will look like.Option on the left, if you have already a miniscule training set size like you know, maybe just one training example and is not going to do well. But by the time you have reached a certain number of training examples, you have almost fit the best possible straight line, and even if you end up with a much larger training set size, a much larger value of m, you know, you're basically getting the same straight line, and so, the cross-validation error - let me label that - or test set error or plateau out, or flatten out pretty soon, once you reached beyond a certain the number of training examples, unless you pretty much fit the best possible straight line. And how about training error? Well, the training error will again be small.And what you find in the high bias case is that the training error will end up close to the cross validation error, because you have so few parameters and so much data, at least when m is large. The performance on the training set and the cross validation set will be very similar.And so, this is what your learning curves will look like, if you have an algorithm that has high bias.And finally, the problem with high bias is reflected in the fact that both the cross validation error and the training error are high, and so you end up with a relatively high value of both Jcv and the j train.This also implies something very interesting, which is that, if a learning algorithm has high bias, as we get more and more training examples, that is, as we move to the right of this figure, we'll notice that the cross validation error isn't going down much, it's basically fattened up, and so if learning algorithms are really suffering from high bias. Getting more training data by itself will actually not help that much,and as our figure example in the figure on the right, here we had only five training. examples, and we fill certain straight line. And when we had a ton more training data, we still end up with roughly the same straight line. And so if the learning algorithm has high bias give me a lot more training data. That doesn't actually help you get a much lower cross validation error or test set error. So knowing if your learning algorithm is suffering from high bias seems like a useful thing to know because this can prevent you from wasting a lot of time collecting more training data where it might just not end up being helpful.
### High variance 【overfitting】 of the learning curve analysis
![High variance 【overfitting】 of the learning curve analysis](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/31-高方差【过拟合】下学习曲线分析.jpg)  
Next let us look at the setting of a learning algorithm that may have high variance.Let us just look at the training error in a around if you have very smart training set like five training examples shown on the figure on the right and if we're fitting say a very high order polynomial,and I've written a hundredth degree polynomial which really no one uses, but just an illustration.And if we're using a fairly small value of lambda, maybe not zero, but a fairly small value of lambda, then we'll end up, you know, fitting this data very well that with a function that overfits this. So, if the training set size is small, our training error, that is, j train of theta will be small. And as this training set size increases a bit, you know, we may still be overfitting this data a little bit but it also becomes slightly harder to fit this data set perfectly, and so, as the training set size increases, we'll find that j train increases, because it is just a little harder to fit the training set perfectly when we have more examples, but the training set error will still be pretty low. Now, how about the cross validation error? Well, in high variance setting, a hypothesis is overfitting and so the cross validation error will remain high, even as we get you know, a moderate number of training examples and, so maybe, the cross validation error may look like that. And the indicative diagnostic that we have a high variance problem,is the fact that there's this large gap between the training error and the cross validation error.And looking at this figure. If we think about adding more training data, that is, taking this figure and extrapolating to the right, we can kind of tell that, you know the two curves, the blue curve and the magenta curve, are converging to each other. And so, if we were to extrapolate this figure to the right, then it seems it likely that the training error will keep on going up and the cross-validation error would keep on going down. And the thing we really care about is the cross-validation error or the test set error, right? So in this sort of figure, we can tell that if we keep on adding training examples and extrapolate to the right, well our cross validation error will keep on coming down. And, so, in the high variance setting, getting more training data is, indeed, likely to help. And so again, this seems like a useful thing to know if your learning algorithm is suffering from a high variance problem, because that tells you, for example that it may be be worth your while to see if you can go and get some more training data.Now, on the previous slide and this slide, I've drawn fairly clean fairly idealized curves. If you plot these curves for an actual learning algorithm, sometimes you will actually see, you know, pretty much curves, like what I've drawn here. Although, sometimes you see curves that are a little bit noisier and a little bit messier than this. But plotting learning curves like these can often tell you, can often help you figure out if your learning algorithm is suffering from bias, or variance or even a little bit of both. So when I'm trying to improve the performance of a learning algorithm, one thing that I'll almost always do is plot these learning curves, and usually this will give you a better sense of whether there is a bias or variance problem.And in the next video we'll see how this can help suggest specific actions is to take, or to not take, in order to try to improve the performance of your learning algorithm.
### Exam_in the video
![Exam_Understand High bias or High variance learning curve](amWiki/images/001/06-Week6/1-Advice for Applying Machine Learning/32-内置习题_理解高偏差和高方差下的学习曲线.jpg)  
