# 梯度下降应用-学习速率
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/02-Week2/1 Linear Regression with Multiple Variables/4-Gradient Descent in Practice II - Learning Rate.mp4" type="video/mp4">
</video>
## 中文
### 讨论学习速度α
![讨论学习速度α](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/15-讨论学习速度α.jpg)
在本段视频中 我想告诉大家 一些关于梯度下降算法的实用技巧 我将集中讨论学习率 α 具体来说 这是梯度下降算法的 更新规则 这里我想要 告诉大家 如何调试 也就是我认为应该如何确定 梯度下降是正常工作的 此外我还想告诉大家 如何选择学习率 α 也就是我平常 如何选择这个参数 我通常是怎样确定 梯度下降正常工作的
### 如何确定梯度下降正常工作？
![如何确定梯度下降正常工作](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/16-如何确定梯度下降正常工作.jpg)
梯度下降算法所做的事情 就是为你找到 一个 θ 值 并希望它能够最小化代价函数 J(θ) 我通常会在 梯度下降算法运行时 绘出代价函数 J(θ) 的值 这里的 x 轴是表示 梯度下降算法的 迭代步数 你可能会得到 这样一条曲线 注意 这里的 x 轴是迭代步数 在我们以前看到的 J(θ) 曲线中 x 轴 也就是横轴 曾经用来表示参数 θ 但这里不是 具体来说 这一点的含义是这样的 当我运行完100步的梯度下降迭代之后 无论我得到 什么 θ 值 总之 100步迭代之后 我将得到 一个 θ 值 根据100步迭代之后得到的这个 θ 值 我将算出 代价函数 J(θ) 的值 而这个点的垂直高度就代表 梯度下降算法 100步迭代之后 得到的 θ 算出的 J(θ) 值 而这个点 则是梯度下降算法 迭代200次之后 得到的 θ 算出的 J(θ) 值 所以这条曲线 显示的是 梯度下降算法迭代过程中代价函数 J(θ) 的值 如果梯度下降算法 正常工作 那么每一步迭代之后 J(θ) 都应该下降 这条曲线 的一个用处在于 它可以告诉你 如果你看一下 我画的这条曲线 当你达到300步迭代之后 也就是300步到400步迭代之间 也就是曲线的这一段 看起来 J(θ) 并没有下降多少 所以当你 到达400步迭代时 这条曲线看起来已经很平坦了 也就是说 在这里400步迭代的时候 梯度下降算法 基本上已经收敛了 因为代价函数并没有继续下降 所以说 看这条曲线 可以帮助你判断 梯度下降算法是否已经收敛 顺便说一下 对于每一个特定的问题 梯度下降算法所需的迭代次数 可以相差很大 也许对于某一个问题梯度下降算法 只需要30步迭代就可以收敛 然而换一个问题 也许梯度下降算法就需要3000步迭代 对于另一个机器学习问题 则可能需要三百万步迭代 实际上 我们很难提前判断梯度下降算法 需要多少步迭代才能收敛 通常我们需要画出这类曲线 画出代价函数随迭代步数数增加的变化曲线 通常 我会通过看这种曲线 来试着判断 梯度下降算法是否已经收敛 另外 也可以 进行一些自动的收敛测试 也就是说用一种算法 来告诉你梯度下降算法 是否已经收敛 自动收敛测试 一个非常典型的例子是 如果代价函数 J(θ) 的下降小于 一个很小的值 ε 那么就认为已经收敛 比如可以选择 1e-3 但我发现 通常要选择一个合适的阈值 ε 是相当困难的 因此 为了检查 梯度下降算法是否收敛 我实际上还是 通过看 左边的这条曲线图 而不是依靠自动收敛测试 此外 这种曲线图 也可以
### 梯度下降并没有正常工作的几种曲线图
![梯度下降并没有正常工作的几种曲线图](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/17-梯度下降并没有正常工作的几种曲线图.jpg)
在算法没有正常工作时 提前警告你 具体地说 如果代价函数 J(θ) 随迭代步数 的变化曲线是这个样子 J(θ) 实际上在不断上升
### 梯度下降实际应用_学习速率小结
![梯度下降实际应用_学习速率小结](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/18-梯度下降实际应用_学习速率小结.jpg)
【缺失部分翻译。以后在github发布出来，希望读者完善咯，给各位热爱学习的同学做点贡献。】
### 内嵌习题
![内嵌习题_学习速率](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/20-内嵌习题_学习速率.jpg)
## English
### Talk about learning rate alpha
![Talk about learning rate α](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/15-讨论学习速度α.jpg)
In this video, I want to give you more practical tips for getting gradient descent to work. The ideas in this video will center around the learning rate alpha.Concretely, here's the gradient descent update rule. And what I want to do in this video is tell you about what I think of as debugging, and some tips for making sure that gradient descent is working correctly. And second, I wanna tell you how to choose the learning rate alpha or at least how I go about choosing it. Here's something that I often do to make sure that gradient descent is working correctly. The job of gradient descent is to find the value of theta for you that hopefully minimizes the cost function J(theta). What I often do is therefore plot the cost function J(theta) as gradient descent runs. So the x axis here is a number of iterations of gradient descent and as gradient descent runs you hopefully get a plot that maybe looks like this.
### How to makesure gradient descent is working well?
![How to makesure gradient descent is working well](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/16-如何确定梯度下降正常工作.jpg)
Notice that the x axis is number of iterations. Previously we where looking at plots of J(theta) where the x axis, where the horizontal axis, was the parameter vector theta but this is not what this is. Concretely, what this point is, is I'm going to run gradient descent for 100 iterations. And whatever value I get for theta after 100 iterations, I'm going to get some value of theta after 100 iterations. And I'm going to evaluate the cost function J(theta). For the value of theta I get after 100 iterations, and this vertical height is the value of J(theta). For the value of theta I got after 100 iterations of gradient descent. And this point here that corresponds to the value of J(theta) for the theta that I get after I've run gradient descent for 200 iterations.So what this plot is showing is, is it's showing the value of your cost function after each iteration of gradient decent. And if gradient is working properly then J(theta) should decrease after every iteration.And one useful thing that this sort of plot can tell you also is that if you look at the specific figure that I've drawn, it looks like by the time you've gotten out to maybe 300 iterations, between 300 and 400 iterations, in this segment it looks like J(theta) hasn't gone down much more. So by the time you get to 400 iterations, it looks like this curve has flattened out here. And so way out here 400 iterations, it looks like gradient descent has more or less converged because your cost function isn't going down much more. So looking at this figure can also help you judge whether or not gradient descent has converged.By the way, the number of iterations the gradient descent takes to converge for a physical application can vary a lot, so maybe for one application, gradient descent may converge after just thirty iterations. For a different application, gradient descent may take 3,000 iterations, for another learning algorithm, it may take 3 million iterations. It turns out to be very difficult to tell in advance how many iterations gradient descent needs to converge. And is usually by plotting this sort of plot, plotting the cost function as we increase in number in iterations, is usually by looking at these plots. But I try to tell if gradient descent has converged. It's also possible to come up with automatic convergence test, namely to have a algorithm try to tell you if gradient descent has converged. And here's maybe a pretty typical example of an automatic convergence test. And such a test may declare convergence if your cost function J(theta) decreases by less than some small value epsilon, some small value 10 to the minus 3 in one iteration. But I find that usually choosing what this threshold is is pretty difficult. And so in order to check your gradient descent's converge I actually tend to look at plots like these, like this figure on the left, rather than rely on an automatic convergence test. Looking at this sort of figure can also tell you, or give you an advance warning, if maybe gradient descent is not working correctly. Concretely, if you plot J(theta) as a function of the number of iterations. Then if you see a figure like this where J(theta) is actually increasing, then that gives you a clear sign that gradient descent is not working. And a theta like this usually means that you should be using learning rate alpha.If J(theta) is actually increasing, the most common cause for that is if you're trying to minimize a function, that maybe looks like this.But if your learning rate is too big then if you start off there, gradient descent may overshoot the minimum and send you there. And if the learning rate is too big, you may overshoot again and it sends you there, and so on. So that, what you really wanted was for it to start here and for it to slowly go downhill, right? But if the learning rate is too big, then gradient descent can instead keep on overshooting the minimum. So that you actually end up getting worse and worse instead of getting to higher values of the cost function J(theta). So you end up with a plot like this and if you see a plot like this, the fix is usually just to use a smaller value of alpha. Oh, and also, of course, make sure your code doesn't have a bug of it. But usually too large a value of alpha could be a common problem.Similarly sometimes you may also see J(theta) do something like this, it may go down for a while then go up then go down for a while then go up go down for a while go up and so on. And a fix for something like this is also to use a smaller value of alpha.I'm not going to prove it here, but under other assumptions about the cost function J, that does hold true for linear regression, mathematicians have shown that if your learning rate alpha is small enough, then J(theta) should decrease on every iteration. So if this doesn't happen probably means the alpha's too big, you should set it smaller. But of course, you also don't want your learning rate to be too small because if you do that then the gradient descent can be slow to converge.And if alpha were too small, you might end up starting out here, say, and end up taking just minuscule baby steps. And just taking a lot of iterations before you finally get to the minimum, and so if alpha is too small, gradient descent can make very slow progress and be slow to converge. To summarize, if the learning rate is too small, you can have a slow convergence problem, and if the learning rate is too large, J(theta) may not decrease on every iteration and it may not even converge. In some cases if the learning rate is too large, slow convergence is also possible. But the more common problem you see is just that J(theta) may not decrease on every iteration. And in order to debug all of these things, often plotting that J(theta) as a function of the number of iterations can help you figure out what's going on. Concretely, what I actually do when I run gradient descent is I would try a range of values. So just try running gradient descent with a range of values for alpha, like 0.001 and 0.01. So these are factor of ten differences. And for these different values of alpha are just plot J(theta) as a function of number of iterations, and then pick the value of alpha that seems to be causing J(theta) to decrease rapidly. In fact, what I do actually isn't these steps of ten. So this is a scale factor of ten of each step up. What I actually do is try this range of values.And so on, where this is 0.001. I'll then increase the learning rate threefold to get 0.003. And then this step up, this is another roughly threefold increase from 0.003 to 0.01. And so these are, roughly, trying out gradient descents with each value I try being about 3x bigger than the previous value.
### Some plots about gradient descent not work well
![Some plots about gradient descent not work well](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/17-梯度下降并没有正常工作的几种曲线图.jpg)
So what I'll do is try a range of values until I've found one value that's too small and made sure that I've found one value that's too large. And then I'll sort of try to pick the largest possible value, or just something slightly smaller than the largest reasonable value that I found. And when I do that usually it just gives me a good learning rate for my problem. And if you do this too, maybe you'll be able to choose a good learning rate for your implementation of gradient descent.
### Gradient Descent in practical_learning rate α summarry]
![Gradient Descent in practical_learning rate α summarry](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/18-梯度下降实际应用_学习速率小结.jpg)
### Exam_in the video
![Exam_learning rate α](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/20-内嵌习题_学习速率.jpg)
