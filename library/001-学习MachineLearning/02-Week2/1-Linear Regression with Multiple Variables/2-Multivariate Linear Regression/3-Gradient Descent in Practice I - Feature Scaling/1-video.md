# 梯度下降应用-特征量缩放
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/02-Week2/1 Linear Regression with Multiple Variables/3-Gradient Descent in Practice I - Feature Scaling.mp4" type="video/mp4">
</video>
## 中文
### 特征缩放_特征量取值范围相近
![特征缩放_特征量取值范围相近](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/11-特征缩放_特征量取值范围相近.jpg)
在这段视频 以及下一段视频中 我想告诉你一些关于 梯度下降运算中的实用技巧 在这段视频中 我会告诉你一个称为特征缩放 (feature scaling) 的方法 我们用一个 for 循环 如果你有一个机器学习问题 这个问题有多个特征 如果你能确保这些特征 都处在一个相近的范围 我的意思是确保 不同特征的取值 在相近的范围内 这样梯度下降法就能更快地收敛
### 举例特征缩放_特征量取值范围相近
![举例特征缩放_特征量取值范围相近](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/12-举例特征缩放_特征量取值范围相近.jpg)
具体地说 假如你有一个具有两个特征的问题 其中 x1 是房屋面积大小 它的取值 在0到2000之间 x2 是卧室的数量 可能这个值 取值范围在1到5之间 如果你画出代价函数 J(θ) 的轮廓图 那么这个轮廓看起来 应该是像这样的 J(θ) 是一个关于 参数 θ0 θ1 和 θ2 的函数 但我要忽略 θ0 所以暂时不考虑 θ0 并假想一个函数的变量 只有 θ1 和 θ2 但如果 x1 的取值范围 远远大于 x2 的取值范围的话 那么最终画出来的 代价函数 J(θ) 的轮廓图 就会呈现出这样一种非常偏斜 并且椭圆的形状 2000 和 5的比例 会让这个椭圆更加瘦长 所以 这是一个又瘦又高的 椭圆形轮廓图 就是这些非常高大细长的椭圆形 构成了代价函数 J(θ) 而如果你用这个代价函数 来运行梯度下降的话 你要得到梯度值 最终可能 需要花很长一段时间 并且可能会来回波动 然后会经过很长时间 最终才收敛到全局最小值 事实上 你可以想像 如果这些 轮廓再被放大一些的话 如果你画的再夸张一些 把它画的更细更长 那么可能情况会更糟糕 梯度下降的过程 可能更加缓慢 需要花更长的时间 反复来回振荡 最终才找到一条正确通往全局最小值的路 在这样的情况下 一种有效的方法是进行特征缩放(feature scaling)
### 举例说明如何进行特征缩放
![举例说明如何进行特征缩放](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/13-举例说明如何进行特征缩放.jpg)
具体来说 把特征 x 定义为 房子的面积大小 除以2000的话 并且把 x2 定义为 卧室的数量除以5 那么这样的话 表示代价函数 J(θ) 的轮廓图的形状就会变得偏移没那么严重 可能看起来更圆一些了 如果你用这样的代价函数 来执行梯度下降的话 那么 梯度下降算法 你可以从数学上来证明 梯度下降算法 就会找到一条 更捷径的路径通向全局最小 而不是像刚才那样 沿着一条让人摸不着头脑的路径 一条复杂得多的轨迹 来找到全局最小值 因此 通过特征缩放 通过"消耗掉"这些值的范围 在这个例子中 我们最终得到的两个特征 x1 和 x2 都在0和1之间 这样你得到的梯度下降算法 就会更快地收敛 更一般地 我们执行特征缩放时 也就是我们经常 我们通常的目的是 将特征的取值约束到 -1 到 +1 的范围内 你的特征 x0 是总是等于1 因此 这已经是在这个范围内 但对其他的特征 你可能需要通过除以不同的数 来让它们处于同一范围内 -1 和 +1 这两个数字并不是太重要 所以 如果你有一个特征x1 它的取值 在0和3之间 这没问题 如果你有另外一个特征 取值在-2 到 +0.5之间 这也没什么关系 这也非常接近 -1 到 +1的范围 这些都可以 但如果你有另一个特征 比如叫 x3 假如它的范围在 -100 到 +100之间 那么 这个范围 跟-1到+1就有很大不同了 所以 这可能是一个 不那么好的特征 类似地 如果你的特征在一个 非常非常小的范围内 比如另外一个特征 x4 它的范围在 0.0001和+0.0001之间 那么这同样是一个 比-1到+1小得多的范围 比-1到+1小得多的范围 因此 我同样会认为这个特征也不太好 所以 可能你认可的范围 也许可以大于 或者小于 -1 到 +1 但是也别太大 只要大得不多就可以接受 比如 +100 或者也别太小 比如这里的0.001 不同的人有不同的经验 但是我一般是这么考虑的 如果一个特征是在 -3 到 +3 的范围内 那么你应该认为 这个范围是可以接受的 但如果这个范围 大于了 -3 到 +3 的范围 我可能就要开始注意了 如果它的取值 在-1/3 到+1/3的话 我觉得 还不错 可以接受 或者是0到1/3 或-1/3到0 这些典型的范围 我都认为是可以接受的 但如果特征的范围 取得很小的话 比如像这里的 x4 你就要开始考虑进行特征缩放了 因此 总的来说 不用过于担心 你的特征是否在完全 相同的范围或区间内 但是只要他们都 只要它们足够接近的话 梯度下降法就会正常地工作 除了在特征缩放中 将特征除以最大值以外 有时候我们也会进行一个 称为均值归一化的工作(mean normalization) 我的意思是这样的 如果你有一个特征 xi 你就用 xi - μi 来替换 通过这样做 让你的特征值 具有为0的平均值 很明显 我们不需要 把这一步应用到 x0中 因为 x0 总是等于1的 所以它不可能有 为0的的平均值 但是 对其他的特征来说 比如房子的大小 取值介于0到2000 并且假如 房子面积 的平均值 是等于1000的 那么你可以用这个公式 将 x1 的值变为 x1 减去平均值 μ1 再除以2000 类似地 如果你的房子有 五间卧室 并且平均一套房子有 两间卧室 那么你可以 使用这个公式 来归一化你的第二个特征 x2 在这两种情况下 你可以算出新的特征 x1 和 x2 这样它们的范围 可以在-0.5和+0.5之间 当然这肯定不对 x2的值实际上肯定会大于0.5 但很接近 更一般的规律是 你可以用这样的公式 你可以用 (x1 - μ1)/S1 来替换原来的特征 x1 其中定义 μ1的意思是 在训练集中特征 x1 的平均值而 S1 是 该特征值的范围 我说的范围是指 最大值减去最小值 最大值减去最小值 或者学过 标准差的同学可以记住 也可以把 S1 设为 变量的标准差 但其实用最大值减最小值就可以了 类似地 对于第二个 特征 x2 你也可以用同样的这个特征减去平均值 再除以范围 来替换原特征 范围的意思依然是最大值减最小值 这类公式将 把你的特征 变成这样的范围 也许不是完全这样 但大概是这样的范围 顺便提一下 有些同学可能比较仔细 如果我们用最大值减最小值 来表示范围的话 这里的5有可能应该是4 如果最大值为5 那么减去最小值1 这个范围值就是4 但不管咋说 这些取值 都是非常近似的 只要将特征转换为 相近似的范围 就都是可以的 特征缩放其实 并不需要太精确 只是为了让梯度下降 能够运行得更快一点而已 好的 现在你知道了 什么是特征缩放 通过使用这个简单的方法 你可以将梯度下降的速度变得更快 让梯度下降收敛所需的循环次数更少 这就是特征缩放 在接下来的视频中 我将介绍另一种技巧来使梯度下降 在实践中工作地更好
## English
### Features Scaling
![Features Scaling](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/11-特征缩放_特征量取值范围相近.jpg)
In this video and in the video after this one, I wanna tell you about some of the practical tricks for making gradient descent work well. In this video, I want to tell you about an idea called feature skill.Here's the idea. If you have a problem where you have multiple features, if you make sure that the features are on a similar scale, by which I mean make sure that the different features take on similar ranges of values,then gradient descents can converge more quickly.
### Example for Features Scaling
![Example for Features Scaling](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/12-举例特征缩放_特征量取值范围相近.jpg)
Concretely let's say you have a problem with two features where X1 is the size of house and takes on values between say zero to two thousand and two is the number of bedrooms, and maybe that takes on values between one and five. If you plot the contours of the cos function J of theta,then the contours may look like this, where, let's see, J of theta is a function of parameters theta zero, theta one and theta two. I'm going to ignore theta zero, so let's about theta 0 and pretend as a function of only theta 1 and theta 2, but if x1 can take on them, you know, much larger range of values and x2 It turns out that the contours of the cause function J of theta can take on this very very skewed elliptical shape, except that with the so 2000 to 5 ratio, it can be even more secure. So, this is very, very tall and skinny ellipses, or these very tall skinny ovals, can form the contours of the cause function J of theta.And if you run gradient descents on this cos-function, your gradients may end up taking a long time and can oscillate back and forth and take a long time before it can finally find its way to the global minimum.In fact, you can imagine if these contours are exaggerated even more when you draw incredibly skinny, tall skinny contours,and it can be even more extreme than, then, gradient descent just have a much harder time taking it's way, meandering around, it can take a long time to find this way to the global minimum.In these settings, a useful thing to do is to scale the features.

### Example for how to Features Scaling
![Example for how to Features Scaling](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/13-举例说明如何进行特征缩放.jpg)
Concretely if you instead define the feature X one to be the size of the house divided by two thousand, and define X two to be maybe the number of bedrooms divided by five, then the count well as of the cost function J can become much more, much less skewed so the contours may look more like circles.And if you run gradient descent on a cost function like this, then gradient descent,you can show mathematically, you can find a much more direct path to the global minimum rather than taking a much more convoluted path where you're sort of trying to follow a much more complicated trajectory to get to the global minimum.So, by scaling the features so that there are, the consumer ranges of values. In this example, we end up with both features, X one and X two, between zero and one.You can wind up with an implementation of gradient descent. They can convert much faster.More generally, when we're performing feature scaling, what we often want to do is get every feature into approximately a -1 to +1 range and concretely, your feature x0 is always equal to 1. So, that's already in that range,but you may end up dividing other features by different numbers to get them to this range. The numbers -1 and +1 aren't too important. So, if you have a feature,x1 that winds up being between zero and three, that's not a problem. If you end up having a different feature that winds being between -2 and + 0.5, again, this is close enough to minus one and plus one that, you know, that's fine, and that's fine.It's only if you have a different feature, say X 3 that is between, that ranges from -100 tp +100 , then, this is a very different values than minus 1 and plus 1. So, this might be a less well-skilled feature and similarly, if your features take on a very, very small range of values so if X 4 takes on values between minus 0.0001 and positive 0.0001, then again this takes on a much smaller range of values than the minus one to plus one range. And again I would consider this feature poorly scaled.So you want the range of values, you know, can be bigger than plus or smaller than plus one, but just not much bigger, like plus 100 here, or too much smaller like 0.00 one over there. Different people have different rules of thumb. But the one that I use is that if a feature takes on the range of values from say minus three the plus 3 how you should think that should be just fine, but maybe it takes on much larger values than plus 3 or minus 3 unless not to worry and if it takes on values from say minus one-third to one-third.You know, I think that's fine too or 0 to one-third or minus one-third to 0. I guess that's typical range of value sector 0 okay. But it will take on a much tinier range of values like x4 here than gain on mine not to worry. So, the take-home message is don't worry if your features are not exactly on the same scale or exactly in the same range of values. But so long as they're all close enough to this gradient descent it should work okay. In addition to dividing by so that the maximum value when performing feature scaling sometimes people will also do what's called mean normalization. And what I mean by that is that you want to take a feature Xi and replace it with Xi minus new i to make your features have approximately 0 mean.And obviously we want to apply this to the future x zero, because the future x zero is always equal to one, so it cannot have an average value of zero.But it concretely for other features if the range of sizes of the house takes on values between 0 to 2000 and if you know, the average size of a house is equal to 1000 then you might use this formula.Size, set the feature X1 to the size minus the average value divided by 2000 and similarly, on average if your houses have one to five bedrooms and if on average a house has two bedrooms then you might use this formula to mean normalize your second feature x2.In both of these cases, you therefore wind up with features x1 and x2. They can take on values roughly between minus .5 and positive .5. Exactly not true - X2 can actually be slightly larger than .5 but, close enough. And the more general rule is that you might take a feature X1 and replace it with X1 minus mu1 over S1 where to define these terms mu1 is the average value of x1 in the training sets and S1 is the range of values of that feature and by range, I mean let's say the maximum value minus the minimum value or for those of you that understand the deviation of the variable is setting S1 to be the standard deviation of the variable would be fine, too. But taking, you know, this max minus min would be fine.And similarly for the second feature, x2, you replace x2 with this sort of
subtract the mean of the feature and divide it by the range of values meaning the max minus min. And this sort of formula will get your features, you know, maybe not exactly, but maybe roughly into these sorts of ranges, and by the way, for those of you that are being super careful technically if we're taking the range as max minus min this five here will actually become a four. So if max is 5 minus 1 then the range of their own values is actually equal to 4, but all of these are approximate and any value that gets the features into anything close to these sorts of ranges will do fine. And the feature scaling doesn't have to be too exact, in order to get gradient descent to run quite a lot faster.So, now you know about feature scaling and if you apply this simple trick, it and make gradient descent run much faster and converge in a lot fewer other iterations.That was feature scaling. In the next video, I'll tell you about another trick to make gradient descent work well in practice.
