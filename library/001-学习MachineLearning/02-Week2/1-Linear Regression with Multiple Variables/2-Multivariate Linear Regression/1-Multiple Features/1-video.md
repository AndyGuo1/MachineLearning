# 多特征量
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/02-Week2/1 Linear Regression with Multiple Variables/1-Multiple Features.mp4" type="video/mp4">
</video>
## 中文
### 回顾单变量线性回归假设函数
![回顾单变量线性回归假设方程](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/1-回顾单变量线性回归假设方程.jpg)  
在这段视频中 我们将开始 介绍一种新的 更为有效的线性回归形式 这种形式适用于多个变量或者多特征量的情况 做矩阵乘法 在之前我们学习过的 线性回归中 我们只有一个单一特征量 房屋面积 x 我们希望用这个特征量 来预测 房子的价格这就是我们的假设
### 多特征量【变量】术语
但是想象一下 如果我们不仅有房屋面积 作为预测房屋 价格的特征量 或者变量 我们还知道 卧室的数量 楼层的数量以及房子的使用年限 这样就给了我们 更多可以用来 预测房屋价格的信息  
![多特征量【变量】术语](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/2-多特征量【变量】.jpg)  
先简单介绍一下记法 我们开始的时候就提到过 我要用 x 下标1 x 下标2 等等 来表示 这种情况下的四个特征量 然后仍然用 Y来表示我们 所想要预测的输出变量 让我们来看看更多的表示方式 现在我们有四个特征量 我要用小写n来表示特征量的数目 因此在这个例子中 我们的n等于4 因为你们看 我们有 1 2 3 4 共4个特征量 这里的n和我们之前 使用的n不同 之前我们是用的“m”来表示样本的数量 所以如果你有47行 那么m就是这个表格里面的行数 或者说是训练样本数 然后我要用x 上标 (i) 来表示第i个 训练样本的 输入特征值 举个具体的例子来说 x上标 (2) 就是表示第二个 训练样本的特征向量 因此这里 x(2)就是向量 [1416, 3, 2, 40] 因为这四个数字对应了 我用来预测房屋价格的 第二个房子的四个特征量 因此在这种记法中 这个上标2就是训练集的一个索引 而不是x的2次方 这个2就对应着 你所看到的表格中的第二行 即我的第二个训练样本x上标(2) 这样表示 就是一个四维向量 事实上更普遍地来说 这是n维的向量 用这种表示方法 x上标2就是一个向量 因此 我用x上标(i) 下标j 来表示第i个训练样本的 第j个特征量 因此具体的来说 x上标(2)下标3代表着 第2个训练样本里的第3个特征量 对吧？ 这个是3 我写的不太好看 所以说x上标(2)下标3就等于2 既然我们有了多个特征量
### 多特征量下的假设函数
![多特征量下的假设函数](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/3-多特征量下的假设函数.jpg)
让我们继续讨论一下 我们的假设形式应该是怎样的 这是我们之前使用的假设形式 x就是我们唯一的特征量 但现在我们有了多个特征量 我们就不能再 使用这种简单的表示方式了 取而代之的 我们将把线性回归的假设改成这样 θ0加上 θ1 乘以 x1 加上 θ2乘以x2 加上 θ3 乘以x3加上θ4乘以x4 然后如果我们有n个特征量 那么我们要将所有的n个特征量相加 而不是四个特征量 我们需要对n个特征量进行相加 举个具体的例子 在我们的设置的参数中 我们可能有h(x)等于80 + 0.1 x1 + 0.01x2 + 3x3 - 2x4 这就是一个假设的范例 别忘了 假设是为了预测 大约以千刀为单位的房屋价格 就是说 一个房子的价格 可以是 80 k加上 0.1乘以x1 也就是说 每平方尺100美元 然后价格 会随着楼层数的增加 再继续增长 x2是楼层数 接着价格会继续增加 随着卧室数的增加 因为x3是 卧室的数量 但是呢 房子的价格会 随着使用年数的增加 而贬值这是重新改写过的假设的形式 接下来 我要来介绍一点 简化这个等式的表示方式为了表示方便 我要将x下标0的值设为1具体而言 这意味着 对于第i个样本 都有一个向量x上标(i) 并且x上标(i) 下标0等于1 你可以认为我们 定义了一个额外的第0个特征量 因此 我过去有n个特征量 因为我们有x1 x2 直到xn 由于我另外定义了 额外的第0个特征向量并且它的取值 总是1 所以我现在的特征向量x 是一个从0开始标记的 n+1维的向量 所以现在就是一个 n+1维的特征量向量 但我要从0开始标记 同时 我也想把我的参数 都看做一个向量 所以我们的参数就是 我们的θ0 θ1 θ2 等等 直到θn 我们要把 所有的参数都写成一个向量 θ0 θ2...一直到 直到θn 这里也有一个从0开始标记的矢量 下标从0开始 这是另外一个 所以我的假设 现在可以写成θ0乘以x0 加上θ1乘以x1直到 θn 乘以xn 这个等式 和上面的等式是一样的 因为你看 x0等于1
### 多特征量假设方程以向量方式表示
![多特征量假设方程以向量方式表示](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/4-多特征量假设方程以向量方式表示.jpg)  
下面 我要 把这种形式假设等式 写成 θ转置乘以X 取决于你对 向量内积有多熟悉 如果你展开 θ转置乘以X 那么就得到 θ0 θ1直到θn 这个就是θ转置 实际上 这就是一个 n+1乘以1维的矩阵 也被称为行向量 用行向量 与X向量相乘 X向量是 x0 x1等等 直到xn 因此内积就是 θ转置乘以X 就等于这个等式 这就为我们提供了一个 表示假设的 更加便利的形式 即用参数向量θ以及 特征向量X的内积 这就是改写以后的 表示方法 这样的表示习惯 就让我们 可以以这种紧凑的形式写出假设 这就是多特征量情况下的假设形式 起另一个名字 就是 所谓的多元线性回归 多元一词 也就是用来预测的多个特征量 或者变量 就是一种更加好听的说法罢了  
### 内置习题
![内置习题](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/5-内置习题.jpg)  
## English
![Review one variable Hypothesis Function](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/1-回顾单变量线性回归假设方程.jpg)  
in this video we will start to talk about a new version of linear regression that's more powerful. One that works with multiple variables or with multiple features.Here's what I mean.In the original version of linear regression that we developed, we have a single feature x, the size of the house, and we wanted to use that to predict why the price of the house and this was our form of our hypothesis.
### Multiple features【Notation】
But now imagine, what if we had not only the size of the house as a feature or as a variable of which to try to predict the price, but that we also knew the number of bedrooms, the number of house and the age of the home and years. It seems like this would give us a lot more information with which to predict the price.To introduce a little bit of notation, we sort of started to talk about this earlier, I'm going to use the variables X subscript 1 X subscript 2 and so on to denote my, in this case, four features and I'm going to continue to use Y to denote the variable, the output variable price that we're trying to predict.
![ Multiple features【Notation】](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/2-多特征量【变量】.jpg)
Let's introduce a little bit more notation.Now that we have four features I'm going to use lowercase "n" to denote the number of features. So in this example we have n4 because we have, you know, one, two, three, four features.And "n" is different from our earlier notation where we were using "n" to denote the number of examples. So if you have 47 rows "M" is the number of rows on this table or the number of training examples.So I'm also going to use X superscript "I" to denote the input features of the "I" training example.As a concrete example let say X2 is going to be a vector of the features for my second training example. And so X2 here is going to be a vector 1416, 3, 2, 40 since those are my four features that I have to try to predict the price of the second house.So, in this notation, the superscript 2 here.That's an index into my training set. This is not X to the power of 2. Instead, this is, you know, an index that says look at the second row of this table. This refers to my second training example.With this notation X2 is a four dimensional vector. In fact, more generally, this is an in-dimensional feature back there.With this notation, X2 is now a vector and so, I'm going to use also Xi subscript J to denote the value of the J,of feature number J and the training example.So concretely X2 subscript 3, will refer to feature number three in the x factor which is equal to 2,right? That was a 3 over there, just fix my handwriting. So x2 subscript 3 is going to be equal to 2.Now that we have multiple features,
### Hypothesis Function with multiple features
![Hypothesis Function with multiple features](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/3-多特征量下的假设函数.jpg)  
Concretely for a particular setting of our parameters we may have H of X 80 + 0.1 X1 + 0.01x2 + 3x3 - 2x4. This would be one example of a hypothesis and you remember a hypothesis is trying to predict the price of the house in thousands of dollars, just saying that, you know, the base price of a house is maybe 80,000 plus another open 1, so that's an extra, what, hundred dollars per square feet, yeah, plus the price goes up a little bit for each additional floor that the house has. X two is the number of floors, and it goes up further for each additional bedroom the house has, because X three was the number of bedrooms, and the price goes down a little bit with each additional age of the house. With each additional year of the age of the house.Here's the form of a hypothesis rewritten on the slide. And what I'm gonna do is introduce a little bit of notation to simplify this equation.For convenience of notation, let me define x subscript 0 to be equals one.Concretely, this means that for every example i I have a feature vector X superscript I and X superscript I subscript 0 is going to be equal to 1. You can think of this as defining an additional zero feature. So whereas previously I had n features because x1, x2 through xn, I'm now defining an additional sort of zero feature vector that always takes on the value of one.So now my feature vector X becomes this N+1 dimensional vector that is zero index.So this is now a n+1 dimensional feature vector, but I'm gonna index it from 0 and I'm also going to think of my parameters as a vector. So, our parameters here, right that would be our theta zero, theta one, theta two, and so on all the way up to theta n, we're going to gather them up into a parameter vector written theta 0, theta 1, theta 2, and so on, down to theta n. This is another zero index vector. It's of index signed from zero.That is another n plus 1 dimensional vector.So, my hypothesis cannot be written theta 0x0 plus theta 1x1+ up to theta n Xn.And this equation is the same as this on top because, you know, eight zero is equal to one.
### Multivariable Hypothesis Function in vector
![Multivariable Hypothesis Function in vector](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/4-多特征量假设方程以向量方式表示.jpg)  
Underneath and I now take this form of the hypothesis and write this as either transpose x, depending on how familiar you are with inner products of vectors if you write what theta transfers x is what theta transfer and this is theta zero, theta one, up to theta N. So this thing here is theta transpose and this is actually a N plus one by one matrix. [It should be a 1 by (n+1) matrix] It's also called a row vector and you take that and multiply it with the vector X which is X zero, X one, and so on, down to X n. And so, the inner product that is theta transpose X is just equal to this. This gives us a convenient way to write the form of the hypothesis as just the inner product between our parameter vector theta and our theta vector X. And it is this little bit of notation, this little excerpt of the notation convention that let us write this in this compact form. So that's the form of a hypthesis when we have multiple features. And, just to give this another name, this is also called multivariate linear regression.And the term multivariable that's just maybe a fancy term for saying we have multiple features, or multivariables with which to try to predict the value Y.

### Exam_in the video
![Exam_in the video](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/5-内置习题.jpg)  
