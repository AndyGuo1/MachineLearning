# 正规方程
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/02-Week2/1 Linear Regression with Multiple Variables/6-Normal Equation.mp4" type="video/mp4">
</video>
## 中文
### 梯度下降VS正规方程
![梯度下降VS正规方程](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/26-梯度下降VS正规方程.jpg)
在这段视频中 我们要讲 正规方程 (Normal Equation) 对于某些线性回归问题 用正规方程法求解参数 θ 的最优值更好  具体而言 到目前为止 我们一直在使用的线性回归的算法 是梯度下降法 就是说 为了最小化代价函数 J(θ) 来最小化这个 我们使用的迭代算法 需要经过很多步 也就是说通过多次迭代来计算梯度下降 也就是说通过多次迭代来计算梯度下降 来收敛到全局最小值 相反地 正规方程法提供了一种求 θ 的解析解法 正规方程法提供了一种求 θ 的解析解法 所以与其使用迭代算法 我们可以直接一次性求解θ的最优值 我们可以直接一次性求解θ的最优值 我们可以直接一次性求解θ的最优值 所以说基本上 一步就可以得到优化值
### 从直观上理解正规方程
![从直观上理解正规方程](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/27-从直观上理解正规方程.jpg)
正规方程法有一些优点 也有一些缺点 但是在我们讲解这个 和何时使用标准方程之前 让我们先对这个算法有一个直观的理解 让我们先对这个算法有一个直观的理解 我们举一个例子来解释这个问题 我们假设 有一个非常简单的代价函数 J(θ) 我们假设 有一个非常简单的代价函数 J(θ) 它就是一个实数 θ 的函数 它就是一个实数 θ 的函数 所以现在 假设 θ 只是一个标量 或者说 θ 只有一行 它是一个数字 不是向量 假设我们的代价函数 J 是这个实参数 θ 的二次函数 所以 J(θ) 看起来是这样的 那么如何最小化一个二次函数呢? 对于那些了解一点微积分的同学来说 你可能知道 最小化的一个函数的方法是 对它求导 并且将导数置零 对它求导 并且将导数置零 所以对 J 求关于 θ 的导数 我不打算推导那些公式 你把那个导数置零 这样你就可以求得 使得 J(θ) 最小的 θ 值 使得 J(θ) 最小的 θ 值 这是数据为实数的 一个比较简单的例子 在这个问题中 我们感兴趣的是 θ不是一个实数的情况 它是一个n+1维的参数向量 它是一个n+1维的参数向量 并且 代价函数 J 是这个向量的函数 并且 代价函数 J 是这个向量的函数 也就是 θ0 到 θm 的函数 一个代价函数看起来是这样 像右边的这个平方代价函数 我们如何最小化这个代价函数J? 实际上 微积分告诉我们一种方法 实际上 微积分告诉我们一种方法 对每个参数 θ 求 J 的偏导数 对每个参数 θ 求 J 的偏导数 然后把它们全部置零 如果你这样做 并且求出θ0 θ1 一直到θn的值 并且求出θ0 θ1 一直到θn的值 并且求出θ0 θ1 一直到θn的值 这样就能得到能够最小化代价函数 J 的 θ 值 这样就能得到能够最小化代价函数 J 的 θ 值 这样就能得到能够最小化代价函数 J 的 θ 值 如果你真的做完微积分和求解参数 θ0 到 θn 如果你真的做完微积分和求解参数 θ0 到 θn 如果你真的做完微积分和求解参数 θ0 到 θn 如果你真的做完微积分和求解参数 θ0 到 θn 你会发现这个偏微分最终可能很复杂 接下来我在视频中要做的 实际上不是遍历所有的偏微分 因为这样太久太费事 我只是想告诉你们 你们想要实现这个过程所需要知道内容 你们想要实现这个过程所需要知道内容 这样你就可以解出 偏导数为0时 θ的值 偏导数为0时 θ的值 偏导数为0时 θ的值 换个方式说 或者等价地 这个 θ 能够使得代价函数 J(θ) 最小化 这个 θ 能够使得代价函数 J(θ) 最小化 我发现可能只有熟悉微积分的同学 我发现可能只有熟悉微积分的同学 比较容易理解我的话 比较容易理解我的话 所以 如果你不了解 或者不那么了解微积分 也不必担心 我会告诉你 要实现这个算法并且使其正常运行 你所需的必要知识
### 举例分析正规方程
![举例分析正规方程](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/28-举例分析正规方程.jpg)
举个例子 我想运行这样一个例子 假如说我有 m=4 个训练样本 假如说我有 m=4 个训练样本 为了实现正规方程法 我要这样做 看我的训练集 在这里就是这四个训练样本 在这种情况下 我们假设 这四个训练样本就是我的所有数据 我所要做的是 在我的训练集中加上一列对应额外特征变量的x0 在我的训练集中加上一列对应额外特征变量的x0 在我的训练集中加上一列对应额外特征变量的x0 就是那个取值永远是1的 就是那个取值永远是1的 接下来我要做的是 构建一个矩阵 X 这个矩阵基本包含了训练样本的所有特征变量 这个矩阵基本包含了训练样本的所有特征变量 这个矩阵基本包含了训练样本的所有特征变量 所以具体地说 这里有我所有的特征变量 这里有我所有的特征变量 我们要把这些数字 全部放到矩阵中 X 中 好吧？ 所以只是 每次复制一列的数据 我要对 y 做类似的事情 我要对我们将要预测的值 我要对我们将要预测的值 构建一个向量 像这样的 并且称之为向量 y 所以 X 会是一个 m*(n+1) 维矩阵 所以 X 会是一个 m*(n+1) 维矩阵 y 会是一个 m 维向量 y 会是一个 m 维向量 其中 m 是训练样本数量 n 是特征变量数 n+1 是因为我加的这个额外的特征变量 x0 n+1 是因为我加的这个额外的特征变量 x0 最后 如​​果你用矩阵 X 和向量 y 来计算这个 最后 如​​果你用矩阵 X 和向量 y 来计算这个 最后 如​​果你用矩阵 X 和向量 y 来计算这个 θ 等于 X 转置乘以 X 的逆 乘以 X 转置 乘以 y θ 等于 X 转置乘以 X 的逆 乘以 X 转置 乘以 y X转置 乘以X的逆 乘以X转置 乘以y θ 等于 X 转置乘以 X 的逆 乘以 X 转置 乘以 y 这样就得到能够使得代价函数最小化的 θ 这样就得到能够使得代价函数最小化的 θ 幻灯片上的内容比较多 幻灯片上的内容比较多 我讲解了这样一个数据组的一个例子 让我把这个写成更加通用的形式
### 正规方程通用形式详细介绍
![正规方程通用形式详细介绍](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/29-正规方程通用形式详细介绍.jpg)
在之后的视频中 我会仔细介绍这个方程 以防你不完全清楚要如何做 在一般情况下 假如我们有 m 个训练样本 x(1) y(1) 直到 x(m) y(m) n 个特征变量 所以每一个训练样本 xi 可能看起来像一个向量 像这样一个 n+1 维特征向量 我要构建矩阵 X 的方法 我要构建矩阵 X 的方法 也被称为设计矩阵 如下所示 每个训练样本给出一个这样的特征向量 每个训练样本给出一个这样的特征向量 也就是说 这样的 n+1 维向量 我构建我的设计矩阵 X 的方法 就是构建这样的矩阵 接下来我要做的是将 取第一个训练样本 取第一个训练样本 也就是一个向量 取它的转置 它最后是这样 扁长的样子 让 x1 转置作为我设计矩阵的第一行 然后我要把我的 第二个训练样本 x2 进行转置 让它作为 X 的第二行 进行转置 让它作为 X 的第二行 以此类推 直到最后一个训练样本 取它的转置作为矩阵 X 的最后一行 取它的转置作为矩阵 X 的最后一行 取它的转置作为矩阵 X 的最后一行 这样矩阵 X 就是一个 m*(n+1) 维矩阵 这样矩阵 X 就是一个 m*(n+1) 维矩阵 这样矩阵 X 就是一个 m*(n+1) 维矩阵 举个具体的例子 假如我只有一个特征变量 就是说除了 x0 之外只有一个特征变量 就是说除了 x0 之外只有一个特征变量 而 x0 始终为1 所以如果我的特征向量 xi等于1 也就是x0 和某个实际的特征变量 xi等于1 也就是x0 和某个实际的特征变量 xi等于1 也就是x0 和某个实际的特征变量 比如说房屋大小 那么我的设计矩阵 X 会是这样 第一行 就是这个的转置 第一行 就是这个的转置 所以最后得到1 然后 x(1)1 对于第二行 我们得到1 然后 x(1)2 对于第二行 我们得到1 然后 x(1)2 对于第二行 我们得到1 然后 x(1)2 这样直到1 然后 x(1)m 这样直到1 然后 x(1)m 这样 这就会是一个 m*2 维矩阵 这样 这就会是一个 m*2 维矩阵 所以 这就是如何构建矩阵X 和向量y 所以 这就是如何构建矩阵X 和向量y 有时我可能会在上面画一个箭头 有时我可能会在上面画一个箭头 来表示这是一个向量 但很多时候 我就只写y 是一样的 向量y 是这样求得的 把所有标签 所有训练集中正确的房子价格 所有训练集中正确的房子价格 放在一起 得到一个 m 维向量 y 放在一起 得到一个 m 维向量 y 最后 构建完矩阵 X 和向量 y 最后 构建完矩阵 X 和向量 y 我们就可以通过计算 X转置 乘以X的逆 乘以X转置 乘以y 来得到 θ 我们就可以通过计算 X转置 乘以X的逆 乘以X转置 乘以y 来得到 θ 我们就可以通过计算 X转置 乘以X的逆 乘以X转置 乘以y 来得到 θ 我现在就想确保你明白这个等式 我现在就想确保你明白这个等式 并且知道如何实现它 所以具体来说 什么是 X 的转置乘以 X 的逆？ X的转置 乘以X的逆 是 X转置 乘以X的逆矩阵 X的转置 乘以X的逆 是 X转置 乘以X的逆矩阵 具体来说 如果你令A等于 X转置乘以X 如果你令A等于 X转置乘以X X的转置是一个矩阵 X的转置乘以X 是另一个矩阵 X的转置乘以X 是另一个矩阵 我们把这个矩阵称为 A 那么 X转置乘以X的逆 就是矩阵 A 的逆 那么 X转置乘以X的逆 就是矩阵 A 的逆 也就是 1/A 这就是计算过程 先计算 X转置乘以X 然后计算它的逆
![正规方程通用形式详细介绍](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/30-正规方程通用形式详细介绍.jpg)
我们还没有谈到Octave 我们将在之后的视频中谈到这个 但是在 Octave 编程语言 或者类似的 MATLAB 编程语言里 或者类似的 MATLAB编程语言里 计算这个量的命令是基本相同的 X转置 乘以X的逆 乘以X转置 乘以y 的代码命令如下所示 在 Octave 中 X’ 表示 X 转置 在 Octave 中 X’ 表示 X 转置 这个用红色框起来的表达式 计算的是 X 转置乘以 X 计算的是 X 转置乘以 X pinv 是用来计算逆矩阵的函数 pinv 是用来计算逆矩阵的函数 所以这个计算 X转置 乘以X的逆 所以这个计算 X转置 乘以X的逆 然后乘以X转置 再乘以y 然后乘以X转置 再乘以y 然后乘以X转置 再乘以y 这样就算完了这个式子 我没有证明这个式子 尽管我并不打算这么做 但是数学上是可以证明的 这个式子会给出最优的 θ 值 这个式子会给出最优的 θ 值 这个式子会给出最优的 θ 值 就是说如果你令 θ 等于这个 就是说如果你令 θ 等于这个 这个 θ 值会最小化这个线性回归的代价函数 J(θ) 这个 θ 值会最小化这个线性回归的代价函数 J(θ) 这个 θ 值会最小化这个线性回归的代价函数 J(θ) 最后一点 在之前视频中我提到特征变量归一化 和让特征变量在相似的范围内的想法 将所有的值归一化在类似范围内 如果你使用正规方程法 那么就不需要归一化特征变量 那么就不需要归一化特征变量 实际上这是没问题的 如果某个特征变量 x1 在 0到1的区间 如果某个特征变量 x1 在 0到1的区间 某个特征变量 x2 在0到1000的区间 某个特征变量 x2 在0到1000的区间 某个特征变量 x2 在0到1000的区间 某个特征变量x3 在0到10^-5的区间 某个特征变量x3 在0到10^-5的区间 某个特征变量x3 在0到10^-5的区间 然后如果使用正规方程法 这样就没有问题 不需要做特征变量归一化 但如果你使用梯度下降法 但如果你使用梯度下降法 特征变量归一化就很重要
### 梯度下降VS正规方程优缺点
![梯度下降VS正规方程优缺点](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/31-梯度下降VS正规方程优缺点.jpg)
最后 你何时应该使用梯度下降法 而何时应该使用正规方程法呢？ 这里列举了一些它们的优点和缺点 假如你有 m 个训练样本和 n 个特征变量 假如你有 m 个训练样本和 n 个特征变量 梯度下降法的缺点之一就是 你需要选择学习速率 α 这通常表示需要运行多次 尝试不同的学习速率 α 这通常表示需要运行多次 尝试不同的学习速率 α 然后找到运行效果最好的那个 所以这是一种额外的工作和麻烦 梯度下降法的另一个缺点是 它需要更多次的迭代 因为一些细节 计算可能会更慢 因为一些细节 计算可能会更慢 我们一会儿会看到更多的东西 至于正规方程 你不需要选择学习速率 α 所以就非常方便 也容易实现 你只要运行一下 通常这就够了 并且你也不需要迭代 所以不需要画出 J(θ) 的曲线 所以不需要画出 J(θ 的曲线 来检查收敛性或者采取所有的额外步骤 到目前为止 天平似乎倾向于正规方程法 这里列举一些正规方程法的缺点 和梯度下降法的优点 梯度下降法在有很多特征变量的情况下也能运行地相当好 梯度下降法在有很多特征变量的情况下也能运行地相当好 所以即使你有上百万的特征变量 所以即使你有上百万的特征变量 你可以运行梯度下降法 并且通常很有效 它会正常的运行 相对地 正规方程法 为了求解参数θ 需要求解这一项 为了求解参数θ 需要求解这一项 我们需要计算这项 X转置乘以X的逆 这个 X转置乘以X矩阵 是一个 n*n 的矩阵 如果你有 n 个特征变量的话 因为如果你看一下 X转置乘以X 的维度 因为如果你看一下 X转置乘以X 的维度 因为如果你看一下 X转置乘以X 的维度 你可以发现他们的积的维度 你可以发现他们的积的维度 X转置乘以X 是一个 n*n 的矩阵 X转置乘以X 是一个 n*n 的矩阵 其中 n是特征变量的数量 实现逆矩阵计算所需要的计算量 实现逆矩阵计算所需要的计算量 大致是矩阵维度的三次方 大致是矩阵维度的三次方 因此计算这个逆矩阵需要计算大致 n 的三次方 因此计算这个逆矩阵需要计算大致 n 的三次方 有时稍微比计算 n 的三次方快一些 但是对我们来说很接近 所以如果特征变量的数量 n 很大的话 那么计算这个量会很慢 那么计算这个量会很慢 实际上标准方程法会慢很多 因此如果 n 很大 因此如果 n 很大 我可能还是会使用梯度下降法 因为我们不想花费 n 的三次方的时间 但如果 n 比较小 那么标准方程法可能更好地求解参数 θ 那么怎么叫大或者小呢？ 那么 如果 n 是上百的 那么 如果 n 是上百的 计算百位数乘百位数的矩阵 对于现代计算机来说没有问题 如果 n 是上千的 我还是会使用正规方程法 千位数乘千位数的矩阵做逆变换 对于现代计算机来说实际上是非常快的 但如果 n 上万 那么我可能会开始犹豫 上万乘上万维的矩阵作逆变换 会开始有点慢 此时我可能开始倾向于 此时我可能开始倾向于 梯度下降法 但也不绝对 n 等于一万 你可以 逆变换一个一万乘一万的矩阵 但如果 n 远大于此 我可能就会使用梯度下降法了 所以如果 n 等于10^6 有一百万个特征变量 那么做百万乘百万的矩阵的逆变换 那么做百万乘百万的矩阵的逆变换 就会变得非常费时间 在这种情况下我一定会使用梯度下降法 所以很难给出一个确定的值 来决定何时该换成梯度下降法 来决定何时该换成梯度下降法 但是 对我来说通常是 在一万左右 我会开始考虑换成梯度下降法 在一万左右 我会开始考虑换成梯度下降法 在一万左右 我会开始考虑换成梯度下降法 或者我们将在以后讨论到的其他算法 总结一下 只要特征变量的数目并不大 正规方程是一个很好的 计算参数 θ 的替代方法 具体地说 只要特征变量数量小于一万 具体地说 只要特征变量数量小于一万 我通常使用正规方程法 我通常使用正规方程法 而不使用梯度下降法 预告一下在之后的课程中我们要讲的 预告一下在之后的课程中我们要讲的 随着我们要讲的学习算法越来越复杂 随着我们要讲的学习算法越来越复杂 例如 当我们讲到分类算法 像逻辑回归算法 我们会看到 实际上对于那些算法 并不能使用正规方程法 对于那些更复杂的学习算法 我们将不得不仍然使用梯度下降法 我们将不得不仍然使用梯度下降法 因此 梯度下降法是一个非常有用的算法 可以用在有大量特征变量的线性回归问题 可以用在有大量特征变量的线性回归问题 或者我们以后在课程中 会讲到的一些其他的算法 因为 标准方程法不适合或者不能用在它们上 因为 标准方程法不适合或者不能用在它们上 但对于这个特定的线性回归模型 但对于这个特定的线性回归模型 正规方程法是一个比梯度下降法更快的替代算法 所以 根据具体的问题 所以 根据具体的问题 以及你的特征变量的数量 这两算法都是值得学习的
### 内置习题_正规方程
![内置习题_正规方程](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/32-内置习题_正规方程.jpg)
## English
### Gradient Descent VS Normal Equation
![Gradient Descent VS Normal Equation](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/26-梯度下降VS正规方程.jpg)
In this video, we'll talk about the normal equation, which for some linear regression problems, will give us a much better way to solve for the optimal value of the parameters theta. Concretely, so far the algorithm that we've been using for linear regression is gradient descent where in order to minimize the cost function J of Theta, we would take this iterative algorithm that takes many steps, multiple iterations of gradient descent to converge to the global minimum. In contrast, the normal equation would give us a method to solve for theta analytically, so that rather than needing to run this iterative algorithm, we can instead just solve for the optimal value for theta all at one go, so that in basically one step you get to the optimal value right there.
### Understand Normal Equation in intuition
![Understand Normal Equation in intuition](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/27-从直观上理解正规方程.jpg)
It turns out the normal equation that has some advantages and some disadvantages, but before we get to that and talk about when you should use it, let's get some intuition about what this method does. For this week's planetary example, let's imagine, let's take a very simplified cost function J of Theta, that's just the function of a real number Theta. So, for now, imagine that Theta is just a scalar value or that Theta is just a row value. It's just a number, rather than a vector. Imagine that we have a cost function J that's a quadratic function of this real value parameter Theta, so J of Theta looks like that. Well, how do you minimize a quadratic function? For those of you that know a little bit of calculus, you may know that the way to minimize a function is to take derivatives and to set derivatives equal to zero. So, you take the derivative of J with respect to the parameter of Theta. You get some formula which I am not going to derive, you set that derivative equal to zero, and this allows you to solve for the value of Theda that minimizes J of Theta. That was a simpler case of when data was just real number. In the problem that we are interested in, Theta is no longer just a real number, but, instead, is this n+1-dimensional parameter vector, and, a cost function J is a function of this vector value or Theta 0 through Theta m. And, a cost function looks like this, some square cost function on the right. How do we minimize this cost function J? Calculus actually tells us that, if you, that one way to do so, is to take the partial derivative of J, with respect to every parameter of Theta J in turn, and then, to set all of these to 0. If you do that, and you solve for the values of Theta 0, Theta 1, up to Theta N, then, this would give you that values of Theta to minimize the cost function J. Where, if you actually work through the calculus and work through the solution to the parameters Theta 0 through Theta N, the derivation ends up being somewhat involved. And, what I am going to do in this video, is actually to not go through the derivation, which is kind of long and kind of involved, but what I want to do is just tell you what you need to know in order to implement this process so you can solve for the values of the thetas that corresponds to where the partial derivatives is equal to zero. Or alternatively, or equivalently, the values of Theta is that minimize the cost function J of Theta. I realize that some of the comments I made that made more sense only to those of you that are normally familiar with calculus. So, but if you don't know, if you're less familiar with calculus, don't worry about it. I'm just going to tell you what you need to know in order to implement this algorithm and get it to work.
### Example for analysis Normal Equation
![Example for analysis Normal Equation](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/28-举例分析正规方程.jpg)
For the example that I want to use as a running example let's say that I have m = 4 training examples.In order to implement this normal equation at big, what I'm going to do is the following. I'm going to take my data set, so here are my four training examples. In this case let's assume that, you know, these four examples is all the data I have. What I am going to do is take my data set and add an extra column that corresponds to my extra feature, x0, that is always takes on this value of 1. What I'm going to do is I'm then going to construct a matrix called X that's a matrix are basically contains all of the features from my training data, so completely here is my here are all my features and we're going to take all those numbers and put them into this matrix "X", okay? So just, you know, copy the data over one column at a time and then I am going to do something similar for y's. I am going to take the values that I'm trying to predict and construct now a vector, like so and call that a vector y. So X is going to be a m by (n+1) - dimensional matrix, and Y is going to be a m-dimensional vector where m is the number of training examples and n is, n is a number of features, n+1, because of this extra feature X0 that I had. Finally if you take your matrix X and you take your vector Y, and if you just compute this, and set theta to be equal to X transpose X inverse times X transpose Y, this would give you the value of theta that minimizes your cost function. There was a lot that happened on the slides and I work through it using one specific example of one dataset. Let me just write this out in a slightly more general form and then let me just, and later on in this video let me explain this equation a little bit more.
### General Form of Normal Equation in detail
![General Form of Normal Equation in detail](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/29-正规方程通用形式详细介绍.jpg)
It is not yet entirely clear how to do this. In a general case, let us say we have M training examples so X1, Y1 up to Xn, Yn and n features. So, each of the training example x(i) may looks like a vector like this, that is a n+1 dimensional feature vector. The way I'm going to construct the matrix "X", this is also called the design matrix is as follows. Each training example gives me a feature vector like this. say, sort of n+1 dimensional vector. The way I am going to construct my design matrix x is only construct the matrix like this. and what I'm going to do is take the first training example, so that's a vector, take its transpose so it ends up being this, you know, long flat thing and make x1 transpose the first row of my design matrix. Then I am going to take my second training example, x2, take the transpose of that and put that as the second row of x and so on, down until my last training example. Take the transpose of that, and that's my last row of my matrix X. And, so, that makes my matrix X, an M by N +1 dimensional matrix. As a concrete example, let's say I have only one feature, really, only one feature other than X zero, which is always equal to 1. So if my feature vectors X-i are equal to this 1, which is X-0, then some real feature, like maybe the size of the house, then my design matrix, X, would be equal to this. For the first row, I'm going to basically take this and take its transpose. So, I'm going to end up with 1, and then X-1-1. For the second row, we're going to end up with 1 and then X-1-2 and so on down to 1, and then X-1-M. And thus, this will be a m by 2-dimensional matrix. So, that's how to construct the matrix X. And, the vector Y--sometimes I might write an arrow on top to denote that it is a vector, but very often I'll just write this as Y, either way. The vector Y is obtained by taking all all the labels, all the correct prices of houses in my training set, and just stacking them up into an M-dimensional vector, and that's Y. Finally, having constructed the matrix X and the vector Y, we then just compute theta as X'(1/X) x X'Y. I just want to make I just want to make sure that this equation makes sense to you and that you know how to implement it. So, you know, concretely, what is this X'(1/X)? Well, X'(1/X) is the inverse of the matrix X'X. Concretely, if you were to say set A to be equal to X' x X, so X' is a matrix, X' x X gives you another matrix, and we call that matrix A. Then, you know, X'(1/X) is just you take this matrix A and you invert it, right! This gives, let's say 1/A.And so that's how you compute this thing. You compute X'X and then you compute its inverse.
![General Form of Normal Equation in detail](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/30-正规方程通用形式详细介绍.jpg)
We haven't yet talked about Octave. We'll do so in the later set of videos, but in the Octave programming language or a similar view, and also the matlab programming language is very similar. The command to compute this quantity, X transpose X inverse times X transpose Y, is as follows. In Octave X prime is the notation that you use to denote X transpose. And so, this expression that's boxed in red, that's computing X transpose times X. pinv is a function for computing the inverse of a matrix, so this computes X transpose X inverse, and then you multiply that by X transpose, and you multiply that by Y. So you end computing that formula which I didn't prove, but it is possible to show mathematically even though I'm not going to do so here, that this formula gives you the optimal value of theta in the sense that if you set theta equal to this, that's the value of theta that minimizes the cost function J of theta for the new regression. One last detail in the earlier video. I talked about the feature skill and the idea of getting features to be on similar ranges of Scales of similar ranges of values of each other. If you are using this normal equation method then feature scaling isn't actually necessary and is actually okay if, say, some feature X one is between zero and one, and some feature X two is between ranges from zero to one thousand and some feature x three ranges from zero to ten to the minus five and if you are using the normal equation method this is okay and there is no need to do features scaling, although of course if you are using gradient descent, then, features scaling is still important.
### Gradient Descent VS Normal Equation advantage and disadvantage
![Gradient Descent VS Normal Equation advantage and disadvantage](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/31-梯度下降VS正规方程优缺点.jpg)
Finally, where should you use the gradient descent and when should you use the normal equation method. Here are some of the their advantages and disadvantages. Let's say you have m training examples and n features. One disadvantage of gradient descent is that, you need to choose the learning rate Alpha. And, often, this means running it few times with different learning rate alphas and then seeing what works best. And so that is sort of extra work and extra hassle. Another disadvantage with gradient descent is it needs many more iterations. So, depending on the details, that could make it slower, although there's more to the story as we'll see in a second. As for the normal equation, you don't need to choose any learning rate alpha. So that, you know, makes it really convenient, makes it simple to implement. You just run it and it usually just works. And you don't need to iterate, so, you don't need to plot J of Theta or check the convergence or take all those extra steps. So far, the balance seems to favor normal the normal equation. Here are some disadvantages of the normal equation, and some advantages of gradient descent. Gradient descent works pretty well, even when you have a very large number of features. So, even if you have millions of features you can run gradient descent and it will be reasonably efficient. It will do something reasonable. In contrast to normal equation, In, in order to solve for the parameters data, we need to solve for this term. We need to compute this term, X transpose, X inverse. This matrix X transpose X. That's an n by n matrix, if you have n features. Because, if you look at the dimensions of X transpose the dimension of X, you multiply, figure out what the dimension of the product is, the matrix X transpose X is an n by n matrix where n is the number of features, and for almost computed implementations the cost of inverting the matrix, rose roughly as the cube of the dimension of the matrix. So, computing this inverse costs, roughly order, and cube time. Sometimes, it's slightly faster than N cube but, it's, you know, close enough for our purposes. So if n the number of features is very large,then computing this quantity can be slow and the normal equation method can actually be much slower. So if n is large then I might usually use gradient descent because we don't want to pay this all in q time. But, if n is relatively small, then the normal equation might give you a better way to solve the parameters. What does small and large mean? Well, if n is on the order of a hundred, then inverting a hundred-by-hundred matrix is no problem by modern computing standards. If n is a thousand, I would still use the normal equation method. Inverting a thousand-by-thousand matrix is actually really fast on a modern computer. If n is ten thousand, then I might start to wonder. Inverting a ten-thousand- by-ten-thousand matrix starts to get kind of slow, and I might then start to maybe lean in the direction of gradient descent, but maybe not quite. n equals ten thousand, you can sort of convert a ten-thousand-by-ten-thousand matrix. But if it gets much bigger than that, then, I would probably use gradient descent. So, if n equals ten to the sixth with a million features, then inverting a million-by-million matrix is going to be very expensive, and I would definitely favor gradient descent if you have that many features. So exactly how large set of features has to be before you convert a gradient descent, it's hard to give a strict number. But, for me, it is usually around ten thousand that I might start to consider switching over to gradient descents or maybe, some other algorithms that we'll talk about later in this class. To summarize, so long as the number of features is not too large, the normal equation gives us a great alternative method to solve for the parameter theta. Concretely, so long as the number of features is less than 1000, you know, I would use, I would usually is used in normal equation method rather than, gradient descent. To preview some ideas that we'll talk about later in this course, as we get to the more complex learning algorithm, for example, when we talk about classification algorithm, like a logistic regression algorithm, We'll see that those algorithm actually... The normal equation method actually do not work for those more sophisticated learning algorithms, and, we will have to resort to gradient descent for those algorithms. So, gradient descent is a very useful algorithm to know. The linear regression will have a large number of features and for some of the other algorithms that we'll see in this course, because, for them, the normal equation method just doesn't apply and doesn't work. But for this specific model of linear regression, the normal equation can give you a alternative that can be much faster, than gradient descent. So, depending on the detail of your algortithm, depending of the detail of the problems and how many features that you have, both of these algorithms are well worth knowing about.
### Exam_in the video
![Exam_Normal Equation](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/32-内置习题_正规方程.jpg)
