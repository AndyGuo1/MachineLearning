# 正规方程的不可逆性
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/02-Week2/1 Linear Regression with Multiple Variables/7-Normal Equation Noninvertibility.mp4" type="video/mp4">
</video>
## 中文
### 正规方程不可逆问题
**注意：正规方程不可逆问题指的是:X'X的结果形成的新矩阵是不可逆矩阵(奇异矩阵、退化矩阵)**
![正规方程不可逆问题](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/34-正规方程不可逆问题.jpg)
在这段视频中我想谈谈正规方程 ( normal equation ) 以及它们的不可逆性 由于这是一种较为深入的概念 并且总有人问我有关这方面的问题 因此 我想在这里来讨论它 由于概念较为深入 所以对这段可选材料大家放轻松吧 也许你可能会深入地探索下去 并且会觉得理解以后会非常有用但即使你没有理解正规方程和线性回归的关系 也没有关系 我们要讲的问题如下 你或许可能对 线性代数比较熟悉 有些同学曾经问过我 当计算 θ等于inv(X'X ) X'y （注：X的转置翻译为X'，下同） 那对于矩阵X'X的结果是不可逆的情况咋办呢? 如果你懂一点线性代数的知识 你或许会知道 有些矩阵可逆 而有些矩阵不可逆 我们称那些不可逆矩阵为 奇异或退化矩阵 问题的重点在于X'X的不可逆的问题 很少发生 在Octave里 如果你用它来实现θ的计算 你将会得到一个正常的解 在这里我不想赘述 在Octave里 有两个函数可以求解矩阵的逆 一个被称为pinv() 另一个是inv() 这两者之间的差异是些许计算过程上的 一个是所谓的伪逆 另一个被称为逆 使用pinv() 函数可以展现数学上的过程 这将计算出θ的值 即便矩阵X'X是不可逆的 在pinv() 和 inv() 之间 又有哪些具体区别呢 ? 其中inv() 引入了先进的数值计算的概念 我真的不希望讲那些 因此 我认为 可以试着给你一点点直观的参考 关于矩阵X'X的不可逆的问题 如果你懂一点线性代数 或许你可能会感兴趣 我不会从数学的角度来证明它
## 矩阵X'X的结果是不可逆矩阵的两个常见原因
![矩阵X'X的结果是不可逆矩阵的两个常见原因](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/35-矩阵X'X的结果是不可逆矩阵的两个常见原因.jpg)
但如果矩阵X'X结果是不可逆的 通常有两种最常见的原因 第一个原因是 如果不知何故 在你的学习问题 你有多余的功能 例如 在预测住房价格时 如果x1是以英尺为尺寸规格计算的房子 x2是以平方米为尺寸规格计算的房子 同时 你也知道1米等于3.28英尺 ( 四舍五入到两位小数 ) 这样 你的这两个特征值将始终满足约束 x1等于x2倍的3.28平方 并且你可以将这过程显示出来 讲到这里 可能 或许对你来说有点难了 但如果你在线性代数上非常熟练 实际上 你可以用这样的一个线性方程 来展示那两个相关联的特征值 矩阵X'X将是不可逆的 第二个原因是 在你想用大量的特征值 尝试实践你的学习算法的时候 可能会导致矩阵X'X的结果是不可逆的 具体地说 在m小于或等于n的时候 例如 有m等于10个的训练样本 也有n等于100的特征数量 要找到适合的 ( n +1 ) 维参数矢量θ 这是第 n+1 维 这将会变成一个101维的矢量 尝试从10个训练样本中找到满足101个参数的值 这工作可能会让你花上一阵子时间 但这并不总是一个好主意 因为 正如我们所看到 你只有10个样本 以适应这100或101个参数 数据还是有些少 稍后我们将看到 如何使用小数据样本以得到这100或101个参数 通常 我们会使用一种叫做正则化的线性代数方法 通过删除某些特征或者是使用某些技术 来解决当m比n小的时候的问题 这也是在本节课后面要讲到的内容 即使你有一个相对较小的训练集 也可使用很多的特征来找到很多合适的参数 有关正规化的内容将是本节之后课程的话题 总之当你发现的矩阵X'X的结果是奇异矩阵 或者找到的其它矩阵是不可逆的 我会建议你这么做 首先 看特征值里是否有一些多余的特征 像这些x1和x2是线性相关的 或像这样 互为线性函数 同时 当有一些多余的特征时 可以删除这两个重复特征里的其中一个 无须两个特征同时保留 所以 发现多余的特征删除二者其一 将解决不可逆性的问题 因此 首先应该通过观察所有特征检查是否有多余的特征 如果有多余的就删除掉
## English
### Normal equation about non-invertibility
![Normal equation about non-invertibility](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/34-正规方程不可逆问题.jpg)
In this video I want to talk about the Normal equation and non-invertibility. This is a somewhat more advanced concept, but it's something that I've often been asked about. And so I want to talk it here and address it here. But this is a somewhat more advanced concept, so feel free to consider this optional material.And there's a phenomenon that you may run into that may be somewhat useful to understand, but even if you don't understand the normal equation and linear progression, you should really get that to work okay.Here's the issue.For those of you there are, maybe some are more familiar with linear algebra, what some students have asked me is, when computing this Theta equals X transpose X inverse X transpose Y. What if the matrix X transpose X is non-invertible? So for those of you that know a bit more linear algebra you may know that only some matrices are invertible and some matrices do not have an inverse we call those non-invertible matrices. Singular or degenerate matrices. The issue or the problem of x transpose x being non invertible should happen pretty rarely. And in Octave if you implement this to compute theta, it turns out that this will actually do the right thing. I'm getting a little technical now, and I don't want to go into the details, but Octave hast two functions for inverting matrices. One is called pinv, and the other is called inv. And the differences between these two are somewhat technical. One's called the pseudo-inverse, one's called the inverse. But you can show mathematically that so long as you use the pinv function then this will actually compute the value of data that you want even if X transpose X is non-invertible. The specific details between inv. What is the difference between pinv? What is inv? That's somewhat advanced numerical computing concepts, I don't really want to get into. But I thought in this optional video, I'll try to give you little bit of intuition about what it means for X transpose X to be non-invertible. For those of you that know a bit more linear Algebra might be interested.
## Two common reasons about the matrix of X'X is non-invertible
![Two common reasons about the matrix of X'X is non-invertible](amWiki/images/001/02-Week2/1 Linear Regression with Multiple Variables/35-矩阵X'X的结果是不可逆矩阵的两个常见原因.jpg)
I'm not gonna prove this mathematically but if X transpose X is non-invertible, there usually two most common causes for this. The first cause is if somehow in your learning problem you have redundant features. Concretely, if you're trying to predict housing prices and if x1 is the size of the house in feet, in square feet and x2 is the size of the house in square meters, then you know 1 meter is equal to 3.28 feet Rounded to two decimals. And so your two features will always satisfy the constraint x1 equals 3.28 squared times x2. And you can show for those of you that are somewhat advanced in linear Algebra, but if you're explaining the algebra you can actually show that if your two features are related, are a linear equation like this. Then matrix X transpose X would be non-invertable. The second thing that can cause X transpose X to be non-invertable is if you are training, if you are trying to run the learning algorithm with a lot of features. Concretely, if m is less than or equal to n. For example, if you imagine that you have m = 10 training examples that you have n equals 100 features then you're trying to fit a parameter back to theta which is, you know, n plus one dimensional. So this is 101 dimensional, you're trying to fit 101 parameters from just 10 training examples.This turns out to sometimes work but not always be a good idea. Because as we'll see later, you might not have enough data if you only have 10 examples to fit you know, 100 or 101 parameters. We'll see later in this course why this might be too little data to fit this many parameters. But commonly what we do then if m is less than n, is to see if we can either delete some features or to use a technique called regularization which is something that we'll talk about later in this class as well, that will kind of let you fit a lot of parameters, use a lot features, even if you have a relatively small training set. But this regularization will be a later topic in this course. But to summarize if ever you find that x transpose x is singular or alternatively you find it non-invertable, what I would recommend you do is first look at your features and see if you have redundant features like this x1, x2. You're being linearly dependent or being a linear function of each other like so. And if you do have redundant features and if you just delete one of these features, you really don't need both of these features. If you just delete one of these features, that would solve your non-invertibility problem. And so I would first think through my features and check if any are redundant. And if so then keep deleting redundant features until they're no longer redundant. And if your features are not redundant, I would check if I may have too many features. And if that's the case, I would either delete some features if I can bear to use fewer features or else I would consider using regularization. Which is this topic that we'll talk about later.So that's it for the normal equation and what it means for if the matrix X transpose X is non-invertable but this is a problem that you should run that hopefully you run into pretty rarely and if you just implement it in octave using P and using the P n function which is called a pseudo inverse function so you could use a different linear out your alive in Is called a pseudo-inverse but that implementation should just do the right thing, even if X transpose X is non-invertable, which should happen pretty rarely anyways, so this should not be a problem for most implementations of linear regression.
