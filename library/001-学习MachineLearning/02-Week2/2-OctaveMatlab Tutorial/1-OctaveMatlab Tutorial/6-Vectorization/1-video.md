# Octave-向量化
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/02-Week2/2 OctaveMatlab Tutorial OctaveMatlab/6-Vectorization.mp4" type="video/mp4">
</video>
## 中文
在这段视频中 我将介绍有关向量化的内容 无论你是用Octave 还是别的语言 比如MATLAB 或者你正在用Python NumPy 或 Java C C++ 所有这些语言都具有 各种线性代数库 这些库文件都是内置的 容易阅读和获取 他们通常写得很好 已经经过高度优化 通常是数值计算方面的博士 或者专业人士开发的 而当你实现机器学习算法时 如果你能 好好利用这些 线性代数库或者说 数值线性代数库 并联合调用它们 而不是自己去做那些 函数库可以做的事情 如果是这样的话 那么 通常你会发现 首先 这样更有效 也就是说运行速度更快 并且更好地利用 你的计算机里可能有的一些并行硬件系统 等等 第二 这也意味着 你可以用更少的代码来实现你需要的功能 因此 实现的方式更简单 代码出现问题的有可能性也就越小 举个具体的例子 与其自己写代码 做矩阵乘法 如果你只在Octave中 输入 a乘以b 就是一个非常有效的 两个矩阵相乘的程序 有很多例子可以说明 如果你用合适的向量化方法来实现 你就会有一个简单得多 也有效得多的代码
### 举例_线性回归假设函数向量化和非向量化实现对比
![举例_线性回归假设函数向量化和非向量化实现对比](amWiki/images/001/02-Week2/2 OctaveMatlab Tutorial OctaveMatlab/1-举例_线性回归假设函数向量化和非向量化实现对比.jpg)
让我们来看一些例子 这是一个常见的线性回归假设函数 如果 你想要计算 h(x) 注意到右边是求和 那么你可以 自己计算 j =0 到 j = n 的和 但换另一种方式来想想 是把 h(x) 看作 θ 转置乘以 x 那么 你就可以写成 两个向量的内积 其中 θ 就是 θ0 θ1 θ2 如果你有两个特征量 如果 n 等于2 并且如果 你把 x 看作 x0 x1 x2 这两种思考角度 会给你两种不同的实现方式 比如说 这是未向量化的代码实现方式 计算 h(x) 是未向量化的 我的意思是 没有被向量化 我们可能首先要初始化变量 prediction 的值为0.0 而这个 变量 prediction 的最终结果就是 h(x) 然后变量 prediction 的最终结果就是 j 取值 0 到 n+1 变量prediction 每次就通过 自身加上 theta(j) 乘以 x(j) 更新值 这个就是算法的代码实现 顺便我要提醒一下 这里的向量 我用的下标是 0 所以我有 θ0 θ1 θ2 但因为 MATLAB 的下标从1开始 在 MATLAB 中 θ0 我们可能会 用 theta(1) 来表示 这第二个元素 最后就会变成 theta(2) 而第三个元素 最终可能就用 theta(3) 表示 因为 MATLAB 中的下标从1开始 即使我们实际的θ 和 x 的下标从0开始 这就是为什么 这里我的 for 循环 j 取值从 1 直到 n+1 而不是 从 0 到 n 清楚了吗？ 但这是一个 未向量化的代码实现方式 我们用一个 for 循环 对 n 个元素进行加和 作为比较 接下来是 向量化的代码实现 你把 x 和 θ 看做向量 而你只需要 令变量 prediction 等于 theta转置 乘以 x 你就可以这样计算 与其写所有这些 for 循环的代码 你只需要一行代码 这行代码 右边所做的 就是 利用 Octave 的高度优化的数值 线性代数算法来计算 两个向量的内积 θ 以及 x 这样向量化的实现不仅仅是更简单 它运行起来也将更加高效 这就是 Octave 所做的
### 举例_线性回归假设函数向量化和非向量化C++实现方式对比
![举例_线性回归假设函数向量化和非向量化C++实现方式对比](amWiki/images/001/02-Week2/2 OctaveMatlab Tutorial OctaveMatlab/2-举例_线性回归假设函数向量化和非向量化C++实现方式对比.jpg)
而向量化的方法 在其他编程语言中同样可以实现 让我们来看一个 C++ 的例子 这就是未向量化的代码实现的样子 我们再次初始化变量 prediction 为 0.0 然后我们现在有一个完整的 从 j 等于 0 直到 n 变量 prediction += theta[j] 乘以 x[j] 再一次 你有这样的自己写的 for 循环 与此相反 使用一个比较好的 C++ 数值线性代数库 你就可以用这个方程 来写这个函数 与此相反 使用较好的 C++ 数值线性代数库 你可以写出 像这样的代码 因此取决于你的 数值线性代数库的内容 你可以有一个对象 (object) 像这个 C++ 对象 theta 和一个 C++ 对象 向量 x 你只需要用 theta.transpose ( ) 乘以 x 而这次是让 C++ 来实现运算 因此 你只需要在 C++ 中将两个向量相乘 根据你所使用的 数值和线性代数库的使用细节的不同 你最终使用的代码表达方式 可能会有些许不同 但是通过一个库来做内积 你可以得到一段更简单 更有效的代码
### 举例_线性回归算法梯度下降向量化实现
现在 让我们来看一个更为复杂的例子 提醒一下 这是线性回归算法梯度下降的更新规则所以 我们用这条规则对 j 等于 0 1 2 等等的所有值 更新 对象 θj 我只是 用 θ0 θ1 θ2 来写方程 那就是假设我们有两个特征量 所以 n等于2 这些都是我们需要对 θ0 θ1 θ2 进行更新 你可能还记得 在以前的视频中说过 这些都应该是同步更新
![举例_线性回归算法梯度下降向量化实现](amWiki/images/001/02-Week2/2 OctaveMatlab Tutorial OctaveMatlab/3-举例_线性回归算法梯度下降向量化注意同步更新.jpg)
因此 让我们来看看 我们是否可以拿出一个 向量化的代码实现 这里是和之前相同的三个方程 只不过写得小一点而已 你可以想象 实现这三个方程的方式之一 就是用 一个 for 循环 就是让 j 等于0 等于 等于2 来更新 θj 但让我们 用向量化的方式来实现 看看我们是否能够有一个更简单的方法 基本上用三行代码 或者一个 for 循环
![举例_线性回归算法梯度下降向量化实现](amWiki/images/001/02-Week2/2 OctaveMatlab Tutorial OctaveMatlab/4-举例_线性回归算法梯度下降向量化实现.jpg)
### 内置习题
![内嵌习题_向量化](amWiki/images/001/02-Week2/2 OctaveMatlab Tutorial OctaveMatlab/5-内嵌习题_向量化.jpg)
## English
In this video I like to tell you about the idea of Vectorization. So, whether you using Octave or a similar language like MATLAB or whether you're using Python [INAUDIBLE], R, Java, C++, all of these languages have either built into them or have regularly and easily accessible difference in numerical linear algebra libraries. They're usually very well written, highly optimized, often sort of developed by people that have PhDs in numerical computing or they're really specialized in numerical computing. And when you're implementing machine learning algorithms, if you're able to take advantage of these linear algebra libraries or these numerical linear algebra libraries, and make some routine calls to them rather than sort of write code yourself to do things that these libraries could be doing. If you do that, then often you get code that, first, is more efficient, so you just run more quickly and take better advantage of any parallel hardware your computer may have and so on. And second, it also means that you end up with less code that you need to write, so it's a simpler implementation that is therefore maybe also more likely to be by free. And as a concrete example, rather than writing code yourself to multiply matrices, if you let Octave do it by typing a times b, that would use a very efficient routine to multiply the two matrices. And there's a bunch of examples like these, where if you use appropriate vectorization implementations you get much simpler code and much more efficient code.
### Example_Compare Vectorization or NonVectorization of Hypothesis Function for Linear Regression
![Example_Compare Vectorization or NonVectorization of Hypothesis Function for Linear Regression](amWiki/images/001/02-Week2/2 OctaveMatlab Tutorial OctaveMatlab/1-举例_线性回归假设函数向量化和非向量化实现对比.jpg)
Let's look at some examples.Here's our usual hypothesis for linear regression, and if you want to compute h(x), notice that there's a sum on the right. And so one thing you could do is, compute the sum from j = 0 to j = n yourself. Another way to think of this is to think of h(x) as theta transpose x, and what you can do is, think of this as you are computing this inner product between two vectors where theta is your vector, say, theta 0, theta 1, theta 2. If you have two features, if n equals two, and if you think x as this vector, x0, x1, x2, and these two views can give you two different implementations. Here's what I mean. Here's an unvectorized implementation for how to compute and by unvectorize, I mean without vectorization. We might first initialize prediction just to be 0.0. The prediction's going to eventually be h(x), and then I'm going to have a for loop for j=1 through n+1, prediction gets incremented by theta(j) * x(j). So it's kind of this expression over here. By the way, I should mention, in these vectors that I wrote over here, I had these vectors being 0 index. So I had theta 0, theta 1, theta 2. But because MATLAB is one index, theta 0 in that MATLAB, we would end up representing as theta 1 and the second element ends up as theta 2 and this third element may end up as theta 3, just because our vectors in MATLAB are indexed starting from 1, even though I wrote theta and x here, starting indexing from 0, which is why here I have a for loop. j goes from 1 through n+1 rather than j goes through 0 up to n, right? But so this is an unvectorized implementation in that we have for loop that is summing up the n elements of the sum. In contrast, here's how you would write a vectorized implementation, which is that you would think of a x and theta as vectors. You just said prediction = theta' * x. You're just computing like so. So instead of writing all these lines of code with a for loop, you instead just have one line of code. And what this line of code on the right will do is, it will use Octaves highly optimized numerical linear algebra routines to compute this inner product between the two vectors, theta and X, and not only is the vectorized implementation simpler, it will also run much more efficiently.So that was octave, but the issue of vectorization applies to other programming language as well.
### Example_Compare Vectorization or NonVectorization of Hypothesis Function for Linear Regression(C++ impl)
![Example_Compare Vectorization or NonVectorization of Hypothesis Function for Linear Regression(C++ impl)](amWiki/images/001/02-Week2/2 OctaveMatlab Tutorial OctaveMatlab/2-举例_线性回归假设函数向量化和非向量化C++实现方式对比.jpg)
Lets look on the example in C++. Here's what an unvectorized implementation might look like. We again initialize prediction to 0.0 and then we now how a for loop for j = 0 up to n. Prediction += theta j * x[j], where again, you have this explicit for loop that you write yourself. In contrast, using a good numerical linear algebra library in C++, you could write a function like, or rather.In contrast, using a good numerical linear algebra library in C++, you can instead write code that might look like this. So depending on the details of your numerical linear algebra library, you might be able to have an object, this is a C++ object, which is vector theta, and a C++ object which is vector x, and you just take theta.transpose * x, where this times becomes a C++ sort of overload operator so you can just multiply these two vectors in C++. And depending on the details of your numerical linear algebra library, you might end up using a slightly different syntax, but by relying on the library to do this inner product, you can get a much simpler piece of code and a much more efficient one.
### Example_Gradient Descent for Linear Regression Vectorization
Let's now look at a more sophisticated example. Just to remind you, here's our update rule for a gradient descent of a linear regression. And so we update theta j using this rule for all values of j = 0, 1, 2, and so on. And if I just write out these equations for theta 0, theta 1, theta 2, assuming we have two features, so n = 2. Then these are the updates we perform for theta 0, theta 1, theta 2, where you might remember my saying in an earlier video, that these should be simultaneous updates.
![Example_Gradient Descent for Linear Regression Vectorization](amWiki/images/001/02-Week2/2 OctaveMatlab Tutorial OctaveMatlab/3-举例_线性回归算法梯度下降向量化注意同步更新.jpg)
So, let's see if we can come up with a vectorizing notation of this.Here are my same three equations written in a slightly smaller font, and you can imagine that one way to implement these three lines of code is to have a for loop that says for j = 0, 1 through 2 to update theta j, or something like that. But instead, let's come up with a vectorized implementation and see if we can have a simpler way to basically compress these three lines of code or a for loop that effectively does these three steps one set at a time. Let's see if we can take these three steps and compress them into one line of vectorized code. Here's the idea. What I'm going to do is, I'm going to think of theta as a vector, and I'm gonna update theta as theta- alpha times some other vector delta, where delta's is going to be equal to 1 over m, sum from i = 1 through m. And then this term over on the right, okay? So, let me explain what's going on here.Here, I'm going to treat theta as a vector, so this is n plus one dimensional vector, and I'm saying that theta gets here updated as that's a vector, Rn + 1. Alpha is a real number, and delta, here is a vector. So, this subtraction operation, that's a vector subtraction, okay? Cuz alpha times delta is a vector, and so I'm saying theta gets this vector, alpha times delta subtracted from it. So, what is a vector delta? Well this vector delta, looks like this, and what it's meant to be is really meant to be this thing over here. Concretely, delta will be a n plus one dimensional vector, and the very first element of the vector delta is going to be equal to that. So, if we have the delta, if we index it from 0, if it's delta 0, delta 1, delta 2, what I want is that delta 0 is equal to this first box in green up above. And indeed, you might be able to convince yourself that delta 0 is this 1 of the m sum of ho(x), x(i) minus y(i) times x(i) 0. So, let's just make sure we're on this same page about how delta really is computed. Delta is 1 over m times this sum over here, and what is this sum? Well, this term over here, that's a real number, and the second term over here, x i, this term over there is a vector, right, because x(i) may be a vector that would be, say, x(i)0, x(i)1, x(i)2, right, and what is the summation? Well, what the summation is saying is that, this term, that is this term over here, this is equal to, (h of(x(1))- y(1)) * x(1) + (h of(x(2))- y(2) x x(2) +, and so on, okay? Because this is summation of i, so as i ranges from i = 1 through m, you get these different terms, and you're summing up these terms here. And the meaning of these terms, this is a lot like if you remember actually from the earlier quiz in this, right, you saw this equation. We said that in order to vectorize this code we will instead said u = 2v + 5w. So we're saying that the vector u is equal to two times the vector v plus five times the vector w. So this is an example of how to add different vectors and this summation's the same thing. This is saying that the summation over here is just some real number, right? That's kinda like the number two or some other number times the vector, x1. So it's kinda like 2v or say some other number times x1, and then plus instead of 5w we instead have some other real number, plus some other vector, and then you add on other vectors, plus dot, dot, dot, plus the other vectors, which is why, over all, this thing over here, that whole quantity, that delta is just some vector.And concretely, the three elements of delta correspond if n = 2, the three elements of delta correspond exactly to this thing, to the second thing, and this third thing. Which is why when you update theta according to theta- alpha delta, we end up carrying exactly the same simultaneous updates as the update rules that we have up top.So, I know that there was a lot that happened on this slide, but again, feel free to pause the video and if you aren't sure what just happened I'd encourage you to step through this slide to make sure you understand why is it that this update here with this definition of delta, right, why is it that that's equal to this update on top? And if it's still not clear, one insight is that, this thing over here, that's exactly the vector x, and so we're just taking all three of these computations, and compressing them into one step with this vector delta, which is why we can come up with a vectorized implementation of this step of the new refresh in this way.So, I hope this step makes sense and do look at the video and see if you can understand it. In case you don't understand quite the equivalence of this map, if you implement this, this turns out to be the right answer anyway. So, even if you didn't quite understand equivalence, if you just implement it this way, you'll be able to get linear regression to work. But if you're able to figure out why these two steps are equivalent, then hopefully that will give you a better understanding of vectorization as well. And finally, if you are implementing linear regression using more than one or two features, so sometimes we use linear regression with 10's or 100's or 1,000's of features. But if you use the vectorized implementation of linear regression, you'll see that will run much faster than if you had, say, your old for loop that was updating theta zero, then theta one, then theta two yourself. So, using a vectorized implementation, you should be able to get a much more efficient implementation of linear regression. And when you vectorize later algorithms that we'll see in this class, there's good trick, whether in Octave or some other language like C++, Java, for getting your code to run more efficiently.
![Example_Gradient Descent for Linear Regression Vectorization](amWiki/images/001/02-Week2/2 OctaveMatlab Tutorial OctaveMatlab/4-举例_线性回归算法梯度下降向量化实现.jpg)
### Exam_in the video
![Exam_Vectorization](amWiki/images/001/02-Week2/2 OctaveMatlab Tutorial OctaveMatlab/5-内嵌习题_向量化.jpg)
