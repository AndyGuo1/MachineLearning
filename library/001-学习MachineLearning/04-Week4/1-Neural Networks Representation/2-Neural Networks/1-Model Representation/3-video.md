# 神经网络表达-模型表达II
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/04-Week4/1-Neural Networks Representation/4-Model Representation II.mp4" type="video/mp4">
</video>
## 中文
### 人工神经网络正向传播模型-向量化实现
![人工神经网络正向传播模型-向量化实现](amWiki/images/001/04-Week4/1-Neural Networks Representation/19-人工神经网络正向传播模型-向量化实现.jpg)  
在前面的视频里 我们 解释了怎样用数学来 定义或者计算 神经网络算法的假设 在这段视频中 我想 告诉你如何 高效地进行计算 并展示一个向量化的实现方法 更重要的是 我想 让你们明白为什么 这样表示神经网络 是一个好的方法 并且明白 它们怎样帮助我们学习复杂的非线性假设 以这个神经网络为例 以前我们说 计算出假设输出 的步骤 是左边的这些 方程 通过这些方程 我们计算出 三个隐藏单元的激励值 然后利用 这些值来计算 假设h(x)的最终输出 接下来 我要 定义一些额外的项 因此 这里 我画线的项 把它定义为 z上标(2) 下标1 这样一来 就有了 a(2)1 这个项 等于 g(z(2)1) 另外顺便提一下 这些上标2 的意思是 在z(2)和a(2)中 括号中的 2表示这些值 与第二层相关 即与神经网络中的 隐藏层有关 接下来 这里的项 我将同样定义为z(2)2 最后这个 我画线的项 我把它定义为z(2)3 这样 我们有a(2)3 等于g(z(2)3) 所以这些z值都是 一个线性组合 是输入值x0 x1 x2 x3的 加权线性组合 它将会进入一个特定的神经元 现在 看一下 这一堆数字 你可能会注意到这块 对应了 矩阵向量运算 类似于矩阵向量乘法 x1乘以向量x 观察到一点 我们就能将 神经网络的计算向量化了 具体而言 我们定义 特征向量x 为x0 x1 x2 x3组成的向量 其中x0 仍然等于1 并定义 z(2)为 这些z值组成的向量 即z(2)1 z(2)2 z(2)3 注意 在这里 z(2) 是一个三维向量 下面 我们可以这样向量化a(2)1 a(2)2 a(2)3的计算 我们只用两个步骤 z(2)等于θ(1) 乘以x 这样就有了向量z(2) 然后 a(2)等于 g(z(2)) 需要明白 这里的z(2)是 三维向量 并且 a(2)也是一个三维 向量 因此这 里的激励g 将s函数 逐元素作用于 z(2)中的每个元素 顺便说一下 为了让我们 的符号和接下来的 工作相一致 在输入层 虽然我们有 输入x 但我们 还可以把这些想成 是第一层的激励 所以 我可以定义a(1) 等于x 因此 a(1)就是一个向量了 我就可以把这里的x 替换成a(1) z(2)就等于θ(1)乘以a(1) 这都是通过在输入层定义a(1)做到的 现在 就我目前所写的 我得到了 a1 a2 a3的值 并且 我应该把 上标加上去 但我还需要一个值 我同样需要这个a(2)0 它对应于 隐藏层的 得到这个输出的偏置单元 当然 这里也有一个 偏置单元 我只是没有 画出来 为了 注意这额外的偏置单元 接下来我们 要额外加上一个a0 上标(2) 它等于1 这样一来 现在 a(2)就是一个 四维的特征向量 因为我们刚添加了 这个额外的 a0 它等于 1并且它是隐藏层的 一个偏置单元 最后 为了计算假设的 实际输出值 我们 只需要计算z(3) z(3)等于 这里我画线的项 这个方框里的项就是z(3) z(3)等于θ(2) 乘以a(2) 最后 假设输出为h(x) 它等于a(3) a(3)是输出层 唯一的单元 它是一个实数 你可以写成a(3) 或a(3)1 这就是g(z(3)) 这个计算h(x)的过程 也称为前向传播(forward propagation) 这样命名是因为 我们从 输入层的激励开始 然后进行前向传播给 隐藏层并计算 隐藏层的激励 然后 我们继续前向传播 并计算输出层的激励 这个从输入层到 隐藏层再到输出层依次计算激励的 过程叫前向传播 我们刚刚得到了 这一过程的向量化 实现方法 如果你 使用右边这些公式实现它 就会得到 一个有效的 计算h(x) 的方法 这种前向传播的角度 也可以帮助我们了解 神经网络的原理 和它为什么能够 帮助我们学习非线性假设
### 类比神经网络算法和逻辑回归算法
![类比神经网络算法和逻辑回归算法](amWiki/images/001/04-Week4/1-Neural Networks Representation/20-类比神经网络算法和逻辑回归算法.jpg)  
看一下这个神经网络 我会暂时盖住 图片的左边部分 如果你观察图中剩下的部分 这看起来很像 逻辑回归 在逻辑回归中 我们用 这个节点 即 这个逻辑回归单元 来预测 h(x)的值 具体来说 假设输出的 h(x)将 等于s型激励函数 g(θ0 xa0 +θ1xa1 +θ2xa2 +θ3xa3) 其中 a1 a2 a3 由这三个单元给出 为了和我之前的定义 保持一致 需要 在这里 还有这些地方都填上上标(2) 同样还要加上这些下标1 因为我只有 一个输出单元 但如果你只观察蓝色的部分 这看起来 非常像标准的 逻辑回归模型 不同之处在于 我现在用的是大写的θ 而不是小写的θ 这样做完 我们只得到了逻辑回归 但是 逻辑回归的 输入特征值 是通过隐藏层计算的 再说一遍 神经网络所做的 就像逻辑回归 但是它 不是使用 x1 x2 x3作为输入特征 而是用a1 a2 a3作为新的输入特征 同样 我们需要把 上标加上来和之前的记号保持一致 有趣的是 特征项a1 a2 a3它们是作为 输入的函数来学习的 具体来说 就是从第一层 映射到第二层的函数 这个函数由其他 一组参数θ(1)决定 所以 在神经网络中 它没有用 输入特征x1 x2 x3 来训练逻辑回归 而是自己 训练逻辑回归 的输入 a1 a2 a3 可以想象 如果 在θ1中选择不同的参数 有时可以学习到一些 很有趣和复杂的特征 就可以得到一个 更好的假设 比使用原始输入 x1 x2或x3时得到的假设更好 你也可以 选择多项式项 x1 x2 x3等作为输入项 但这个算法可以 灵活地 快速学习任意的特征项 把这些a1 a2 a3 输入这个 最后的单元 实际上 它是逻辑回归 我觉得现在描述的这个例子 有点高端 所以 我不知道 你是否能理解 这个具有更复杂特征项的 神经网络 但是 如果你没理解 在接下来的两个视频里 我会讲解一个具体的例子 它描述了怎样用神经网络 如何利用这个隐藏层 计算更复杂的特征 并输入到最后的输出层 以及为什么这样就可以学习更复杂的假设 所以 如果我 现在讲的 你没理解 请继续 观看接下来的两个视频 希望它们 提供的例子能够 让你更加理解神经网络 但有一点 你还可以用其他类型的图来 表示神经网络 神经网络中神经元 相连接的方式 称为神经网络的架构 所以说 架构是指 不同的神经元是如何相互连接的
### 另一种人工神经网络架构
![另一种人工神经网络架构](amWiki/images/001/04-Week4/1-Neural Networks Representation/21-另一种人工神经网络架构.jpg)  
不同的神经元是如何相互连接的 这里有一个不同的 神经网络架构的例子 你可以 意识到这个第二层 是如何工作的 在这里 我们有三个隐藏单元 它们根据输入层 计算一个复杂的函数 然后第三层 可以将第二层 训练出的特征项作为输入 并在第三层计算一些更复杂的函数 这样 在你到达 输出层之前 即第四层 就可以利用第三层 训练出的更复杂的 特征项作为输入 以此得到非常有趣的非线性假设 顺便说一下 在这样的 网络里 第一层 被称为输入层 第四层 仍然是我们的输出层 这个网络有两个隐藏层 所以 任何一个不是 输入层或输出层的 都被称为隐藏层 我希望从这个视频中 你已经大致理解 前向传播在 神经网络里的工作原理： 从输入层的激励 开始 向前 传播到 第一隐藏层 然后传播到第二 隐藏层 最终到达输出层 并且你也知道了如何 向量化这些计算 我发现 这个视频里我讲了 某些层是如何 计算前面层的复杂特征项 我意识到这可能 仍然有点抽象 显得比较高端 所以 我将 在接下来的两个视频中 讨论具体的例子 它描述了怎样用神经网络 来计算 输入的非线性函数 希望能使你 更好的理解 从神经网络中得到的复杂非线性假设 【教育无边界字幕组】翻译:午后阳光 校对:sherry 审核:所羅門捷列夫
### 内置习题
![内置习题_理解人工神经网络计算](amWiki/images/001/04-Week4/1-Neural Networks Representation/22-内置习题_理解人工神经网络计算.jpg)
## English
### Artificial Neurons Networks Forward Propagation Mode-Vectorization implement
![Artificial Neurons Networks Forward Propagation Mode-Vectorization implement](amWiki/images/001/04-Week4/1-Neural Networks Representation/19-人工神经网络正向传播模型-向量化实现.jpg)  
In the last video, we gave a mathematical definition of how to represent or how to compute the hypotheses used by Neural Network.In this video, I like show you how to actually carry out that computation efficiently, and that is show you a vector rise implementation.And second, and more importantly, I want to start giving you intuition about why these neural network representations might be a good idea and how they can help us to learn complex nonlinear hypotheses.Consider this neural network. Previously we said that the sequence of steps that we need in order to compute the output of a hypotheses is these equations given on the left where we compute the activation values of the three hidden uses and then we use those to compute the final output of our hypotheses h of x. Now, I'm going to define a few extra terms. So, this term that I'm underlining here, I'm going to define that to be z superscript 2 subscript 1. So that we have that a(2)1, which is this term is equal to g of z to 1. And by the way, these superscript 2, you know, what that means is that the z2 and this a2 as well, the superscript 2 in parentheses means that these are values associated with layer 2, that is with the hidden layer in the neural network.Now this term here I'm going to similarly define as z(2)2. And finally, this last term here that I'm underlining,let me define that as z(2)3. So that similarly we have a(2)3 equals g of z(2)3. So these z values are just a linear combination, a weighted linear combination, of the input values x0, x1, x2, x3 that go into a particular neuron.Now if you look at this block of numbers,you may notice that that block of numbers corresponds suspiciously similar to the matrix vector operation, matrix vector multiplication of x1 times the vector x. Using this observation we're going to be able to vectorize this computation of the neural network.Concretely, let's define the feature vector x as usual to be the vector of x0, x1, x2, x3 where x0 as usual is always equal 1 and that defines z2 to be the vector of these z-values, you know, of z(2)1 z(2)2, z(2)3.And notice that, there, z2 this is a three dimensional vector.We can now vectorize the computation of a(2)1, a(2)2, a(2)3 as follows. We can just write this in two steps. We can compute z2 as theta 1 times x and that would give us this vector z2; and then a2 is g of z2 and just to be clear z2 here, This is a three-dimensional vector and a2 is also a three-dimensional vector and thus this activation g. This applies the sigmoid function element-wise to each of the z2's elements. And by the way, to make our notation a little more consistent with what we'll do later, in this input layer we have the inputs x, but we can also thing it is as in activations of the first layers. So, if I defined a1 to be equal to x. So, the a1 is vector, I can now take this x here and replace this with z2 equals theta1 times a1 just by defining a1 to be activations in my input layer.Now, with what I've written so far I've now gotten myself the values for a1, a2, a3, and really I should put the superscripts there as well. But I need one more value, which is I also want this a(0)2 and that corresponds to a bias unit in the hidden layer that goes to the output there. Of course, there was a bias unit here too that, you know, it just didn't draw under here but to take care of this extra bias unit, what we're going to do is add an extra a0 superscript 2, that's equal to one, and after taking this step we now have that a2 is going to be a four dimensional feature vector because we just added this extra, you know, a0 which is equal to 1 corresponding to the bias unit in the hidden layer. And finally, to compute the actual value output of our hypotheses, we then simply need to compute z3. So z3 is equal to this term here that I'm just underlining. This inner term there is z3.And z3 is stated 2 times a2 and finally my hypotheses output h of x which is a3 that is the activation of my one and only unit in the output layer. So, that's just the real number. You can write it as a3 or as a(3)1 and that's g of z3. This process of computing h of x is also called forward propagation and is called that because we start of with the activations of the input-units and then we sort of forward-propagate that to the hidden layer and compute the activations of the hidden layer and then we sort of forward propagate that and compute the activations of the output layer, but this process of computing the activations from the input then the hidden then the output layer, and that's also called forward propagation and what we just did is we just worked out a vector wise implementation of this procedure. So, if you implement it using these equations that we have on the right, these would give you an efficient way or both of the efficient way of computing h of x.This forward propagation view also helps us to understand what Neural Networks might be doing and why they might help us to learn interesting nonlinear hypotheses.
### Compare Artificial Neurons Networks and Logistic Regression Algorithm
![Compare Artificial Neurons Networks and Logistic Regression Algorithm](amWiki/images/001/04-Week4/1-Neural Networks Representation/20-类比神经网络算法和逻辑回归算法.jpg)  
Consider the following neural network and let's say I cover up the left path of this picture for now. If you look at what's left in this picture. This looks a lot like logistic regression where what we're doing is we're using that note, that's just the logistic regression unit and we're using that to make a prediction h of x. And concretely, what the hypotheses is outputting is h of x is going to be equal to g which is my sigmoid activation function times theta 0 times a0 is equal to 1 plus theta 1 plus theta 2 times a2 plus theta 3 times a3 whether values a1, a2, a3 are those given by these three given units.Now, to be actually consistent to my early notation. Actually, we need to, you know, fill in these superscript 2's here everywhere and I also have these indices 1 there because I have only one output unit, but if you focus on the blue parts of the notation. This is, you know, this looks awfully like the standard logistic regression model, except that I now have a capital theta instead of lower case theta.And what this is doing is just logistic regression.But where the features fed into logistic regression are these values computed by the hidden layer.Just to say that again, what this neural network is doing is just like logistic regression, except that rather than using the original features x1, x2, x3,is using these new features a1, a2, a3. Again, we'll put the superscripts there, you know, to be consistent with the notation.And the cool thing about this, is that the features a1, a2, a3, they themselves are learned as functions of the input.Concretely, the function mapping from layer 1 to layer 2, that is determined by some other set of parameters, theta 1. So it's as if the neural network, instead of being constrained to feed the features x1, x2, x3 to logistic regression. It gets to learn its own features, a1, a2, a3, to feed into the logistic regression and as you can imagine depending on what parameters it chooses for theta 1. You can learn some pretty interesting and complex features and therefore you can end up with a better hypotheses than if you were constrained to use the raw features x1, x2 or x3 or if you will constrain to say choose the polynomial terms, you know, x1, x2, x3, and so on. But instead, this algorithm has the flexibility to try to learn whatever features at once, using these a1, a2, a3 in order to feed into this last unit that's essentially a logistic regression here. I realized this example is described as a somewhat high level and so I'm not sure if this intuition of the neural network, you know, having more complex features will quite make sense yet, but if it doesn't yet in the next two videos I'm going to go through a specific example of how a neural network can use this hidden there to compute more complex features to feed into this final output layer and how that can learn more complex hypotheses. So, in case what I'm saying here doesn't quite make sense, stick with me for the next two videos and hopefully out there working through those examples this explanation will make a little bit more sense. But just the point O. You can have neural networks with other types of diagrams as well, and the way that neural networks are connected, that's called the architecture. So the term architecture refers to how the different neurons are connected to each other.
### Artificial Neurons Networks Architecture
![Artificial Neurons Networks Architecture](amWiki/images/001/04-Week4/1-Neural Networks Representation/21-另一种人工神经网络架构.jpg)  
This is an example of a different neural network architecture and once again you may be able to get this intuition of how the second layer, here we have three heading units that are computing some complex function maybe of the input layer, and then the third layer can take the second layer's features and compute even more complex features in layer three so that by the time you get to the output layer, layer four, you can have even more complex features of what you are able to compute in layer three and so get very interesting nonlinear hypotheses.By the way, in a network like this, layer one, this is called an input layer. Layer four is still our output layer, and this network has two hidden layers. So anything that's not an input layer or an output layer is called a hidden layer.So, hopefully from this video you've gotten a sense of how the feed forward propagation step in a neural network works where you start from the activations of the input layer and forward propagate that to the first hidden layer, then the second hidden layer, and then finally the output layer. And you also saw how we can vectorize that computation.In the next, I realized that some of the intuitions in this video of how, you know, other certain layers are computing complex features of the early layers. I realized some of that intuition may be still slightly abstract and kind of a high level. And so what I would like to do in the two videos is work through a detailed example of how a neural network can be used to compute nonlinear functions of the input and hope that will give you a good sense of the sorts of complex nonlinear hypotheses we can get out of Neural Networks.
### Exam_in the video
![Exam_Understand the Artificial Neurons Networks Calculate](amWiki/images/001/04-Week4/1-Neural Networks Representation/22-内置习题_理解人工神经网络计算.jpg)
