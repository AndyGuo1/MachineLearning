# 神经网络表达-模型表达II
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/04-Week4/1-Neural Networks Representation/5-Examples and Intuitions I.mp4" type="video/mp4">
</video>
## 中文
### 初步介绍人工神经网络的逻辑门运算-异或和异或非运算
![初步介绍人工神经网络的逻辑门运算-异或和异或非运算](amWiki/images/001/04-Week4/1-Neural Networks Representation/24-初步介绍人工神经网络的逻辑门运算-异或和异或非运算.jpg)  
在接下来两节视频中 我要通过讲解 一个具体的例子来解释 神经网络是如何计算 关于输入的复杂的非线性函数 希望这个例子可以 让你了解为什么 神经网络可以用来 学习复杂的非线性假设 考虑下面的问题 我们有二进制的 输入特征 x1 x2 要么取0 要么取1 所以x1和x2只能 有两种取值 在这个例子中 我只画出了 两个正样本和 两个负样本 但你可以认为这是一个 更复杂的学习问题的 简化版本 在这个复杂问题中 我们可能 在右上角有一堆正样本 在左下方有 一堆用圆圈 表示的负样本 我们想要学习一种非线性的决策边界来 区分正负样本 那么 神经网络是 如何做到的呢？ 为了描述方便我不用右边这个例子 我用左边这个例子 这样更容易说明 具体来讲 这里需要计算的是 目标函数y 等于x1异或x2 或者 y也可以等于 x1 异或非 x2 其中异或非表示 x1异或x2后取反 X1异或X2 为真当且仅当 这两个值 X1或者X2中有且仅有一个为1 如果我 用XNOR作为例子 比用NOT作为例子 结果会好一些 但这两个其实是相同的 这就意味着在x1 异或x2后再取反 即 当它们同时为真 或者同时为假的时候 我们将获得 y等于1 y为0的结果 如果它们中仅有一个 为真 y则为0 我们想要知道是否能 找到一个神经网络模型来拟合这种训练集
### 案例_人工神经网络执行逻辑与运算分析
![案例_人工神经网络执行逻辑与运算分析](amWiki/images/001/04-Week4/1-Neural Networks Representation/25-案例_人工神经网络执行逻辑与运算分析.jpg)
为了建立 能拟合XNOR运算 的神经网络 我们先 讲解一个稍微简单 的神经网络 它拟合了“且运算” 假设我们 有输入x1和 x2 并且都是二进制 即要么为0要么为1 我们的目标 函数y正如你所知道的 等于x1且x2 这是一个逻辑与 那么 我们怎样得到一个 具有单个神经元的神经网络来计算 这个逻辑与呢 为了做到这一点 我也需要画出偏置单元 即这个里面有个+1的单元 现在 让我给这个网络 分配一些权重 或参数 我在图上写出这些参数 这里是-30正20 正20 即我给 x0前面的 系数赋值 为-30. 这个正1会 作为这个单元的值 关于20的参数值 且x1乘以+20 以及x2乘以+20 都是这个单元的输入 所以 我的假设ħ(x) 等于 g(-30 + 20x1 + 20x2) 在图上画出 这些参数和 权重是很方便很直观的 其实 在这幅神经网络图中 这个-30 其实是θ(1)10 这个是 θ(1)11 这是 θ(1)12 但把它想成 这些边的 权重会更容易理解 让我们来看看这个小神经元是怎样计算的 回忆一下 s型 激励函数g(z)看起来是这样的 它从0开始 光滑 上升 穿过0.5 渐进到1.我们给出一些坐标 如果横轴值 z等于4.6 则S形函数等于0.99 这是非常接近 1的 并且由于对称性 如果z为-4.6 S形函数 等于0.01 非常接近0 让我们来看看四种可能的输入值 x1和x2的四种可能输入 看看我们的假设 在各种情况下的输出 如果X1和X2均为 0 那么 你看看这个 如果 x1和x2都等于 为0 则假设会输出g(-30) g(-30)在图的 很左边的地方 非常接近于0 如果x1等于0且 x2等于1 那么 此公式等于 g关于 -10取值 也在很左边的位置 所以 也是非常接近0 这个也是g(-10) 也就是说 如果x1 等于1并且 x2等于0 这就是-30加20等于-10 最后 如​​果 x1等于1 x2等于 1 那么这等于 -30 +20 +20 所以这是 取+10时 非常接近1 如果你看看 在这一列 这就是 逻辑“与”的计算结果 所以 这里得到的h h关于x取值 近似等于x1和x2的与运算的值 换句话说 假设输出 1 当且仅当 x1 x2 都等于1 所以 通过写出 这张真值表 我们就弄清楚了 神经网络 计算出的逻辑函数
### 案例_人工神经网络执行逻辑或运算分析
![案例_人工神经网络执行逻辑或运算分析](amWiki/images/001/04-Week4/1-Neural Networks Representation/26-案例_人工神经网络执行逻辑或运算分析.jpg)
这里的神经网络 实现了或函数的功能 接下来我告诉你是怎么看出来的 如果你把 假设写出来 会发现它等于 g关于-10 +20x1 +20x2的取值 如果把这些值都填上 会发现 这是g(-10) 约等于0 这是g(10) 约等于1 这个也约等于1 这些数字 本质上就是逻辑或 运算得到的值 所以 我希望 通过这个例子 你现在明白了 神经网络里单个的 神经元在计算 如AND和OR逻辑运算时是怎样发挥作用的 在接下来的视频中 我们将继续 讲解一个更复杂的例子 我们将告诉你 一个多层的神经网络 怎样被用于 计算更复杂的函数  如 XOR 函数或 XNOR 函数
### 内置习题
![内置习题_理解人工神经网络计算](amWiki/images/001/04-Week4/1-Neural Networks Representation/22-内置习题_理解人工神经网络计算.jpg)
## English
### Artificial Neurons Networks Forward Propagation Mode-Vectorization implement
![Artificial Neurons Networks Forward Propagation Mode-Vectorization implement](amWiki/images/001/04-Week4/1-Neural Networks Representation/19-人工神经网络正向传播模型-向量化实现.jpg)  
In this and the next video I want to work through a detailed example showing how a neural network can compute a complex non linear function of the input. And hopefully this will give you a good sense of why neural networks can be used to learn complex non linear hypotheses. Consider the following problem where we have features X1 and X2 that are binary values. So, either 0 or 1. So, X1 and X2 can each take on only one of two possible values. In this example, I've drawn only two positive examples and two negative examples. That you can think of this as a simplified version of a more complex learning problem where we may have a bunch of positive examples in the upper right and lower left and a bunch of negative examples denoted by the circles. And what we'd like to do is learn a non-linear division of boundary that may need to separate the positive and negative examples.So, how can a neural network do this and rather than using the example and the variable to use this maybe easier to examine example on the left. Concretely what this is, is really computing the type of label y equals x 1 x or x 2. Or actually this is actually the x 1 x nor x 2 function where x nor is the alternative notation for not x 1 or x 2. So, x 1 x or x 2 that's true only if exactly 1 of x 1 or x 2 is equal to 1. It turns out that these specific examples in the works out a little bit better if we use the XNOR example instead. These two are the same of course. This means not x1 or x2 and so, we're going to have positive examples of either both are true or both are false and what have as y equals 1, y equals 1. And we're going to have y equals 0 if only one of them is true and we're going to figure out if we can get a neural network to fit to this sort of training set.
### Example_Artificial Neurons Networks process logical AND Calculate Analysis
![Example_Artificial Neurons Networks process logical AND Calculate Analysis](amWiki/images/001/04-Week4/1-Neural Networks Representation/25-案例_人工神经网络执行逻辑与运算分析.jpg)
In order to build up to a network that fits the XNOR example we're going to start with a slightly simpler one and show a network that fits the AND function. Concretely, let's say we have input x1 and x2 that are again binaries so, it's either 0 or 1 and let's say our target labels y = x1 AND x2. This is a logical AND.So, can we get a one-unit network to compute this logical AND function? In order to do so, I'm going to actually draw in the bias unit as well the plus one unit.Now let me just assign some values to the weights or parameters of this network. I'm gonna write down the parameters on this diagram here, -30 here. +20 and + 20. And what this mean is just that I'm assigning a value of -30 to the value associated with X0 this +1 going into this unit and a parameter value of +20 that multiplies to X1 a value of +20 for the parameter that multiplies into x 2. So, concretely it's the same that the hypothesis h(x)=g(-30+20 X1 plus 20 X2. So, sometimes it's just convenient to draw these weights. Draw these parameters up here in the diagram within and of course this- 30. This is actually theta 1 of 1 0. This is theta 1 of 1 1 and that's theta 1 of 1 2 but it's just easier to think about it as associating these parameters with the edges of the network.Let's look at what this little single neuron network will compute. Just to remind you the sigmoid activation function g(z) looks like this. It starts from 0 rises smoothly crosses 0.5 and then it asymptotic as 1 and to give you some landmarks, if the horizontal axis value z is equal to 4.6 then the sigmoid function is equal to 0.99. This is very close to 1 and kind of symmetrically, if it's -4.6 then the sigmoid function there is 0.01 which is very close to 0. Let's look at the four possible input values for x1 and x2 and look at what the hypotheses will output in that case. If x1 and x2 are both equal to 0. If you look at this, if x1 x2 are both equal to 0 then the hypothesis of g of -30. So, this is a very far to the left of this diagram so it will be very close to 0. If x 1 equals 0 and x equals 1, then this formula here evaluates the g that is the sigma function applied to -10, and again that's you know to the far left of this plot and so, that's again very close to 0.This is also g of minus 10 that is f x 1 is equal to 1 and x 2 0, this minus 30 plus 20 which is minus 10 and finally if x 1 equals 1 x 2 equals 1 then you have g of minus 30 plus 20 plus 20. So, that's g of positive 10 which is there for very close to 1.And if you look in this column this is exactly the logical and function. So, this is computing h of x is approximately x 1 and x 2. In other words it outputs one If and only if x2, x1 and x2, are both equal to 1. So, by writing out our little truth table like this we manage to figure what's the logical function that our neural network computes.
### Example_Artificial Neurons Networks process logical OR Calculate Analysis
![Example_Artificial Neurons Networks process logical OR Calculate Analysis](amWiki/images/001/04-Week4/1-Neural Networks Representation/26-案例_人工神经网络执行逻辑或运算分析.jpg)
This network showed here computes the OR function. Just to show you how I worked that out. If you are write out the hypothesis that this confusing g of -10 + 20 x 1 + 20 x 2 and so you fill in these values. You find that's g of minus 10 which is approximately 0. g of 10 which is approximately 1 and so on and these are approximately 1 and approximately 1 and these numbers are essentially the logical OR function.So, hopefully with this you now understand how single neurons in a neural network can be used to compute logical functions like AND and OR and so on. In the next video we'll continue building on these examples and work through a more complex example. We'll get to show you how a neural network now with multiple layers of units can be used to compute more complex functions like the XOR function or the XNOR function.
### Exam_in the video
![Exam_Understand the Artificial Neurons Networks Calculate](amWiki/images/001/04-Week4/1-Neural Networks Representation/22-内置习题_理解人工神经网络计算.jpg)
