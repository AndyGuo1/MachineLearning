# 神经网络表达-模型表达II
## 视频
<video height=510 width=900 controls="controls" preload="none">
      <source src="amWiki/videos/001/04-Week4/1-Neural Networks Representation/6-Examples and Intuitions II.mp4" type="video/mp4">
</video>
## 中文
### 案例_人工神经网络执行逻辑非运算分析
![案例_人工神经网络执行逻辑非运算分析](amWiki/images/001/04-Week4/1-Neural Networks Representation/29-案例_人工神经网络执行逻辑非运算分析.jpg)   
在这段视频中 我想通过例子来向大家展示 一个神经网络 是怎样计算非线性的假设函数 在上一段视频中 我们学习了 怎样运用神经网络 来计算x1和x2的与运算 以及x1和x2的或运算 其中x1和x2都是二进制数 也就是说 它们的值只能为0或1 同时 我们也学习了 怎样进行逻辑非运算 也就是计算 "非x1" 我先写出这个神经网络中 相连接的各权值 这里我们只有一个输入量x1 在这里我们也加上了 表示偏差的单位元 +1 如果我将输入单元和两个权数相连 也就是+10和-20 则可用以下假设方程来计算 h(x)=g(10-20x1) 其中g是一个S型函数 那么 当x1等于0时 计算出假设函数 g(10-20*0) 也就是g(10) 这个值近似的等于1 而当x等于1时 计算出的假设函数则变成 g(-10) 也就是约等于0 如果你观察这两个值 你会发现这实际上计算的 就是“非x1”函数 所以要计算逻辑非运算 总体思路是 在你希望取非运算的变量前面 放上一个绝对值大的负数作为权值 因此 如果放一个-20 那么和x1相乘 很显然 最终的结果 就得到了对x1进行非运算的效果 另外 我再给出一个例子 计算这样一个函数 (非x1)与(非x2) 我希望大家思考一下 自己动手算一算 你大概应该知道 至少应该在x1和x2前面 放一个绝对值比较大的负数作为权值 不过 还有一种可行的方法 是建立一个神经网络来计算 用只有一个输出单元的神经网络 没问题吧？ 因此 这个看起来很长的逻辑函数 “(非x1)与(非x2)”的值 将等于1 当且仅当 x1等于x2等于0 所以 这是个逻辑函数 这里是非x1 也就是说x1必为0 然后是非x2 这表示x2也必为0 因此这个逻辑函数等于1 当且仅当 x1和x2的值都为0时成立 现在你应该也清楚了 怎样建立一个小规模的神经网络 来计算 这个逻辑函数的值
### 案例_人工神经网络执行与或非运算【与门非门或门运算综合】
![案例_人工神经网络执行与或非运算【与门非门或门运算综合】](amWiki/images/001/04-Week4/1-Neural Networks Representation/30-案例_人工神经网络执行与或非运算【与门非门或门运算综合】.jpg)  
把以上我们介绍的 这三个部分内容放在一起 "x1与x2"与运算的网络 以及计算 "(非x1)与(非x2)"的网络 还有最后一个是 "x1或x2"的或运算网络 把这三个网络放在一起 我们就应该能计算 "x1 XNOR x2" 也就是同或门运算 提醒一下 如果这是x1 x2 那么我们想要计算的这个函数 在这里和这里是负样本 而在这里和这里 函数有正样本值 那么很显然 为了分隔开正样本和负样本 我们需要一个非线性的判别边界 这里我们用以下这个网络来解决 取输入单元 +1 x1和x2 建立第一个隐藏层单元 我们称其为a(2)1 因为它是第一个隐藏单元 接下来我要从红色的网络 也就是"x1与x2"这个网络 复制出权值 也就是-30 20 20 接下来 我再建立第二个隐藏单元 我们称之为a(2)2 它是第二层的第二个隐藏单元 然后再从中间的青色网络中 复制出权值 这样我们就有了 10 -20 -20 这样三个权值 因此 我们来看一下真值表中的值 对于红色的这个网络 我们知道是x1和x2的与运算 所以 这里的值大概等于0 0 0 1 这取决于x1和x2的具体取值 对于a (2)2 也就是青色的网络 我们知道这是“(非x1)与(非x2)”的运算 那么对于x1和x2的四种取值 其结果将为 1 0 0 0 最后 建立输出节点 也就是输出单元 a(3)1 这也是等于输出值h(x) 然后 复制一个或运算网络 同时 我需要一个+1作为偏差单元 将其添加进来 然后从绿色的网络中复制出所有的权值 也就是-10 20 20 我们之前已经知道这是一个或运算函数 那么我们继续看真值表的值 第一行的值是0和1的或运算 其结果为1 然后是0和0的或运算 其结果为0 0和0的或运算 结果还是0 1和0的或运算 其结果为1 因此 h(x)的值等于1 当x1和x2都为0 或者x1和x2都为1的时候成立 具体来说 在这两种情况时 h(x)输出1 在另两种情况时 h(x)输出0 那么对于这样一个神经网络 有一个输入层 一个隐藏层 和一个输出层 我们最终得到了 计算XNOR函数的非线性判别边界
### 人工神经网络计算复杂函数直观理解
![人工神经网络计算复杂函数直观理解](amWiki/images/001/04-Week4/1-Neural Networks Representation/31-人工神经网络计算复杂函数直观理解.jpg)  
更一般的理解是 在输入层中 我们只有原始输入值 然后我们建立了一个隐藏层 用来计算稍微复杂一些的 输入量的函数 如图所示 这些都是稍微复杂一些的函数 然后 通过添加另一个层 我们得到了一个更复杂一点的函数 这就是关于 神经网络可以计算较复杂函数 的某种直观解释 我们知道 当层数很多的时候 你有一个相对简单的输入量的函数 作为第二层 而第三层可以建立在此基础上 来计算更加复杂一些的函数 然后再下一层 又可以计算再复杂一些的函数
### 案例_使用人工神经网络的算法进行手写数字的辨识
![案例_使用人工神经网络的算法进行手写数字的辨识](amWiki/images/001/04-Week4/1-Neural Networks Representation/32-案例_使用人工神经网络的算法进行手写数字的辨识.jpg)  
![案例_使用人工神经网络的算法 进行手写数字的辨识](amWiki/images/001/04-Week4/1-Neural Networks Representation/33-案例_使用人工神经网络的算法进行手写数字的辨识.jpg)  
在这段视频的最后 我想给大家展示一个有趣的例子 这是一个神经网络 通过运用更深的层数 来计算更加复杂函数的例子 我将要展示的这段视频 来源于我的一个好朋友 阳乐昆(Yann LeCun) Yann是一名教授 供职于纽约大学 他也是神经网络研究 早期的奠基者之一 也是这一领域的大牛 他的很多理论和想法 现在都已经被应用于 各种各样的产品和应用中 遍布于全世界 所以我想向大家展示一段 他早期工作中的视频 这段视频中 他使用神经网络的算法 进行手写数字的辨识 你也许记得 在这门课刚开始的时候 我说过 关于神经网络的一个早期成就 就是应用神经网络 读取邮政编码 以帮助我们进行邮递 那么这便是其中一种尝试 这就是为了解决这个问题 而尝试采用的一种算法 在视频中 这个区域 是输入区域 表示的是手写字符 它们将被传递给神经网络 这一列数字表示 通过该网络第一个隐藏层运算后 特征量的可视化结果 因此通过第一个隐藏层 可视化结果显示的是 探测出的不同特征 不同边缘和边线 这是下一个隐藏层的可视化结果 似乎很难看出 怎样理解更深的隐藏层 以及下一个隐藏层 计算的可视化结果 可能你如果要想看出到底在进行怎样的运算 还是比较困难的 最终远远超出了第一个隐藏层的效果 但不管怎样 最终这些学习后的特征量 将被送到最后一层 也就是输出层 并且在最后作为结果 显示出来 最终预测到的结果 就是这个神经网络辨识出的手写数字的值 下面我们来观看这段视频 我希望你 喜欢这段视频 也希望这段视频能给你一些直观的感受 关于神经网络可以学习的 较为复杂一些的函数 在这个过程中 它使用的输入是不同的图像 或者说 就是一些原始的像素点 第一层计算出一些特征 然后下一层再计算出 一些稍复杂的特征 然后是更复杂的特征 然后这些特征 实际上被最终传递给 最后一层逻辑回归分类器上 使其准确地预测出 神经网络“看”到的数字
### 内置习题
![内置习题_人工神经网络非门计算](amWiki/images/001/04-Week4/1-Neural Networks Representation/34-内置习题_人工神经网络非门计算.jpg)
## English
###  Example_Artificial Neurons Networks process Logical Negations Calculate Analysis
![Example_Artificial Neurons Networks process Logical Negations Calculate Analysis](amWiki/images/001/04-Week4/1-Neural Networks Representation/29-案例_人工神经网络执行逻辑非运算分析.jpg)  
In this video I'd like to keep working through our example to show how a Neural Network can compute complex non linear hypothesis. In the last video we saw how a Neural Network can be used to compute the functions x1 AND x2, and the function x1 OR x2 when x1 and x2 are binary, that is when they take on values 0,1. We can also have a network to compute negation, that is to compute the function not x1. Let me just write down the ways associated with this network. We have only one input feature x1 in this case and the bias unit +1. And if I associate this with the weights plus 10 and -20, then my hypothesis is computing this h(x) equals sigmoid (10- 20 x1). So when x1 is equal to 0, my hypothesis would be computing g(10- 20 x 0) is just 10. And so that's approximately 1, and when x is equal to 1, this will be g(-10) which is approximately equal to 0. And if you look at what these values are, that's essentially the not x1 function. Cells include negations, the general idea is to put that large negative weight in front of the variable you want to negate. Minus 20 multiplied by x1 and that's the general idea of how you end up negating x1. And so in an example that I hope that you can figure out yourself. If you want to compute a function like this NOT x1 AND NOT x2, part of that will probably be putting large negative weights in front of x1 and x2, but it should be feasible. So you get a neural network with just one output unit to compute this as well. All right, so this logical function, NOT x1 AND NOT x2, is going to be equal to 1 if and only if x1 equals x2 equals 0. All right since this is a logical function, this says NOT x1 means x1 must be 0 and NOT x2, that means x2 must be equal to 0 as well. So this logical function is equal to 1 if and only if both x1 and x2 are equal to 0 and hopefully you should be able to figure out how to make a small neural network to compute this logical function as well.
### Example_Artificial Neurons Networks process Logical XNOR Calculate Analysis【AND NOT OR Logical Synthetical Calculation】
![Example_Artificial Neurons Networks process Logical XNOR Calculate Analysis【AND NOT OR Logical Synthetical Calculation】](amWiki/images/001/04-Week4/1-Neural Networks Representation/30-案例_人工神经网络执行与或非运算【与门非门或门运算综合】.jpg)  
Let's draw the network. I'm going to take my input +1, x1, x2 and create my first hidden unit here. I'm gonna call this a 21 cuz that's my first hidden unit. And I'm gonna copy the weight over from the red network, the x1 and x2. As well so then -30, 20, 20. Next let me create a second hidden unit which I'm going to call a 2 2. That is the second hidden unit of layer two. I'm going to copy over the cyan that's work in the middle, so I'm gonna have the weights 10 -20 -20. And so, let's pull some of the truth table values. For the red network, we know that was computing the x1 and x2, and so this will be approximately 0 0 0 1, depending on the values of x1 and x2, and for a 2 2, the cyan network. What do we know? The function NOT x1 AND NOT x2, that outputs 1 0 0 0, for the 4 values of x1 and x2.Finally, I'm going to create my output node, my output unit that is a 3 1. This is one more output h(x) and I'm going to copy over the old network for that. And I'm going to need a +1 bias unit here, so you draw that in, And I'm going to copy over the weights from the green networks. So that's -10, 20, 20 and we know earlier that this computes the OR function.So let's fill in the truth table entries.So the first entry is 0 OR 1 which can be 1 that makes 0 OR 0 which is 0, 0 OR 0 which is 0, 1 OR 0 and that falls to 1. And thus h(x) is equal to 1 when either both x1 and x2 are zero or when x1 and x2 are both 1 and concretely h(x) outputs 1 exactly at these two locations and then outputs 0 otherwise.And thus will this neural network, which has a input layer, one hidden layer, and one output layer, we end up with a nonlinear decision boundary that computes this XNOR function.
### Artificial Neurons Networks Calculate Complex Function Intuitions
![Artificial Neurons Networks Calculate Complex Function Intuitions](amWiki/images/001/04-Week4/1-Neural Networks Representation/31-人工神经网络计算复杂函数直观理解.jpg)  
And the more general intuition is that in the input layer, we just have our four inputs. Then we have a hidden layer, which computed some slightly more complex functions of the inputs that its shown here this is slightly more complex functions. And then by adding yet another layer we end up with an even more complex non linear function.And this is a sort of intuition about why neural networks can compute pretty complicated functions. That when you have multiple layers you have relatively simple function of the inputs of the second layer. But the third layer I can build on that to complete even more complex functions, and then the layer after that can compute even more complex functions.
### Example_Artificial Neurons Networks to recognize handwriting
![Example_Artificial Neurons Networks to recognize handwriting](amWiki/images/001/04-Week4/1-Neural Networks Representation/32-案例_使用人工神经网络的算法进行手写数字的辨识.jpg)  
![Example_Artificial Neurons Networks to recognize handwriting](amWiki/images/001/04-Week4/1-Neural Networks Representation/33-案例_使用人工神经网络的算法进行手写数字的辨识.jpg)   
To wrap up this video, I want to show you a fun example of an application of a the Neural Network that captures this intuition of the deeper layers computing more complex features. I want to show you a video of that customer a good friend of mine Yann LeCunj. Yann is a professor at New York University, NYU and he was one of the early pioneers of Neural Network reasearch and is sort of a legend in the field now and his ideas are used in all sorts of products and applications throughout the world now.So I wanna show you a video from some of his early work in which he was using a neural network to recognize handwriting, to do handwritten digit recognition. You might remember early in this class, at the start of this class I said that one of the earliest successes of neural networks was trying to use it to read zip codes to help USPS Laws and read postal codes. So this is one of the attempts, this is one of the algorithms used to try to address that problem. In the video that I'll show you this area here is the input area that shows a canvasing character shown to the network. This column here shows a visualization of the features computed by sort of the first hidden layer of the network. So that the first hidden layer of the network and so the first hidden layer, this visualization shows different features. Different edges and lines and so on detected. This is a visualization of the next hidden layer. It's kinda harder to see, harder to understand the deeper, hidden layers, and that's a visualization of why the next hidden layer is confusing. You probably have a hard time seeing what's going on much beyond the first hidden layer, but then finally, all of these learned features get fed to the upper layer. And shown over here is the final answer, it's the final predictive value for what handwritten digit the neural network thinks it is being shown. So let's take a look at the video. [MUSIC] So I hope you enjoyed the video and that this hopefully gave you some intuition about the source of pretty complicated functions neural networks can learn. In which it takes its input this image, just takes this input, the raw pixels and the first hidden layer computes some set of features. The next hidden layer computes even more complex features and even more complex features. And these features can then be used by essentially the final layer of the logistic classifiers to make accurate predictions without the numbers that the network sees.
### Exam_in the video
![Exam_Understand the Artificial Neurons Networks Calculate](amWiki/images/001/04-Week4/1-Neural Networks Representation/34-内置习题_人工神经网络非门计算.jpg)
